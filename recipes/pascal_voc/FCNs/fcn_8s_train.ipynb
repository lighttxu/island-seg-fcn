{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "\n",
    "# Use second GPU -- change if you want to use a first one\n",
    "\n",
    "\n",
    "# Add a path to a custom fork of TF-Slim\n",
    "# Get it from here:\n",
    "# https://github.com/warmspringwinds/models/tree/fully_conv_vgg\n",
    "sys.path.append(\"/home/shou/network/tf-models/research/slim\")\n",
    "\n",
    "# Add path to the cloned library\n",
    "sys.path.append(\"/home/shou/network/fcn/tf-image-segmentation\")\n",
    "\n",
    "checkpoints_dir = '/home/shou/network/checkpoints'\n",
    "log_folder = '/home/shou/network/log_folder_fcn8s'\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from tf_image_segmentation.utils.tf_records import read_tfrecord_and_decode_into_image_annotation_pair_tensors\n",
    "from tf_image_segmentation.models.fcn_8s import FCN_8s\n",
    "\n",
    "from tf_image_segmentation.utils.pascal_voc import pascal_segmentation_lut\n",
    "\n",
    "from tf_image_segmentation.utils.training import get_valid_logits_and_labels\n",
    "\n",
    "from tf_image_segmentation.utils.augmentation import (distort_randomly_image_color,\n",
    "                                                      flip_randomly_left_right_image_with_annotation,\n",
    "                                                      scale_randomly_image_with_annotation_with_fixed_size_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:36: calling cond (from tensorflow.python.ops.control_flow_ops) with fn2 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:36: calling cond (from tensorflow.python.ops.control_flow_ops) with fn1 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:40: calling cond (from tensorflow.python.ops.control_flow_ops) with fn2 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:40: calling cond (from tensorflow.python.ops.control_flow_ops) with fn1 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From <ipython-input-3-6a76c411be5d>:48: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "image_train_size = [384, 384]\n",
    "number_of_classes = 2\n",
    "tfrecord_filename = '/home/shou/network/dataset/pascal_augmented_train_island.tfrecords'\n",
    "pascal_voc_lut = pascal_segmentation_lut(number_of_classes)\n",
    "class_labels = pascal_voc_lut.keys()\n",
    "\n",
    "fcn_16s_checkpoint_path = '/home/shou/network/dataset/model_fcn16s_final.ckpt'\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    [tfrecord_filename], num_epochs=5)\n",
    "\n",
    "image, annotation = read_tfrecord_and_decode_into_image_annotation_pair_tensors(filename_queue)\n",
    "\n",
    "# Various data augmentation stages\n",
    "image, annotation = flip_randomly_left_right_image_with_annotation(image, annotation)\n",
    "\n",
    "# image = distort_randomly_image_color(image)\n",
    "\n",
    "resized_image, resized_annotation = scale_randomly_image_with_annotation_with_fixed_size_output(image, annotation, image_train_size)\n",
    "\n",
    "\n",
    "resized_annotation = tf.squeeze(resized_annotation)\n",
    "\n",
    "image_batch, annotation_batch = tf.train.shuffle_batch( [resized_image, resized_annotation],\n",
    "                                             batch_size=1,\n",
    "                                             capacity=3000,\n",
    "                                             num_threads=2,\n",
    "                                             min_after_dequeue=1000)\n",
    "\n",
    "upsampled_logits_batch, fcn_16s_variables_mapping = FCN_8s(image_batch_tensor=image_batch,\n",
    "                                                           number_of_classes=number_of_classes,\n",
    "                                                           is_training=True)\n",
    "\n",
    "\n",
    "valid_labels_batch_tensor, valid_logits_batch_tensor = get_valid_logits_and_labels(annotation_batch_tensor=annotation_batch,\n",
    "                                                                                     logits_batch_tensor=upsampled_logits_batch,\n",
    "                                                                                    class_labels=class_labels)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=valid_logits_batch_tensor,\n",
    "                                                          labels=valid_labels_batch_tensor)\n",
    "\n",
    "#cross_entropy_sum = tf.reduce_sum(cross_entropies)\n",
    "\n",
    "cross_entropy_sum = tf.reduce_mean(cross_entropies)\n",
    "\n",
    "pred = tf.argmax(upsampled_logits_batch, dimension=3)\n",
    "\n",
    "probabilities = tf.nn.softmax(upsampled_logits_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"adam_vars\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=0.000000001).minimize(cross_entropy_sum)\n",
    "\n",
    "\n",
    "#adam_optimizer_variables = slim.get_variables_to_restore(include=['adam_vars'])\n",
    "\n",
    "# Variable's initialization functions\n",
    "init_fn = slim.assign_from_checkpoint_fn(model_path=fcn_16s_checkpoint_path,\n",
    "                                         var_list=fcn_16s_variables_mapping)\n",
    "\n",
    "global_vars_init_op = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar('cross_entropy_loss', cross_entropy_sum)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "summary_string_writer = tf.summary.FileWriter(log_folder)\n",
    "\n",
    "# Create the log folder if doesn't exist yet\n",
    "if not os.path.exists(log_folder):\n",
    "     os.makedirs(log_folder)\n",
    "\n",
    "#optimization_variables_initializer = tf.variables_initializer(adam_optimizer_variables)\n",
    "    \n",
    "#The op for initializing the variables.\n",
    "local_vars_init_op = tf.local_variables_initializer()\n",
    "\n",
    "combined_op = tf.group(local_vars_init_op, global_vars_init_op)\n",
    "\n",
    "# We need this to save only model variables and omit\n",
    "# optimization-related and other variables.\n",
    "model_variables = slim.get_model_variables()\n",
    "saver = tf.train.Saver(model_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/shou/network/dataset/model_fcn16s_final.ckpt\n",
      "step :0 Loss: 0.109686\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn8s_final.ckpt\n",
      "step :1 Loss: 0.109197\n",
      "step :2 Loss: 0.0807109\n",
      "step :3 Loss: 0.0777678\n",
      "step :4 Loss: 0.0934385\n",
      "step :5 Loss: 0.044567\n",
      "step :6 Loss: 0.201067\n",
      "step :7 Loss: 0.0962793\n",
      "step :8 Loss: 0.155758\n",
      "step :9 Loss: 0.160842\n",
      "step :10 Loss: 0.15019\n",
      "step :11 Loss: 0.162537\n",
      "step :12 Loss: 0.130364\n",
      "step :13 Loss: 0.157157\n",
      "step :14 Loss: 0.112052\n",
      "step :15 Loss: 0.133374\n",
      "step :16 Loss: 0.143589\n",
      "step :17 Loss: 0.0891468\n",
      "step :18 Loss: 0.0960048\n",
      "step :19 Loss: 0.184017\n",
      "step :20 Loss: 0.132332\n",
      "step :21 Loss: 0.158992\n",
      "step :22 Loss: 0.198495\n",
      "step :23 Loss: 0.131338\n",
      "step :24 Loss: 0.0379294\n",
      "step :25 Loss: 0.12577\n",
      "step :26 Loss: 0.0718431\n",
      "step :27 Loss: 0.154613\n",
      "step :28 Loss: 0.217096\n",
      "step :29 Loss: 0.161405\n",
      "step :30 Loss: 0.0904403\n",
      "step :31 Loss: 0.0555786\n",
      "step :32 Loss: 0.0865612\n",
      "step :33 Loss: 0.0774507\n",
      "step :34 Loss: 0.121367\n",
      "step :35 Loss: 0.0857501\n",
      "step :36 Loss: 0.0712766\n",
      "step :37 Loss: 0.097765\n",
      "step :38 Loss: 0.100243\n",
      "step :39 Loss: 0.149997\n",
      "step :40 Loss: 0.107207\n",
      "step :41 Loss: 0.197913\n",
      "step :42 Loss: 0.209538\n",
      "step :43 Loss: 0.141014\n",
      "step :44 Loss: 0.114295\n",
      "step :45 Loss: 0.143012\n",
      "step :46 Loss: 0.173786\n",
      "step :47 Loss: 0.108353\n",
      "step :48 Loss: 0.117513\n",
      "step :49 Loss: 0.124903\n",
      "step :50 Loss: 0.101842\n",
      "step :51 Loss: 0.200367\n",
      "step :52 Loss: 0.149676\n",
      "step :53 Loss: 0.0924297\n",
      "step :54 Loss: 0.112999\n",
      "step :55 Loss: 0.219224\n",
      "step :56 Loss: 0.0642467\n",
      "step :57 Loss: 0.149702\n",
      "step :58 Loss: 0.119479\n",
      "step :59 Loss: 0.109125\n",
      "step :60 Loss: 0.101268\n",
      "step :61 Loss: 0.104399\n",
      "step :62 Loss: 0.0853685\n",
      "step :63 Loss: 0.11919\n",
      "step :64 Loss: 0.19542\n",
      "step :65 Loss: 0.152654\n",
      "step :66 Loss: 0.0912121\n",
      "step :67 Loss: 0.122554\n",
      "step :68 Loss: 0.0395048\n",
      "step :69 Loss: 0.114656\n",
      "step :70 Loss: 0.110584\n",
      "step :71 Loss: 0.124427\n",
      "step :72 Loss: 0.0904504\n",
      "step :73 Loss: 0.0677112\n",
      "step :74 Loss: 0.134022\n",
      "step :75 Loss: 0.103541\n",
      "step :76 Loss: 0.0844398\n",
      "step :77 Loss: 0.173911\n",
      "step :78 Loss: 0.0744322\n",
      "step :79 Loss: 0.0624056\n",
      "step :80 Loss: 0.106803\n",
      "step :81 Loss: 0.125619\n",
      "step :82 Loss: 0.108296\n",
      "step :83 Loss: 0.135213\n",
      "step :84 Loss: 0.0835282\n",
      "step :85 Loss: 0.0875476\n",
      "step :86 Loss: 0.133968\n",
      "step :87 Loss: 0.108976\n",
      "step :88 Loss: 0.112944\n",
      "step :89 Loss: 0.100919\n",
      "step :90 Loss: 0.195011\n",
      "step :91 Loss: 0.0666527\n",
      "step :92 Loss: 0.256648\n",
      "step :93 Loss: 0.0480351\n",
      "step :94 Loss: 0.158048\n",
      "step :95 Loss: 0.186009\n",
      "step :96 Loss: 0.282726\n",
      "step :97 Loss: 0.0901273\n",
      "step :98 Loss: 0.104018\n",
      "step :99 Loss: 0.0736199\n",
      "step :100 Loss: 0.0311249\n",
      "step :101 Loss: 0.0805306\n",
      "step :102 Loss: 0.122799\n",
      "step :103 Loss: 0.236092\n",
      "step :104 Loss: 0.0927102\n",
      "step :105 Loss: 0.0792347\n",
      "step :106 Loss: 0.21998\n",
      "step :107 Loss: 0.115514\n",
      "step :108 Loss: 0.21002\n",
      "step :109 Loss: 0.164318\n",
      "step :110 Loss: 0.11911\n",
      "step :111 Loss: 0.124544\n",
      "step :112 Loss: 0.10396\n",
      "step :113 Loss: 0.142524\n",
      "step :114 Loss: 0.0926642\n",
      "step :115 Loss: 0.118024\n",
      "step :116 Loss: 0.174146\n",
      "step :117 Loss: 0.229476\n",
      "step :118 Loss: 0.144143\n",
      "step :119 Loss: 0.0853102\n",
      "step :120 Loss: 0.161746\n",
      "step :121 Loss: 0.187627\n",
      "step :122 Loss: 0.121824\n",
      "step :123 Loss: 0.228683\n",
      "step :124 Loss: 0.203495\n",
      "step :125 Loss: 0.162798\n",
      "step :126 Loss: 0.106741\n",
      "step :127 Loss: 0.102485\n",
      "step :128 Loss: 0.0903034\n",
      "step :129 Loss: 0.0700852\n",
      "step :130 Loss: 0.168848\n",
      "step :131 Loss: 0.139938\n",
      "step :132 Loss: 0.260764\n",
      "step :133 Loss: 0.10763\n",
      "step :134 Loss: 0.0979778\n",
      "step :135 Loss: 0.0627868\n",
      "step :136 Loss: 0.105657\n",
      "step :137 Loss: 0.130597\n",
      "step :138 Loss: 0.0883209\n",
      "step :139 Loss: 0.0796787\n",
      "step :140 Loss: 0.10377\n",
      "step :141 Loss: 0.0382737\n",
      "step :142 Loss: 0.0862853\n",
      "step :143 Loss: 0.112439\n",
      "step :144 Loss: 0.0519756\n",
      "step :145 Loss: 0.30727\n",
      "step :146 Loss: 0.115183\n",
      "step :147 Loss: 0.193753\n",
      "step :148 Loss: 0.140105\n",
      "step :149 Loss: 0.083592\n",
      "step :150 Loss: 0.0842025\n",
      "step :151 Loss: 0.111215\n",
      "step :152 Loss: 0.0437231\n",
      "step :153 Loss: 0.135009\n",
      "step :154 Loss: 0.0638274\n",
      "step :155 Loss: 0.109069\n",
      "step :156 Loss: 0.164981\n",
      "step :157 Loss: 0.0745014\n",
      "step :158 Loss: 0.0971173\n",
      "step :159 Loss: 0.218488\n",
      "step :160 Loss: 0.0223825\n",
      "step :161 Loss: 0.0712983\n",
      "step :162 Loss: 0.266066\n",
      "step :163 Loss: 0.0935161\n",
      "step :164 Loss: 0.24453\n",
      "step :165 Loss: 0.0485926\n",
      "step :166 Loss: 0.217924\n",
      "step :167 Loss: 0.117021\n",
      "step :168 Loss: 0.0734876\n",
      "step :169 Loss: 0.0961239\n",
      "step :170 Loss: 0.201197\n",
      "step :171 Loss: 0.0701552\n",
      "step :172 Loss: 0.182361\n",
      "step :173 Loss: 0.282171\n",
      "step :174 Loss: 0.0754657\n",
      "step :175 Loss: 0.108863\n",
      "step :176 Loss: 0.225661\n",
      "step :177 Loss: 0.0326633\n",
      "step :178 Loss: 0.100313\n",
      "step :179 Loss: 0.149825\n",
      "step :180 Loss: 0.110102\n",
      "step :181 Loss: 0.211682\n",
      "step :182 Loss: 0.113103\n",
      "step :183 Loss: 0.0323161\n",
      "step :184 Loss: 0.22903\n",
      "step :185 Loss: 0.0398239\n",
      "step :186 Loss: 0.24373\n",
      "step :187 Loss: 0.145324\n",
      "step :188 Loss: 0.10054\n",
      "step :189 Loss: 0.145074\n",
      "step :190 Loss: 0.0787965\n",
      "step :191 Loss: 0.0524009\n",
      "step :192 Loss: 0.136355\n",
      "step :193 Loss: 0.160239\n",
      "step :194 Loss: 0.0946553\n",
      "step :195 Loss: 0.0181165\n",
      "step :196 Loss: 0.0798599\n",
      "step :197 Loss: 0.117499\n",
      "step :198 Loss: 0.0876941\n",
      "step :199 Loss: 0.108224\n",
      "step :200 Loss: 0.180703\n",
      "step :201 Loss: 0.118471\n",
      "step :202 Loss: 0.135345\n",
      "step :203 Loss: 0.203575\n",
      "step :204 Loss: 0.106343\n",
      "step :205 Loss: 0.0979817\n",
      "step :206 Loss: 0.138839\n",
      "step :207 Loss: 0.147986\n",
      "step :208 Loss: 0.137955\n",
      "step :209 Loss: 0.0794795\n",
      "step :210 Loss: 0.194442\n",
      "step :211 Loss: 0.215898\n",
      "step :212 Loss: 0.0771218\n",
      "step :213 Loss: 0.0315359\n",
      "step :214 Loss: 0.0632776\n",
      "step :215 Loss: 0.0789057\n",
      "step :216 Loss: 0.0848205\n",
      "step :217 Loss: 0.143655\n",
      "step :218 Loss: 0.0883863\n",
      "step :219 Loss: 0.0837332\n",
      "step :220 Loss: 0.146795\n",
      "step :221 Loss: 0.0913267\n",
      "step :222 Loss: 0.0478809\n",
      "step :223 Loss: 0.133134\n",
      "step :224 Loss: 0.133436\n",
      "step :225 Loss: 0.155149\n",
      "step :226 Loss: 0.0916899\n",
      "step :227 Loss: 0.15314\n",
      "step :228 Loss: 0.109478\n",
      "step :229 Loss: 0.0998452\n",
      "step :230 Loss: 0.272291\n",
      "step :231 Loss: 0.111319\n",
      "step :232 Loss: 0.21819\n",
      "step :233 Loss: 0.0899848\n",
      "step :234 Loss: 0.144009\n",
      "step :235 Loss: 0.0538973\n",
      "step :236 Loss: 0.140235\n",
      "step :237 Loss: 0.0713199\n",
      "step :238 Loss: 0.116228\n",
      "step :239 Loss: 0.0874178\n",
      "step :240 Loss: 0.206058\n",
      "step :241 Loss: 0.07973\n",
      "step :242 Loss: 0.16917\n",
      "step :243 Loss: 0.1676\n",
      "step :244 Loss: 0.209324\n",
      "step :245 Loss: 0.182891\n",
      "step :246 Loss: 0.0820999\n",
      "step :247 Loss: 0.180855\n",
      "step :248 Loss: 0.172614\n",
      "step :249 Loss: 0.209719\n",
      "step :250 Loss: 0.0957425\n",
      "step :251 Loss: 0.089766\n",
      "step :252 Loss: 0.12352\n",
      "step :253 Loss: 0.069844\n",
      "step :254 Loss: 0.113246\n",
      "step :255 Loss: 0.122904\n",
      "step :256 Loss: 0.0921648\n",
      "step :257 Loss: 0.0645868\n",
      "step :258 Loss: 0.127758\n",
      "step :259 Loss: 0.125976\n",
      "step :260 Loss: 0.116755\n",
      "step :261 Loss: 0.105824\n",
      "step :262 Loss: 0.0941889\n",
      "step :263 Loss: 0.0542514\n",
      "step :264 Loss: 0.0977994\n",
      "step :265 Loss: 0.227058\n",
      "step :266 Loss: 0.243916\n",
      "step :267 Loss: 0.067665\n",
      "step :268 Loss: 0.0677904\n",
      "step :269 Loss: 0.446098\n",
      "step :270 Loss: 0.262131\n",
      "step :271 Loss: 0.263291\n",
      "step :272 Loss: 0.13441\n",
      "step :273 Loss: 0.0805408\n",
      "step :274 Loss: 0.149142\n",
      "step :275 Loss: 0.0947141\n",
      "step :276 Loss: 0.088299\n",
      "step :277 Loss: 0.0937945\n",
      "step :278 Loss: 0.181178\n",
      "step :279 Loss: 0.201881\n",
      "step :280 Loss: 0.201182\n",
      "step :281 Loss: 0.0994319\n",
      "step :282 Loss: 0.0846269\n",
      "step :283 Loss: 0.145505\n",
      "step :284 Loss: 0.0973631\n",
      "step :285 Loss: 0.0948335\n",
      "step :286 Loss: 0.143837\n",
      "step :287 Loss: 0.0810138\n",
      "step :288 Loss: 0.0492578\n",
      "step :289 Loss: 0.086816\n",
      "step :290 Loss: 0.0264069\n",
      "step :291 Loss: 0.0172218\n",
      "step :292 Loss: 0.142396\n",
      "step :293 Loss: 0.247229\n",
      "step :294 Loss: 0.167034\n",
      "step :295 Loss: 0.148553\n",
      "step :296 Loss: 0.11819\n",
      "step :297 Loss: 0.0371342\n",
      "step :298 Loss: 0.0885762\n",
      "step :299 Loss: 0.278742\n",
      "step :300 Loss: 0.11564\n",
      "step :301 Loss: 0.13403\n",
      "step :302 Loss: 0.155163\n",
      "step :303 Loss: 0.111179\n",
      "step :304 Loss: 0.0327618\n",
      "step :305 Loss: 0.108812\n",
      "step :306 Loss: 0.150843\n",
      "step :307 Loss: 0.117102\n",
      "step :308 Loss: 0.305299\n",
      "step :309 Loss: 0.194219\n",
      "step :310 Loss: 0.158507\n",
      "step :311 Loss: 0.150734\n",
      "step :312 Loss: 0.147463\n",
      "step :313 Loss: 0.18052\n",
      "step :314 Loss: 0.0889341\n",
      "step :315 Loss: 0.214143\n",
      "step :316 Loss: 0.22822\n",
      "step :317 Loss: 0.269156\n",
      "step :318 Loss: 0.142939\n",
      "step :319 Loss: 0.063278\n",
      "step :320 Loss: 0.0473885\n",
      "step :321 Loss: 0.0854765\n",
      "step :322 Loss: 0.091255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :323 Loss: 0.181175\n",
      "step :324 Loss: 0.150646\n",
      "step :325 Loss: 0.135598\n",
      "step :326 Loss: 0.210695\n",
      "step :327 Loss: 0.131739\n",
      "step :328 Loss: 0.143778\n",
      "step :329 Loss: 0.0807763\n",
      "step :330 Loss: 0.123752\n",
      "step :331 Loss: 0.0512108\n",
      "step :332 Loss: 0.0814593\n",
      "step :333 Loss: 0.164825\n",
      "step :334 Loss: 0.0977962\n",
      "step :335 Loss: 0.13645\n",
      "step :336 Loss: 0.159602\n",
      "step :337 Loss: 0.149109\n",
      "step :338 Loss: 0.0972088\n",
      "step :339 Loss: 0.0742145\n",
      "step :340 Loss: 0.107283\n",
      "step :341 Loss: 0.0932185\n",
      "step :342 Loss: 0.080807\n",
      "step :343 Loss: 0.0803085\n",
      "step :344 Loss: 0.0932037\n",
      "step :345 Loss: 0.150645\n",
      "step :346 Loss: 0.10428\n",
      "step :347 Loss: 0.151033\n",
      "step :348 Loss: 0.111929\n",
      "step :349 Loss: 0.122784\n",
      "step :350 Loss: 0.155318\n",
      "step :351 Loss: 0.0903587\n",
      "step :352 Loss: 0.128373\n",
      "step :353 Loss: 0.103495\n",
      "step :354 Loss: 0.104344\n",
      "step :355 Loss: 0.26093\n",
      "step :356 Loss: 0.0584579\n",
      "step :357 Loss: 0.179274\n",
      "step :358 Loss: 0.180735\n",
      "step :359 Loss: 0.193826\n",
      "step :360 Loss: 0.0992287\n",
      "step :361 Loss: 0.111282\n",
      "step :362 Loss: 0.244549\n",
      "step :363 Loss: 0.199666\n",
      "step :364 Loss: 0.0798027\n",
      "step :365 Loss: 0.124901\n",
      "step :366 Loss: 0.0891311\n",
      "step :367 Loss: 0.113161\n",
      "step :368 Loss: 0.161528\n",
      "step :369 Loss: 0.288078\n",
      "step :370 Loss: 0.0750188\n",
      "step :371 Loss: 0.177203\n",
      "step :372 Loss: 0.056711\n",
      "step :373 Loss: 0.0734694\n",
      "step :374 Loss: 0.0685452\n",
      "step :375 Loss: 0.273678\n",
      "step :376 Loss: 0.0920049\n",
      "step :377 Loss: 0.156605\n",
      "step :378 Loss: 0.0693826\n",
      "step :379 Loss: 0.231718\n",
      "step :380 Loss: 0.119833\n",
      "step :381 Loss: 0.104382\n",
      "step :382 Loss: 0.125033\n",
      "step :383 Loss: 0.22095\n",
      "step :384 Loss: 0.0668957\n",
      "step :385 Loss: 0.146634\n",
      "step :386 Loss: 0.0564463\n",
      "step :387 Loss: 0.257838\n",
      "step :388 Loss: 0.097954\n",
      "step :389 Loss: 0.162883\n",
      "step :390 Loss: 0.0774162\n",
      "step :391 Loss: 0.123057\n",
      "step :392 Loss: 0.0955748\n",
      "step :393 Loss: 0.0740384\n",
      "step :394 Loss: 0.140131\n",
      "step :395 Loss: 0.137813\n",
      "step :396 Loss: 0.255508\n",
      "step :397 Loss: 0.0914457\n",
      "step :398 Loss: 0.119817\n",
      "step :399 Loss: 0.0878163\n",
      "step :400 Loss: 0.140866\n",
      "step :401 Loss: 0.230175\n",
      "step :402 Loss: 0.154611\n",
      "step :403 Loss: 0.20555\n",
      "step :404 Loss: 0.145374\n",
      "step :405 Loss: 0.0979613\n",
      "step :406 Loss: 0.156658\n",
      "step :407 Loss: 0.070346\n",
      "step :408 Loss: 0.14042\n",
      "step :409 Loss: 0.0935052\n",
      "step :410 Loss: 0.0980656\n",
      "step :411 Loss: 0.123185\n",
      "step :412 Loss: 0.177501\n",
      "step :413 Loss: 0.106714\n",
      "step :414 Loss: 0.133174\n",
      "step :415 Loss: 0.0829483\n",
      "step :416 Loss: 0.0687103\n",
      "step :417 Loss: 0.117444\n",
      "step :418 Loss: 0.218865\n",
      "step :419 Loss: 0.16018\n",
      "step :420 Loss: 0.159247\n",
      "step :421 Loss: 0.0880468\n",
      "step :422 Loss: 0.0716647\n",
      "step :423 Loss: 0.0598218\n",
      "step :424 Loss: 0.0836871\n",
      "step :425 Loss: 0.132977\n",
      "step :426 Loss: 0.093737\n",
      "step :427 Loss: 0.0784297\n",
      "step :428 Loss: 0.161188\n",
      "step :429 Loss: 0.109929\n",
      "step :430 Loss: 0.101667\n",
      "step :431 Loss: 0.0460262\n",
      "step :432 Loss: 0.190991\n",
      "step :433 Loss: 0.0606861\n",
      "step :434 Loss: 0.127218\n",
      "step :435 Loss: 0.0932193\n",
      "step :436 Loss: 0.0880789\n",
      "step :437 Loss: 0.061114\n",
      "step :438 Loss: 0.206854\n",
      "step :439 Loss: 0.0795824\n",
      "step :440 Loss: 0.133061\n",
      "step :441 Loss: 0.130872\n",
      "step :442 Loss: 0.147818\n",
      "step :443 Loss: 0.0764099\n",
      "step :444 Loss: 0.187589\n",
      "step :445 Loss: 0.0943925\n",
      "step :446 Loss: 0.174384\n",
      "step :447 Loss: 0.136861\n",
      "step :448 Loss: 0.213299\n",
      "step :449 Loss: 0.361423\n",
      "step :450 Loss: 0.127659\n",
      "step :451 Loss: 0.0833819\n",
      "step :452 Loss: 0.0532529\n",
      "step :453 Loss: 0.0824313\n",
      "step :454 Loss: 0.088309\n",
      "step :455 Loss: 0.134645\n",
      "step :456 Loss: 0.113591\n",
      "step :457 Loss: 0.142502\n",
      "step :458 Loss: 0.0869716\n",
      "step :459 Loss: 0.109725\n",
      "step :460 Loss: 0.161671\n",
      "step :461 Loss: 0.0842846\n",
      "step :462 Loss: 0.175052\n",
      "step :463 Loss: 0.132788\n",
      "step :464 Loss: 0.139673\n",
      "step :465 Loss: 0.218246\n",
      "step :466 Loss: 0.121904\n",
      "step :467 Loss: 0.112583\n",
      "step :468 Loss: 0.0639508\n",
      "step :469 Loss: 0.240031\n",
      "step :470 Loss: 0.127612\n",
      "step :471 Loss: 0.199791\n",
      "step :472 Loss: 0.188457\n",
      "step :473 Loss: 0.113662\n",
      "step :474 Loss: 0.131446\n",
      "step :475 Loss: 0.0651123\n",
      "step :476 Loss: 0.131196\n",
      "step :477 Loss: 0.0577434\n",
      "step :478 Loss: 0.161973\n",
      "step :479 Loss: 0.124703\n",
      "step :480 Loss: 0.203374\n",
      "step :481 Loss: 0.0430862\n",
      "step :482 Loss: 0.0485092\n",
      "step :483 Loss: 0.0982245\n",
      "step :484 Loss: 0.153779\n",
      "step :485 Loss: 0.114867\n",
      "step :486 Loss: 0.0600323\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn8s_final.ckpt\n",
      "step :487 Loss: 0.169192\n",
      "step :488 Loss: 0.0740479\n",
      "step :489 Loss: 0.115312\n",
      "step :490 Loss: 0.125551\n",
      "step :491 Loss: 0.115505\n",
      "step :492 Loss: 0.164021\n",
      "step :493 Loss: 0.0426663\n",
      "step :494 Loss: 0.170138\n",
      "step :495 Loss: 0.0884433\n",
      "step :496 Loss: 0.152499\n",
      "step :497 Loss: 0.0914698\n",
      "step :498 Loss: 0.0627318\n",
      "step :499 Loss: 0.241264\n",
      "step :500 Loss: 0.0715146\n",
      "step :501 Loss: 0.116324\n",
      "step :502 Loss: 0.0693919\n",
      "step :503 Loss: 0.183896\n",
      "step :504 Loss: 0.10249\n",
      "step :505 Loss: 0.413472\n",
      "step :506 Loss: 0.118622\n",
      "step :507 Loss: 0.107611\n",
      "step :508 Loss: 0.031845\n",
      "step :509 Loss: 0.256784\n",
      "step :510 Loss: 0.122035\n",
      "step :511 Loss: 0.114206\n",
      "step :512 Loss: 0.196519\n",
      "step :513 Loss: 0.0848807\n",
      "step :514 Loss: 0.0331388\n",
      "step :515 Loss: 0.101218\n",
      "step :516 Loss: 0.0481461\n",
      "step :517 Loss: 0.10146\n",
      "step :518 Loss: 0.143014\n",
      "step :519 Loss: 0.236234\n",
      "step :520 Loss: 0.0756706\n",
      "step :521 Loss: 0.0908583\n",
      "step :522 Loss: 0.0959243\n",
      "step :523 Loss: 0.181462\n",
      "step :524 Loss: 0.191736\n",
      "step :525 Loss: 0.0696073\n",
      "step :526 Loss: 0.146598\n",
      "step :527 Loss: 0.151525\n",
      "step :528 Loss: 0.0209652\n",
      "step :529 Loss: 0.22077\n",
      "step :530 Loss: 0.262527\n",
      "step :531 Loss: 0.164355\n",
      "step :532 Loss: 0.152165\n",
      "step :533 Loss: 0.206146\n",
      "step :534 Loss: 0.216924\n",
      "step :535 Loss: 0.0835302\n",
      "step :536 Loss: 0.11561\n",
      "step :537 Loss: 0.0290828\n",
      "step :538 Loss: 0.191945\n",
      "step :539 Loss: 0.220038\n",
      "step :540 Loss: 0.193684\n",
      "step :541 Loss: 0.0848955\n",
      "step :542 Loss: 0.111365\n",
      "step :543 Loss: 0.0667258\n",
      "step :544 Loss: 0.119129\n",
      "step :545 Loss: 0.11792\n",
      "step :546 Loss: 0.0672012\n",
      "step :547 Loss: 0.217733\n",
      "step :548 Loss: 0.0523774\n",
      "step :549 Loss: 0.0868831\n",
      "step :550 Loss: 0.198235\n",
      "step :551 Loss: 0.0241319\n",
      "step :552 Loss: 0.158541\n",
      "step :553 Loss: 0.147286\n",
      "step :554 Loss: 0.111802\n",
      "step :555 Loss: 0.102727\n",
      "step :556 Loss: 0.305505\n",
      "step :557 Loss: 0.196776\n",
      "step :558 Loss: 0.23683\n",
      "step :559 Loss: 0.244034\n",
      "step :560 Loss: 0.135149\n",
      "step :561 Loss: 0.0917355\n",
      "step :562 Loss: 0.115482\n",
      "step :563 Loss: 0.161265\n",
      "step :564 Loss: 0.202397\n",
      "step :565 Loss: 0.157962\n",
      "step :566 Loss: 0.222893\n",
      "step :567 Loss: 0.0985901\n",
      "step :568 Loss: 0.161769\n",
      "step :569 Loss: 0.236625\n",
      "step :570 Loss: 0.0603965\n",
      "step :571 Loss: 0.14195\n",
      "step :572 Loss: 0.0744766\n",
      "step :573 Loss: 0.100626\n",
      "step :574 Loss: 0.0791772\n",
      "step :575 Loss: 0.160312\n",
      "step :576 Loss: 0.0849523\n",
      "step :577 Loss: 0.0710812\n",
      "step :578 Loss: 0.127781\n",
      "step :579 Loss: 0.21057\n",
      "step :580 Loss: 0.159639\n",
      "step :581 Loss: 0.148707\n",
      "step :582 Loss: 0.174428\n",
      "step :583 Loss: 0.111245\n",
      "step :584 Loss: 0.161013\n",
      "step :585 Loss: 0.169502\n",
      "step :586 Loss: 0.0934974\n",
      "step :587 Loss: 0.212231\n",
      "step :588 Loss: 0.166327\n",
      "step :589 Loss: 0.0495972\n",
      "step :590 Loss: 0.117686\n",
      "step :591 Loss: 0.29739\n",
      "step :592 Loss: 0.183087\n",
      "step :593 Loss: 0.29621\n",
      "step :594 Loss: 0.0824832\n",
      "step :595 Loss: 0.0818896\n",
      "step :596 Loss: 0.148147\n",
      "step :597 Loss: 0.205266\n",
      "step :598 Loss: 0.0825958\n",
      "step :599 Loss: 0.112254\n",
      "step :600 Loss: 0.154474\n",
      "step :601 Loss: 0.0932304\n",
      "step :602 Loss: 0.0843158\n",
      "step :603 Loss: 0.162844\n",
      "step :604 Loss: 0.162595\n",
      "step :605 Loss: 0.0331914\n",
      "step :606 Loss: 0.210124\n",
      "step :607 Loss: 0.175287\n",
      "step :608 Loss: 0.158126\n",
      "step :609 Loss: 0.0850922\n",
      "step :610 Loss: 0.12781\n",
      "step :611 Loss: 0.235019\n",
      "step :612 Loss: 0.12003\n",
      "step :613 Loss: 0.0320884\n",
      "step :614 Loss: 0.154798\n",
      "step :615 Loss: 0.054621\n",
      "step :616 Loss: 0.0260241\n",
      "step :617 Loss: 0.150335\n",
      "step :618 Loss: 0.136345\n",
      "step :619 Loss: 0.15717\n",
      "step :620 Loss: 0.148902\n",
      "step :621 Loss: 0.1264\n",
      "step :622 Loss: 0.135222\n",
      "step :623 Loss: 0.0772856\n",
      "step :624 Loss: 0.130364\n",
      "step :625 Loss: 0.172337\n",
      "step :626 Loss: 0.160194\n",
      "step :627 Loss: 0.0895007\n",
      "step :628 Loss: 0.1765\n",
      "step :629 Loss: 0.0504484\n",
      "step :630 Loss: 0.138992\n",
      "step :631 Loss: 0.0761078\n",
      "step :632 Loss: 0.117351\n",
      "step :633 Loss: 0.13506\n",
      "step :634 Loss: 0.0879312\n",
      "step :635 Loss: 0.106673\n",
      "step :636 Loss: 0.318747\n",
      "step :637 Loss: 0.0281138\n",
      "step :638 Loss: 0.117049\n",
      "step :639 Loss: 0.0595896\n",
      "step :640 Loss: 0.0807306\n",
      "step :641 Loss: 0.181074\n",
      "step :642 Loss: 0.0886396\n",
      "step :643 Loss: 0.208649\n",
      "step :644 Loss: 0.167685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :645 Loss: 0.0617564\n",
      "step :646 Loss: 0.0926444\n",
      "step :647 Loss: 0.392855\n",
      "step :648 Loss: 0.105298\n",
      "step :649 Loss: 0.0658216\n",
      "step :650 Loss: 0.0264588\n",
      "step :651 Loss: 0.0988386\n",
      "step :652 Loss: 0.0974738\n",
      "step :653 Loss: 0.112532\n",
      "step :654 Loss: 0.120634\n",
      "step :655 Loss: 0.0917438\n",
      "step :656 Loss: 0.100884\n",
      "step :657 Loss: 0.10751\n",
      "step :658 Loss: 0.323577\n",
      "step :659 Loss: 0.22428\n",
      "step :660 Loss: 0.234202\n",
      "step :661 Loss: 0.160397\n",
      "step :662 Loss: 0.137665\n",
      "step :663 Loss: 0.158973\n",
      "step :664 Loss: 0.176385\n",
      "step :665 Loss: 0.113487\n",
      "step :666 Loss: 0.13486\n",
      "step :667 Loss: 0.0701581\n",
      "step :668 Loss: 0.114325\n",
      "step :669 Loss: 0.162177\n",
      "step :670 Loss: 0.0980463\n",
      "step :671 Loss: 0.22798\n",
      "step :672 Loss: 0.0574754\n",
      "step :673 Loss: 0.0680944\n",
      "step :674 Loss: 0.0482682\n",
      "step :675 Loss: 0.098393\n",
      "step :676 Loss: 0.0913149\n",
      "step :677 Loss: 0.140912\n",
      "step :678 Loss: 0.0990581\n",
      "step :679 Loss: 0.113615\n",
      "step :680 Loss: 0.151019\n",
      "step :681 Loss: 0.0470667\n",
      "step :682 Loss: 0.0155081\n",
      "step :683 Loss: 0.0677532\n",
      "step :684 Loss: 0.138984\n",
      "step :685 Loss: 0.10176\n",
      "step :686 Loss: 0.148803\n",
      "step :687 Loss: 0.0662804\n",
      "step :688 Loss: 0.163992\n",
      "step :689 Loss: 0.128371\n",
      "step :690 Loss: 0.174089\n",
      "step :691 Loss: 0.0403033\n",
      "step :692 Loss: 0.158142\n",
      "step :693 Loss: 0.110084\n",
      "step :694 Loss: 0.153447\n",
      "step :695 Loss: 0.188372\n",
      "step :696 Loss: 0.135602\n",
      "step :697 Loss: 0.0899626\n",
      "step :698 Loss: 0.106999\n",
      "step :699 Loss: 0.142199\n",
      "step :700 Loss: 0.0822683\n",
      "step :701 Loss: 0.161241\n",
      "step :702 Loss: 0.112907\n",
      "step :703 Loss: 0.0851378\n",
      "step :704 Loss: 0.139397\n",
      "step :705 Loss: 0.124\n",
      "step :706 Loss: 0.172385\n",
      "step :707 Loss: 0.0626578\n",
      "step :708 Loss: 0.149692\n",
      "step :709 Loss: 0.0510027\n",
      "step :710 Loss: 0.051529\n",
      "step :711 Loss: 0.160284\n",
      "step :712 Loss: 0.0853329\n",
      "step :713 Loss: 0.137487\n",
      "step :714 Loss: 0.145533\n",
      "step :715 Loss: 0.0561767\n",
      "step :716 Loss: 0.0314427\n",
      "step :717 Loss: 0.0797324\n",
      "step :718 Loss: 0.0167541\n",
      "step :719 Loss: 0.0483488\n",
      "step :720 Loss: 0.0903754\n",
      "step :721 Loss: 0.0930116\n",
      "step :722 Loss: 0.0293449\n",
      "step :723 Loss: 0.296752\n",
      "step :724 Loss: 0.117799\n",
      "step :725 Loss: 0.11657\n",
      "step :726 Loss: 0.0918007\n",
      "step :727 Loss: 0.129126\n",
      "step :728 Loss: 0.18327\n",
      "step :729 Loss: 0.068638\n",
      "step :730 Loss: 0.101568\n",
      "step :731 Loss: 0.132406\n",
      "step :732 Loss: 0.180744\n",
      "step :733 Loss: 0.110702\n",
      "step :734 Loss: 0.168833\n",
      "step :735 Loss: 0.0886053\n",
      "step :736 Loss: 0.127775\n",
      "step :737 Loss: 0.0364891\n",
      "step :738 Loss: 0.164675\n",
      "step :739 Loss: 0.172784\n",
      "step :740 Loss: 0.189085\n",
      "step :741 Loss: 0.0552682\n",
      "step :742 Loss: 0.0707315\n",
      "step :743 Loss: 0.0565131\n",
      "step :744 Loss: 0.162669\n",
      "step :745 Loss: 0.171155\n",
      "step :746 Loss: 0.119953\n",
      "step :747 Loss: 0.134547\n",
      "step :748 Loss: 0.173922\n",
      "step :749 Loss: 0.140895\n",
      "step :750 Loss: 0.0480855\n",
      "step :751 Loss: 0.191993\n",
      "step :752 Loss: 0.151131\n",
      "step :753 Loss: 0.16537\n",
      "step :754 Loss: 0.164778\n",
      "step :755 Loss: 0.0386563\n",
      "step :756 Loss: 0.167273\n",
      "step :757 Loss: 0.102182\n",
      "step :758 Loss: 0.0980356\n",
      "step :759 Loss: 0.0906365\n",
      "step :760 Loss: 0.0895494\n",
      "step :761 Loss: 0.0998369\n",
      "step :762 Loss: 0.0366947\n",
      "step :763 Loss: 0.222268\n",
      "step :764 Loss: 0.136744\n",
      "step :765 Loss: 0.249103\n",
      "step :766 Loss: 0.227326\n",
      "step :767 Loss: 0.0470156\n",
      "step :768 Loss: 0.0845117\n",
      "step :769 Loss: 0.157498\n",
      "step :770 Loss: 0.0661567\n",
      "step :771 Loss: 0.0634854\n",
      "step :772 Loss: 0.0773733\n",
      "step :773 Loss: 0.0841608\n",
      "step :774 Loss: 0.0901057\n",
      "step :775 Loss: 0.15826\n",
      "step :776 Loss: 0.17922\n",
      "step :777 Loss: 0.123503\n",
      "step :778 Loss: 0.085613\n",
      "step :779 Loss: 0.132602\n",
      "step :780 Loss: 0.18122\n",
      "step :781 Loss: 0.0670942\n",
      "step :782 Loss: 0.0589001\n",
      "step :783 Loss: 0.190777\n",
      "step :784 Loss: 0.18852\n",
      "step :785 Loss: 0.129554\n",
      "step :786 Loss: 0.107136\n",
      "step :787 Loss: 0.131852\n",
      "step :788 Loss: 0.161969\n",
      "step :789 Loss: 0.0716372\n",
      "step :790 Loss: 0.12221\n",
      "step :791 Loss: 0.0731928\n",
      "step :792 Loss: 0.15751\n",
      "step :793 Loss: 0.147743\n",
      "step :794 Loss: 0.2353\n",
      "step :795 Loss: 0.10586\n",
      "step :796 Loss: 0.110772\n",
      "step :797 Loss: 0.169359\n",
      "step :798 Loss: 0.0952073\n",
      "step :799 Loss: 0.081797\n",
      "step :800 Loss: 0.185184\n",
      "step :801 Loss: 0.127092\n",
      "step :802 Loss: 0.0822579\n",
      "step :803 Loss: 0.155781\n",
      "step :804 Loss: 0.2516\n",
      "step :805 Loss: 0.0437182\n",
      "step :806 Loss: 0.0750988\n",
      "step :807 Loss: 0.115369\n",
      "step :808 Loss: 0.122028\n",
      "step :809 Loss: 0.0898779\n",
      "step :810 Loss: 0.0745489\n",
      "step :811 Loss: 0.0677607\n",
      "step :812 Loss: 0.153423\n",
      "step :813 Loss: 0.135434\n",
      "step :814 Loss: 0.177712\n",
      "step :815 Loss: 0.106871\n",
      "step :816 Loss: 0.11462\n",
      "step :817 Loss: 0.115669\n",
      "step :818 Loss: 0.136601\n",
      "step :819 Loss: 0.166427\n",
      "step :820 Loss: 0.0887011\n",
      "step :821 Loss: 0.127379\n",
      "step :822 Loss: 0.112237\n",
      "step :823 Loss: 0.0781699\n",
      "step :824 Loss: 0.129083\n",
      "step :825 Loss: 0.189319\n",
      "step :826 Loss: 0.151665\n",
      "step :827 Loss: 0.112076\n",
      "step :828 Loss: 0.0895546\n",
      "step :829 Loss: 0.0623412\n",
      "step :830 Loss: 0.125611\n",
      "step :831 Loss: 0.0781523\n",
      "step :832 Loss: 0.118099\n",
      "step :833 Loss: 0.12243\n",
      "step :834 Loss: 0.0697941\n",
      "step :835 Loss: 0.150885\n",
      "step :836 Loss: 0.136554\n",
      "step :837 Loss: 0.180159\n",
      "step :838 Loss: 0.0850554\n",
      "step :839 Loss: 0.200589\n",
      "step :840 Loss: 0.0426159\n",
      "step :841 Loss: 0.0928577\n",
      "step :842 Loss: 0.153577\n",
      "step :843 Loss: 0.102055\n",
      "step :844 Loss: 0.109388\n",
      "step :845 Loss: 0.0506252\n",
      "step :846 Loss: 0.0181556\n",
      "step :847 Loss: 0.107054\n",
      "step :848 Loss: 0.130929\n",
      "step :849 Loss: 0.0487782\n",
      "step :850 Loss: 0.132214\n",
      "step :851 Loss: 0.0595185\n",
      "step :852 Loss: 0.0799974\n",
      "step :853 Loss: 0.255417\n",
      "step :854 Loss: 0.148369\n",
      "step :855 Loss: 0.145162\n",
      "step :856 Loss: 0.249076\n",
      "step :857 Loss: 0.0486471\n",
      "step :858 Loss: 0.230858\n",
      "step :859 Loss: 0.160737\n",
      "step :860 Loss: 0.193997\n",
      "step :861 Loss: 0.0814402\n",
      "step :862 Loss: 0.106354\n",
      "step :863 Loss: 0.156089\n",
      "step :864 Loss: 0.234868\n",
      "step :865 Loss: 0.117942\n",
      "step :866 Loss: 0.184114\n",
      "step :867 Loss: 0.154025\n",
      "step :868 Loss: 0.163724\n",
      "step :869 Loss: 0.130032\n",
      "step :870 Loss: 0.0930559\n",
      "step :871 Loss: 0.111462\n",
      "step :872 Loss: 0.151052\n",
      "step :873 Loss: 0.115004\n",
      "step :874 Loss: 0.1139\n",
      "step :875 Loss: 0.121362\n",
      "step :876 Loss: 0.182098\n",
      "step :877 Loss: 0.0622235\n",
      "step :878 Loss: 0.0903944\n",
      "step :879 Loss: 0.212089\n",
      "step :880 Loss: 0.153473\n",
      "step :881 Loss: 0.115311\n",
      "step :882 Loss: 0.0694473\n",
      "step :883 Loss: 0.155723\n",
      "step :884 Loss: 0.0753358\n",
      "step :885 Loss: 0.221155\n",
      "step :886 Loss: 0.185632\n",
      "step :887 Loss: 0.160611\n",
      "step :888 Loss: 0.131735\n",
      "step :889 Loss: 0.241103\n",
      "step :890 Loss: 0.16475\n",
      "step :891 Loss: 0.229347\n",
      "step :892 Loss: 0.21516\n",
      "step :893 Loss: 0.123161\n",
      "step :894 Loss: 0.240161\n",
      "step :895 Loss: 0.126251\n",
      "step :896 Loss: 0.147308\n",
      "step :897 Loss: 0.077312\n",
      "step :898 Loss: 0.120353\n",
      "step :899 Loss: 0.100181\n",
      "step :900 Loss: 0.113173\n",
      "step :901 Loss: 0.0976643\n",
      "step :902 Loss: 0.149802\n",
      "step :903 Loss: 0.0681751\n",
      "step :904 Loss: 0.0431526\n",
      "step :905 Loss: 0.135583\n",
      "step :906 Loss: 0.124369\n",
      "step :907 Loss: 0.155652\n",
      "step :908 Loss: 0.109564\n",
      "step :909 Loss: 0.147556\n",
      "step :910 Loss: 0.0904808\n",
      "step :911 Loss: 0.0797734\n",
      "step :912 Loss: 0.135856\n",
      "step :913 Loss: 0.0727528\n",
      "step :914 Loss: 0.0788932\n",
      "step :915 Loss: 0.276804\n",
      "step :916 Loss: 0.0744409\n",
      "step :917 Loss: 0.101982\n",
      "step :918 Loss: 0.0516082\n",
      "step :919 Loss: 0.143436\n",
      "step :920 Loss: 0.0532414\n",
      "step :921 Loss: 0.216034\n",
      "step :922 Loss: 0.1042\n",
      "step :923 Loss: 0.107169\n",
      "step :924 Loss: 0.135079\n",
      "step :925 Loss: 0.0887641\n",
      "step :926 Loss: 0.170177\n",
      "step :927 Loss: 0.202872\n",
      "step :928 Loss: 0.299724\n",
      "step :929 Loss: 0.184604\n",
      "step :930 Loss: 0.1066\n",
      "step :931 Loss: 0.0850733\n",
      "step :932 Loss: 0.143569\n",
      "step :933 Loss: 0.0657456\n",
      "step :934 Loss: 0.157675\n",
      "step :935 Loss: 0.0585986\n",
      "step :936 Loss: 0.169547\n",
      "step :937 Loss: 0.243549\n",
      "step :938 Loss: 0.0584249\n",
      "step :939 Loss: 0.117692\n",
      "step :940 Loss: 0.118823\n",
      "step :941 Loss: 0.246077\n",
      "step :942 Loss: 0.0222542\n",
      "step :943 Loss: 0.0836582\n",
      "step :944 Loss: 0.0564938\n",
      "step :945 Loss: 0.128422\n",
      "step :946 Loss: 0.114661\n",
      "step :947 Loss: 0.173484\n",
      "step :948 Loss: 0.187595\n",
      "step :949 Loss: 0.276159\n",
      "step :950 Loss: 0.118926\n",
      "step :951 Loss: 0.0832579\n",
      "step :952 Loss: 0.111291\n",
      "step :953 Loss: 0.110242\n",
      "step :954 Loss: 0.0795963\n",
      "step :955 Loss: 0.0310713\n",
      "step :956 Loss: 0.0531288\n",
      "step :957 Loss: 0.100226\n",
      "step :958 Loss: 0.124214\n",
      "step :959 Loss: 0.101493\n",
      "step :960 Loss: 0.175333\n",
      "step :961 Loss: 0.161901\n",
      "step :962 Loss: 0.0847687\n",
      "step :963 Loss: 0.367369\n",
      "step :964 Loss: 0.177299\n",
      "step :965 Loss: 0.180557\n",
      "step :966 Loss: 0.11946\n",
      "step :967 Loss: 0.286683\n",
      "step :968 Loss: 0.231106\n",
      "step :969 Loss: 0.269228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :970 Loss: 0.0837784\n",
      "step :971 Loss: 0.132962\n",
      "step :972 Loss: 0.222037\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn8s_final.ckpt\n",
      "step :973 Loss: 0.152095\n",
      "step :974 Loss: 0.132157\n",
      "step :975 Loss: 0.130765\n",
      "step :976 Loss: 0.151212\n",
      "step :977 Loss: 0.116721\n",
      "step :978 Loss: 0.0682985\n",
      "step :979 Loss: 0.102206\n",
      "step :980 Loss: 0.136084\n",
      "step :981 Loss: 0.0766607\n",
      "step :982 Loss: 0.103454\n",
      "step :983 Loss: 0.0395665\n",
      "step :984 Loss: 0.0827084\n",
      "step :985 Loss: 0.173528\n",
      "step :986 Loss: 0.114488\n",
      "step :987 Loss: 0.211213\n",
      "step :988 Loss: 0.193318\n",
      "step :989 Loss: 0.0935314\n",
      "step :990 Loss: 0.195075\n",
      "step :991 Loss: 0.132398\n",
      "step :992 Loss: 0.210732\n",
      "step :993 Loss: 0.105158\n",
      "step :994 Loss: 0.0640544\n",
      "step :995 Loss: 0.118787\n",
      "step :996 Loss: 0.150904\n",
      "step :997 Loss: 0.124493\n",
      "step :998 Loss: 0.0980743\n",
      "step :999 Loss: 0.0953046\n",
      "step :1000 Loss: 0.166581\n",
      "step :1001 Loss: 0.220533\n",
      "step :1002 Loss: 0.260649\n",
      "step :1003 Loss: 0.0229348\n",
      "step :1004 Loss: 0.097039\n",
      "step :1005 Loss: 0.0175893\n",
      "step :1006 Loss: 0.0892317\n",
      "step :1007 Loss: 0.0395677\n",
      "step :1008 Loss: 0.215015\n",
      "step :1009 Loss: 0.211359\n",
      "step :1010 Loss: 0.188501\n",
      "step :1011 Loss: 0.0552332\n",
      "step :1012 Loss: 0.199402\n",
      "step :1013 Loss: 0.195335\n",
      "step :1014 Loss: 0.141738\n",
      "step :1015 Loss: 0.126486\n",
      "step :1016 Loss: 0.0803272\n",
      "step :1017 Loss: 0.152437\n",
      "step :1018 Loss: 0.0958621\n",
      "step :1019 Loss: 0.149315\n",
      "step :1020 Loss: 0.111503\n",
      "step :1021 Loss: 0.180124\n",
      "step :1022 Loss: 0.0929944\n",
      "step :1023 Loss: 0.124171\n",
      "step :1024 Loss: 0.0760032\n",
      "step :1025 Loss: 0.153933\n",
      "step :1026 Loss: 0.0979685\n",
      "step :1027 Loss: 0.0533381\n",
      "step :1028 Loss: 0.152639\n",
      "step :1029 Loss: 0.118655\n",
      "step :1030 Loss: 0.180924\n",
      "step :1031 Loss: 0.155233\n",
      "step :1032 Loss: 0.229578\n",
      "step :1033 Loss: 0.170013\n",
      "step :1034 Loss: 0.0507853\n",
      "step :1035 Loss: 0.116943\n",
      "step :1036 Loss: 0.11309\n",
      "step :1037 Loss: 0.137089\n",
      "step :1038 Loss: 0.089394\n",
      "step :1039 Loss: 0.163557\n",
      "step :1040 Loss: 0.0963044\n",
      "step :1041 Loss: 0.119168\n",
      "step :1042 Loss: 0.192223\n",
      "step :1043 Loss: 0.161589\n",
      "step :1044 Loss: 0.12799\n",
      "step :1045 Loss: 0.117941\n",
      "step :1046 Loss: 0.041392\n",
      "step :1047 Loss: 0.16713\n",
      "step :1048 Loss: 0.102391\n",
      "step :1049 Loss: 0.108653\n",
      "step :1050 Loss: 0.159633\n",
      "step :1051 Loss: 0.13173\n",
      "step :1052 Loss: 0.146873\n",
      "step :1053 Loss: 0.112614\n",
      "step :1054 Loss: 0.149955\n",
      "step :1055 Loss: 0.160377\n",
      "step :1056 Loss: 0.0511563\n",
      "step :1057 Loss: 0.132897\n",
      "step :1058 Loss: 0.188924\n",
      "step :1059 Loss: 0.0503848\n",
      "step :1060 Loss: 0.0633449\n",
      "step :1061 Loss: 0.203342\n",
      "step :1062 Loss: 0.0945824\n",
      "step :1063 Loss: 0.157254\n",
      "step :1064 Loss: 0.213445\n",
      "step :1065 Loss: 0.0976033\n",
      "step :1066 Loss: 0.167488\n",
      "step :1067 Loss: 0.125807\n",
      "step :1068 Loss: 0.127501\n",
      "step :1069 Loss: 0.192278\n",
      "step :1070 Loss: 0.0953355\n",
      "step :1071 Loss: 0.0885357\n",
      "step :1072 Loss: 0.112307\n",
      "step :1073 Loss: 0.0306704\n",
      "step :1074 Loss: 0.112715\n",
      "step :1075 Loss: 0.112432\n",
      "step :1076 Loss: 0.0655754\n",
      "step :1077 Loss: 0.15831\n",
      "step :1078 Loss: 0.0692555\n",
      "step :1079 Loss: 0.0857098\n",
      "step :1080 Loss: 0.0780336\n",
      "step :1081 Loss: 0.10427\n",
      "step :1082 Loss: 0.0850287\n",
      "step :1083 Loss: 0.126407\n",
      "step :1084 Loss: 0.226048\n",
      "step :1085 Loss: 0.0845003\n",
      "step :1086 Loss: 0.151148\n",
      "step :1087 Loss: 0.161039\n",
      "step :1088 Loss: 0.0928312\n",
      "step :1089 Loss: 0.0897764\n",
      "step :1090 Loss: 0.101633\n",
      "step :1091 Loss: 0.386646\n",
      "step :1092 Loss: 0.148034\n",
      "step :1093 Loss: 0.0824222\n",
      "step :1094 Loss: 0.0795897\n",
      "step :1095 Loss: 0.17708\n",
      "step :1096 Loss: 0.149155\n",
      "step :1097 Loss: 0.234728\n",
      "step :1098 Loss: 0.0833127\n",
      "step :1099 Loss: 0.143349\n",
      "step :1100 Loss: 0.15381\n",
      "step :1101 Loss: 0.093318\n",
      "step :1102 Loss: 0.115542\n",
      "step :1103 Loss: 0.0909573\n",
      "step :1104 Loss: 0.15676\n",
      "step :1105 Loss: 0.167361\n",
      "step :1106 Loss: 0.23928\n",
      "step :1107 Loss: 0.248459\n",
      "step :1108 Loss: 0.15193\n",
      "step :1109 Loss: 0.130415\n",
      "step :1110 Loss: 0.121223\n",
      "step :1111 Loss: 0.104503\n",
      "step :1112 Loss: 0.104358\n",
      "step :1113 Loss: 0.0868814\n",
      "step :1114 Loss: 0.0519447\n",
      "step :1115 Loss: 0.0792813\n",
      "step :1116 Loss: 0.0808061\n",
      "step :1117 Loss: 0.234004\n",
      "step :1118 Loss: 0.0825724\n",
      "step :1119 Loss: 0.131709\n",
      "step :1120 Loss: 0.242123\n",
      "step :1121 Loss: 0.13002\n",
      "step :1122 Loss: 0.115422\n",
      "step :1123 Loss: 0.0308596\n",
      "step :1124 Loss: 0.092228\n",
      "step :1125 Loss: 0.264986\n",
      "step :1126 Loss: 0.133735\n",
      "step :1127 Loss: 0.157368\n",
      "step :1128 Loss: 0.0704362\n",
      "step :1129 Loss: 0.162179\n",
      "step :1130 Loss: 0.0941979\n",
      "step :1131 Loss: 0.122585\n",
      "step :1132 Loss: 0.131814\n",
      "step :1133 Loss: 0.140312\n",
      "step :1134 Loss: 0.114407\n",
      "step :1135 Loss: 0.173254\n",
      "step :1136 Loss: 0.134007\n",
      "step :1137 Loss: 0.0825253\n",
      "step :1138 Loss: 0.160267\n",
      "step :1139 Loss: 0.0997102\n",
      "step :1140 Loss: 0.154762\n",
      "step :1141 Loss: 0.0970338\n",
      "step :1142 Loss: 0.107678\n",
      "step :1143 Loss: 0.123887\n",
      "step :1144 Loss: 0.103317\n",
      "step :1145 Loss: 0.126896\n",
      "step :1146 Loss: 0.101513\n",
      "step :1147 Loss: 0.0521795\n",
      "step :1148 Loss: 0.0913775\n",
      "step :1149 Loss: 0.0893687\n",
      "step :1150 Loss: 0.0538915\n",
      "step :1151 Loss: 0.165544\n",
      "step :1152 Loss: 0.131115\n",
      "step :1153 Loss: 0.113993\n",
      "step :1154 Loss: 0.0475706\n",
      "step :1155 Loss: 0.140871\n",
      "step :1156 Loss: 0.10169\n",
      "step :1157 Loss: 0.0589678\n",
      "step :1158 Loss: 0.150003\n",
      "step :1159 Loss: 0.222256\n",
      "step :1160 Loss: 0.0556075\n",
      "step :1161 Loss: 0.210331\n",
      "step :1162 Loss: 0.0854357\n",
      "step :1163 Loss: 0.294358\n",
      "step :1164 Loss: 0.115577\n",
      "step :1165 Loss: 0.188441\n",
      "step :1166 Loss: 0.115668\n",
      "step :1167 Loss: 0.0936494\n",
      "step :1168 Loss: 0.167755\n",
      "step :1169 Loss: 0.0829325\n",
      "step :1170 Loss: 0.244504\n",
      "step :1171 Loss: 0.146406\n",
      "step :1172 Loss: 0.196089\n",
      "step :1173 Loss: 0.155915\n",
      "step :1174 Loss: 0.159577\n",
      "step :1175 Loss: 0.0993434\n",
      "step :1176 Loss: 0.11043\n",
      "step :1177 Loss: 0.180808\n",
      "step :1178 Loss: 0.0977557\n",
      "step :1179 Loss: 0.115649\n",
      "step :1180 Loss: 0.0623056\n",
      "step :1181 Loss: 0.123507\n",
      "step :1182 Loss: 0.0552228\n",
      "step :1183 Loss: 0.309662\n",
      "step :1184 Loss: 0.0728696\n",
      "step :1185 Loss: 0.108103\n",
      "step :1186 Loss: 0.0217821\n",
      "step :1187 Loss: 0.111941\n",
      "step :1188 Loss: 0.0565624\n",
      "step :1189 Loss: 0.145255\n",
      "step :1190 Loss: 0.202079\n",
      "step :1191 Loss: 0.0920591\n",
      "step :1192 Loss: 0.1121\n",
      "step :1193 Loss: 0.281134\n",
      "step :1194 Loss: 0.0788574\n",
      "step :1195 Loss: 0.0997052\n",
      "step :1196 Loss: 0.0336872\n",
      "step :1197 Loss: 0.116956\n",
      "step :1198 Loss: 0.100157\n",
      "step :1199 Loss: 0.172471\n",
      "step :1200 Loss: 0.15826\n",
      "step :1201 Loss: 0.108165\n",
      "step :1202 Loss: 0.169121\n",
      "step :1203 Loss: 0.126397\n",
      "step :1204 Loss: 0.119966\n",
      "step :1205 Loss: 0.0541358\n",
      "step :1206 Loss: 0.0377966\n",
      "step :1207 Loss: 0.053614\n",
      "step :1208 Loss: 0.13529\n",
      "step :1209 Loss: 0.313847\n",
      "step :1210 Loss: 0.107796\n",
      "step :1211 Loss: 0.0728002\n",
      "step :1212 Loss: 0.168114\n",
      "step :1213 Loss: 0.0546421\n",
      "step :1214 Loss: 0.140685\n",
      "step :1215 Loss: 0.0914176\n",
      "step :1216 Loss: 0.213085\n",
      "step :1217 Loss: 0.0788869\n",
      "step :1218 Loss: 0.135963\n",
      "step :1219 Loss: 0.12272\n",
      "step :1220 Loss: 0.129135\n",
      "step :1221 Loss: 0.0546479\n",
      "step :1222 Loss: 0.0236765\n",
      "step :1223 Loss: 0.0422062\n",
      "step :1224 Loss: 0.137413\n",
      "step :1225 Loss: 0.0800047\n",
      "step :1226 Loss: 0.147432\n",
      "step :1227 Loss: 0.161956\n",
      "step :1228 Loss: 0.210314\n",
      "step :1229 Loss: 0.110893\n",
      "step :1230 Loss: 0.141023\n",
      "step :1231 Loss: 0.145534\n",
      "step :1232 Loss: 0.169714\n",
      "step :1233 Loss: 0.0709416\n",
      "step :1234 Loss: 0.108598\n",
      "step :1235 Loss: 0.141904\n",
      "step :1236 Loss: 0.0944853\n",
      "step :1237 Loss: 0.0530668\n",
      "step :1238 Loss: 0.103885\n",
      "step :1239 Loss: 0.224276\n",
      "step :1240 Loss: 0.162863\n",
      "step :1241 Loss: 0.192325\n",
      "step :1242 Loss: 0.0995886\n",
      "step :1243 Loss: 0.295651\n",
      "step :1244 Loss: 0.0546843\n",
      "step :1245 Loss: 0.138342\n",
      "step :1246 Loss: 0.0331984\n",
      "step :1247 Loss: 0.0398882\n",
      "step :1248 Loss: 0.218741\n",
      "step :1249 Loss: 0.212392\n",
      "step :1250 Loss: 0.120587\n",
      "step :1251 Loss: 0.135505\n",
      "step :1252 Loss: 0.265142\n",
      "step :1253 Loss: 0.142979\n",
      "step :1254 Loss: 0.112358\n",
      "step :1255 Loss: 0.111884\n",
      "step :1256 Loss: 0.181233\n",
      "step :1257 Loss: 0.116088\n",
      "step :1258 Loss: 0.0655495\n",
      "step :1259 Loss: 0.142099\n",
      "step :1260 Loss: 0.193815\n",
      "step :1261 Loss: 0.145485\n",
      "step :1262 Loss: 0.124519\n",
      "step :1263 Loss: 0.13497\n",
      "step :1264 Loss: 0.149759\n",
      "step :1265 Loss: 0.0295427\n",
      "step :1266 Loss: 0.0771734\n",
      "step :1267 Loss: 0.0672704\n",
      "step :1268 Loss: 0.172488\n",
      "step :1269 Loss: 0.074836\n",
      "step :1270 Loss: 0.0538565\n",
      "step :1271 Loss: 0.099485\n",
      "step :1272 Loss: 0.136192\n",
      "step :1273 Loss: 0.0779947\n",
      "step :1274 Loss: 0.183621\n",
      "step :1275 Loss: 0.177865\n",
      "step :1276 Loss: 0.0616523\n",
      "step :1277 Loss: 0.102752\n",
      "step :1278 Loss: 0.126377\n",
      "step :1279 Loss: 0.12209\n",
      "step :1280 Loss: 0.177991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :1281 Loss: 0.103953\n",
      "step :1282 Loss: 0.136192\n",
      "step :1283 Loss: 0.258043\n",
      "step :1284 Loss: 0.230416\n",
      "step :1285 Loss: 0.124164\n",
      "step :1286 Loss: 0.150596\n",
      "step :1287 Loss: 0.145064\n",
      "step :1288 Loss: 0.1598\n",
      "step :1289 Loss: 0.158132\n",
      "step :1290 Loss: 0.163492\n",
      "step :1291 Loss: 0.110396\n",
      "step :1292 Loss: 0.216246\n",
      "step :1293 Loss: 0.152851\n",
      "step :1294 Loss: 0.0903331\n",
      "step :1295 Loss: 0.138666\n",
      "step :1296 Loss: 0.140571\n",
      "step :1297 Loss: 0.0736295\n",
      "step :1298 Loss: 0.140802\n",
      "step :1299 Loss: 0.167558\n",
      "step :1300 Loss: 0.0770572\n",
      "step :1301 Loss: 0.145615\n",
      "step :1302 Loss: 0.135288\n",
      "step :1303 Loss: 0.0974527\n",
      "step :1304 Loss: 0.13711\n",
      "step :1305 Loss: 0.0715674\n",
      "step :1306 Loss: 0.138077\n",
      "step :1307 Loss: 0.136125\n",
      "step :1308 Loss: 0.0930995\n",
      "step :1309 Loss: 0.138643\n",
      "step :1310 Loss: 0.121257\n",
      "step :1311 Loss: 0.148059\n",
      "step :1312 Loss: 0.15063\n",
      "step :1313 Loss: 0.184187\n",
      "step :1314 Loss: 0.135883\n",
      "step :1315 Loss: 0.137403\n",
      "step :1316 Loss: 0.108983\n",
      "step :1317 Loss: 0.117887\n",
      "step :1318 Loss: 0.143323\n",
      "step :1319 Loss: 0.0714075\n",
      "step :1320 Loss: 0.0795162\n",
      "step :1321 Loss: 0.108104\n",
      "step :1322 Loss: 0.153032\n",
      "step :1323 Loss: 0.0724025\n",
      "step :1324 Loss: 0.0894634\n",
      "step :1325 Loss: 0.105308\n",
      "step :1326 Loss: 0.176951\n",
      "step :1327 Loss: 0.1168\n",
      "step :1328 Loss: 0.0663948\n",
      "step :1329 Loss: 0.204312\n",
      "step :1330 Loss: 0.172644\n",
      "step :1331 Loss: 0.184824\n",
      "step :1332 Loss: 0.0965798\n",
      "step :1333 Loss: 0.135058\n",
      "step :1334 Loss: 0.103938\n",
      "step :1335 Loss: 0.118118\n",
      "step :1336 Loss: 0.206386\n",
      "step :1337 Loss: 0.142479\n",
      "step :1338 Loss: 0.167049\n",
      "step :1339 Loss: 0.157439\n",
      "step :1340 Loss: 0.136422\n",
      "step :1341 Loss: 0.117391\n",
      "step :1342 Loss: 0.0901041\n",
      "step :1343 Loss: 0.137463\n",
      "step :1344 Loss: 0.0529851\n",
      "step :1345 Loss: 0.295732\n",
      "step :1346 Loss: 0.119303\n",
      "step :1347 Loss: 0.0845629\n",
      "step :1348 Loss: 0.165147\n",
      "step :1349 Loss: 0.0738473\n",
      "step :1350 Loss: 0.150776\n",
      "step :1351 Loss: 0.145803\n",
      "step :1352 Loss: 0.0377311\n",
      "step :1353 Loss: 0.132702\n",
      "step :1354 Loss: 0.134166\n",
      "step :1355 Loss: 0.230861\n",
      "step :1356 Loss: 0.224631\n",
      "step :1357 Loss: 0.168339\n",
      "step :1358 Loss: 0.0970543\n",
      "step :1359 Loss: 0.195078\n",
      "step :1360 Loss: 0.192803\n",
      "step :1361 Loss: 0.118255\n",
      "step :1362 Loss: 0.0720757\n",
      "step :1363 Loss: 0.104975\n",
      "step :1364 Loss: 0.168412\n",
      "step :1365 Loss: 0.0592117\n",
      "step :1366 Loss: 0.0771091\n",
      "step :1367 Loss: 0.0831678\n",
      "step :1368 Loss: 0.0821171\n",
      "step :1369 Loss: 0.11729\n",
      "step :1370 Loss: 0.214928\n",
      "step :1371 Loss: 0.0948932\n",
      "step :1372 Loss: 0.157934\n",
      "step :1373 Loss: 0.105404\n",
      "step :1374 Loss: 0.192737\n",
      "step :1375 Loss: 0.121813\n",
      "step :1376 Loss: 0.102769\n",
      "step :1377 Loss: 0.0899426\n",
      "step :1378 Loss: 0.149563\n",
      "step :1379 Loss: 0.13502\n",
      "step :1380 Loss: 0.176792\n",
      "step :1381 Loss: 0.132356\n",
      "step :1382 Loss: 0.158732\n",
      "step :1383 Loss: 0.0856248\n",
      "step :1384 Loss: 0.166598\n",
      "step :1385 Loss: 0.141923\n",
      "step :1386 Loss: 0.142414\n",
      "step :1387 Loss: 0.179243\n",
      "step :1388 Loss: 0.146784\n",
      "step :1389 Loss: 0.0683984\n",
      "step :1390 Loss: 0.126798\n",
      "step :1391 Loss: 0.0631259\n",
      "step :1392 Loss: 0.114223\n",
      "step :1393 Loss: 0.217338\n",
      "step :1394 Loss: 0.0859578\n",
      "step :1395 Loss: 0.0580916\n",
      "step :1396 Loss: 0.0447699\n",
      "step :1397 Loss: 0.124694\n",
      "step :1398 Loss: 0.130216\n",
      "step :1399 Loss: 0.161158\n",
      "step :1400 Loss: 0.0993499\n",
      "step :1401 Loss: 0.184762\n",
      "step :1402 Loss: 0.119319\n",
      "step :1403 Loss: 0.0886954\n",
      "step :1404 Loss: 0.0462889\n",
      "step :1405 Loss: 0.140386\n",
      "step :1406 Loss: 0.0700343\n",
      "step :1407 Loss: 0.119245\n",
      "step :1408 Loss: 0.0665192\n",
      "step :1409 Loss: 0.164413\n",
      "step :1410 Loss: 0.14392\n",
      "step :1411 Loss: 0.266365\n",
      "step :1412 Loss: 0.143061\n",
      "step :1413 Loss: 0.210303\n",
      "step :1414 Loss: 0.0813894\n",
      "step :1415 Loss: 0.0931687\n",
      "step :1416 Loss: 0.171741\n",
      "step :1417 Loss: 0.0809059\n",
      "step :1418 Loss: 0.170909\n",
      "step :1419 Loss: 0.217088\n",
      "step :1420 Loss: 0.131717\n",
      "step :1421 Loss: 0.202303\n",
      "step :1422 Loss: 0.0431931\n",
      "step :1423 Loss: 0.144117\n",
      "step :1424 Loss: 0.131717\n",
      "step :1425 Loss: 0.11106\n",
      "step :1426 Loss: 0.246043\n",
      "step :1427 Loss: 0.206431\n",
      "step :1428 Loss: 0.182179\n",
      "step :1429 Loss: 0.140277\n",
      "step :1430 Loss: 0.192521\n",
      "step :1431 Loss: 0.0712782\n",
      "step :1432 Loss: 0.0933277\n",
      "step :1433 Loss: 0.15482\n",
      "step :1434 Loss: 0.141793\n",
      "step :1435 Loss: 0.115064\n",
      "step :1436 Loss: 0.104179\n",
      "step :1437 Loss: 0.0796221\n",
      "step :1438 Loss: 0.197369\n",
      "step :1439 Loss: 0.0986689\n",
      "step :1440 Loss: 0.22273\n",
      "step :1441 Loss: 0.0646569\n",
      "step :1442 Loss: 0.128482\n",
      "step :1443 Loss: 0.0943937\n",
      "step :1444 Loss: 0.0959166\n",
      "step :1445 Loss: 0.144441\n",
      "step :1446 Loss: 0.349418\n",
      "step :1447 Loss: 0.0886384\n",
      "step :1448 Loss: 0.199812\n",
      "step :1449 Loss: 0.11044\n",
      "step :1450 Loss: 0.125043\n",
      "step :1451 Loss: 0.0676749\n",
      "step :1452 Loss: 0.0898216\n",
      "step :1453 Loss: 0.115659\n",
      "step :1454 Loss: 0.0997405\n",
      "step :1455 Loss: 0.0962395\n",
      "step :1456 Loss: 0.194919\n",
      "step :1457 Loss: 0.0716168\n",
      "step :1458 Loss: 0.110328\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn8s_final.ckpt\n",
      "step :1459 Loss: 0.0882867\n",
      "step :1460 Loss: 0.105925\n",
      "step :1461 Loss: 0.172603\n",
      "step :1462 Loss: 0.149545\n",
      "step :1463 Loss: 0.12347\n",
      "step :1464 Loss: 0.181386\n",
      "step :1465 Loss: 0.118613\n",
      "step :1466 Loss: 0.123515\n",
      "step :1467 Loss: 0.140043\n",
      "step :1468 Loss: 0.087952\n",
      "step :1469 Loss: 0.0382995\n",
      "step :1470 Loss: 0.0259443\n",
      "step :1471 Loss: 0.110749\n",
      "step :1472 Loss: 0.0795909\n",
      "step :1473 Loss: 0.29237\n",
      "step :1474 Loss: 0.064311\n",
      "step :1475 Loss: 0.121308\n",
      "step :1476 Loss: 0.0466938\n",
      "step :1477 Loss: 0.140063\n",
      "step :1478 Loss: 0.128803\n",
      "step :1479 Loss: 0.185874\n",
      "step :1480 Loss: 0.122584\n",
      "step :1481 Loss: 0.109199\n",
      "step :1482 Loss: 0.0281383\n",
      "step :1483 Loss: 0.0476833\n",
      "step :1484 Loss: 0.0411739\n",
      "step :1485 Loss: 0.155739\n",
      "step :1486 Loss: 0.0360978\n",
      "step :1487 Loss: 0.145524\n",
      "step :1488 Loss: 0.0546881\n",
      "step :1489 Loss: 0.157076\n",
      "step :1490 Loss: 0.0190418\n",
      "step :1491 Loss: 0.237846\n",
      "step :1492 Loss: 0.0964268\n",
      "step :1493 Loss: 0.108355\n",
      "step :1494 Loss: 0.148192\n",
      "step :1495 Loss: 0.0700061\n",
      "step :1496 Loss: 0.0854894\n",
      "step :1497 Loss: 0.0438127\n",
      "step :1498 Loss: 0.130965\n",
      "step :1499 Loss: 0.157478\n",
      "step :1500 Loss: 0.159691\n",
      "step :1501 Loss: 0.117128\n",
      "step :1502 Loss: 0.0744251\n",
      "step :1503 Loss: 0.174054\n",
      "step :1504 Loss: 0.126479\n",
      "step :1505 Loss: 0.0338689\n",
      "step :1506 Loss: 0.144719\n",
      "step :1507 Loss: 0.163411\n",
      "step :1508 Loss: 0.0746685\n",
      "step :1509 Loss: 0.0891777\n",
      "step :1510 Loss: 0.0980501\n",
      "step :1511 Loss: 0.19038\n",
      "step :1512 Loss: 0.0570609\n",
      "step :1513 Loss: 0.0698573\n",
      "step :1514 Loss: 0.193075\n",
      "step :1515 Loss: 0.19399\n",
      "step :1516 Loss: 0.0774497\n",
      "step :1517 Loss: 0.118701\n",
      "step :1518 Loss: 0.0813453\n",
      "step :1519 Loss: 0.200042\n",
      "step :1520 Loss: 0.141975\n",
      "step :1521 Loss: 0.202877\n",
      "step :1522 Loss: 0.0995241\n",
      "step :1523 Loss: 0.0962058\n",
      "step :1524 Loss: 0.0999229\n",
      "step :1525 Loss: 0.092645\n",
      "step :1526 Loss: 0.149202\n",
      "step :1527 Loss: 0.13671\n",
      "step :1528 Loss: 0.195644\n",
      "step :1529 Loss: 0.0397809\n",
      "step :1530 Loss: 0.0866051\n",
      "step :1531 Loss: 0.0495178\n",
      "step :1532 Loss: 0.302255\n",
      "step :1533 Loss: 0.0735152\n",
      "step :1534 Loss: 0.100291\n",
      "step :1535 Loss: 0.131474\n",
      "step :1536 Loss: 0.0681868\n",
      "step :1537 Loss: 0.101141\n",
      "step :1538 Loss: 0.0662423\n",
      "step :1539 Loss: 0.0589274\n",
      "step :1540 Loss: 0.0891039\n",
      "step :1541 Loss: 0.15828\n",
      "step :1542 Loss: 0.253357\n",
      "step :1543 Loss: 0.0813563\n",
      "step :1544 Loss: 0.124404\n",
      "step :1545 Loss: 0.116795\n",
      "step :1546 Loss: 0.100665\n",
      "step :1547 Loss: 0.103783\n",
      "step :1548 Loss: 0.0966044\n",
      "step :1549 Loss: 0.132066\n",
      "step :1550 Loss: 0.077647\n",
      "step :1551 Loss: 0.0910419\n",
      "step :1552 Loss: 0.191662\n",
      "step :1553 Loss: 0.108806\n",
      "step :1554 Loss: 0.162314\n",
      "step :1555 Loss: 0.11269\n",
      "step :1556 Loss: 0.182557\n",
      "step :1557 Loss: 0.091278\n",
      "step :1558 Loss: 0.121068\n",
      "step :1559 Loss: 0.0530753\n",
      "step :1560 Loss: 0.128238\n",
      "step :1561 Loss: 0.104871\n",
      "step :1562 Loss: 0.0357057\n",
      "step :1563 Loss: 0.188665\n",
      "step :1564 Loss: 0.193112\n",
      "step :1565 Loss: 0.0495732\n",
      "step :1566 Loss: 0.0753707\n",
      "step :1567 Loss: 0.188955\n",
      "step :1568 Loss: 0.0504143\n",
      "step :1569 Loss: 0.077971\n",
      "step :1570 Loss: 0.0934848\n",
      "step :1571 Loss: 0.138742\n",
      "step :1572 Loss: 0.277987\n",
      "step :1573 Loss: 0.157045\n",
      "step :1574 Loss: 0.0489938\n",
      "step :1575 Loss: 0.0882051\n",
      "step :1576 Loss: 0.0888465\n",
      "step :1577 Loss: 0.0804887\n",
      "step :1578 Loss: 0.136555\n",
      "step :1579 Loss: 0.134307\n",
      "step :1580 Loss: 0.0916412\n",
      "step :1581 Loss: 0.1294\n",
      "step :1582 Loss: 0.107898\n",
      "step :1583 Loss: 0.0376239\n",
      "step :1584 Loss: 0.0941213\n",
      "step :1585 Loss: 0.134294\n",
      "step :1586 Loss: 0.147385\n",
      "step :1587 Loss: 0.151317\n",
      "step :1588 Loss: 0.150713\n",
      "step :1589 Loss: 0.0561089\n",
      "step :1590 Loss: 0.152167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :1591 Loss: 0.0747982\n",
      "step :1592 Loss: 0.0769119\n",
      "step :1593 Loss: 0.123229\n",
      "step :1594 Loss: 0.107513\n",
      "step :1595 Loss: 0.149902\n",
      "step :1596 Loss: 0.110943\n",
      "step :1597 Loss: 0.144352\n",
      "step :1598 Loss: 0.0986694\n",
      "step :1599 Loss: 0.0729545\n",
      "step :1600 Loss: 0.169871\n",
      "step :1601 Loss: 0.0167788\n",
      "step :1602 Loss: 0.1657\n",
      "step :1603 Loss: 0.11362\n",
      "step :1604 Loss: 0.045865\n",
      "step :1605 Loss: 0.11849\n",
      "step :1606 Loss: 0.145366\n",
      "step :1607 Loss: 0.193167\n",
      "step :1608 Loss: 0.0794194\n",
      "step :1609 Loss: 0.175328\n",
      "step :1610 Loss: 0.166751\n",
      "step :1611 Loss: 0.264926\n",
      "step :1612 Loss: 0.0962313\n",
      "step :1613 Loss: 0.157164\n",
      "step :1614 Loss: 0.177723\n",
      "step :1615 Loss: 0.110814\n",
      "step :1616 Loss: 0.144102\n",
      "step :1617 Loss: 0.0937548\n",
      "step :1618 Loss: 0.176764\n",
      "step :1619 Loss: 0.097874\n",
      "step :1620 Loss: 0.0856891\n",
      "step :1621 Loss: 0.125061\n",
      "step :1622 Loss: 0.162488\n",
      "step :1623 Loss: 0.187503\n",
      "step :1624 Loss: 0.148682\n",
      "step :1625 Loss: 0.0882031\n",
      "step :1626 Loss: 0.149281\n",
      "step :1627 Loss: 0.116653\n",
      "step :1628 Loss: 0.118226\n",
      "step :1629 Loss: 0.0766906\n",
      "step :1630 Loss: 0.0396489\n",
      "step :1631 Loss: 0.146997\n",
      "step :1632 Loss: 0.142062\n",
      "step :1633 Loss: 0.0893224\n",
      "step :1634 Loss: 0.153935\n",
      "step :1635 Loss: 0.156942\n",
      "step :1636 Loss: 0.182945\n",
      "step :1637 Loss: 0.146665\n",
      "step :1638 Loss: 0.164546\n",
      "step :1639 Loss: 0.124028\n",
      "step :1640 Loss: 0.102469\n",
      "step :1641 Loss: 0.149312\n",
      "step :1642 Loss: 0.135338\n",
      "step :1643 Loss: 0.184708\n",
      "step :1644 Loss: 0.126607\n",
      "step :1645 Loss: 0.15268\n",
      "step :1646 Loss: 0.098604\n",
      "step :1647 Loss: 0.167751\n",
      "step :1648 Loss: 0.0889926\n",
      "step :1649 Loss: 0.0593994\n",
      "step :1650 Loss: 0.0897308\n",
      "step :1651 Loss: 0.196921\n",
      "step :1652 Loss: 0.123271\n",
      "step :1653 Loss: 0.0468164\n",
      "step :1654 Loss: 0.113636\n",
      "step :1655 Loss: 0.103078\n",
      "step :1656 Loss: 0.189615\n",
      "step :1657 Loss: 0.111077\n",
      "step :1658 Loss: 0.317943\n",
      "step :1659 Loss: 0.0269326\n",
      "step :1660 Loss: 0.0363016\n",
      "step :1661 Loss: 0.114889\n",
      "step :1662 Loss: 0.134584\n",
      "step :1663 Loss: 0.148344\n",
      "step :1664 Loss: 0.194586\n",
      "step :1665 Loss: 0.100024\n",
      "step :1666 Loss: 0.110682\n",
      "step :1667 Loss: 0.121466\n",
      "step :1668 Loss: 0.145673\n",
      "step :1669 Loss: 0.146431\n",
      "step :1670 Loss: 0.0386583\n",
      "step :1671 Loss: 0.150388\n",
      "step :1672 Loss: 0.0837834\n",
      "step :1673 Loss: 0.0624842\n",
      "step :1674 Loss: 0.116938\n",
      "step :1675 Loss: 0.111672\n",
      "step :1676 Loss: 0.149471\n",
      "step :1677 Loss: 0.208359\n",
      "step :1678 Loss: 0.0859125\n",
      "step :1679 Loss: 0.154519\n",
      "step :1680 Loss: 0.190361\n",
      "step :1681 Loss: 0.294831\n",
      "step :1682 Loss: 0.0980361\n",
      "step :1683 Loss: 0.151873\n",
      "step :1684 Loss: 0.227253\n",
      "step :1685 Loss: 0.177683\n",
      "step :1686 Loss: 0.0794777\n",
      "step :1687 Loss: 0.0332093\n",
      "step :1688 Loss: 0.181371\n",
      "step :1689 Loss: 0.136628\n",
      "step :1690 Loss: 0.0606364\n",
      "step :1691 Loss: 0.0490028\n",
      "step :1692 Loss: 0.170458\n",
      "step :1693 Loss: 0.116613\n",
      "step :1694 Loss: 0.111538\n",
      "step :1695 Loss: 0.220514\n",
      "step :1696 Loss: 0.0637784\n",
      "step :1697 Loss: 0.171262\n",
      "step :1698 Loss: 0.141137\n",
      "step :1699 Loss: 0.0750782\n",
      "step :1700 Loss: 0.104152\n",
      "step :1701 Loss: 0.135743\n",
      "step :1702 Loss: 0.0914274\n",
      "step :1703 Loss: 0.136583\n",
      "step :1704 Loss: 0.190724\n",
      "step :1705 Loss: 0.159036\n",
      "step :1706 Loss: 0.152729\n",
      "step :1707 Loss: 0.0958242\n",
      "step :1708 Loss: 0.115086\n",
      "step :1709 Loss: 0.184051\n",
      "step :1710 Loss: 0.257188\n",
      "step :1711 Loss: 0.0943561\n",
      "step :1712 Loss: 0.1285\n",
      "step :1713 Loss: 0.138094\n",
      "step :1714 Loss: 0.0933209\n",
      "step :1715 Loss: 0.246355\n",
      "step :1716 Loss: 0.0841857\n",
      "step :1717 Loss: 0.170243\n",
      "step :1718 Loss: 0.175293\n",
      "step :1719 Loss: 0.0920588\n",
      "step :1720 Loss: 0.139672\n",
      "step :1721 Loss: 0.061903\n",
      "step :1722 Loss: 0.135309\n",
      "step :1723 Loss: 0.10303\n",
      "step :1724 Loss: 0.166508\n",
      "step :1725 Loss: 0.103792\n",
      "step :1726 Loss: 0.271109\n",
      "step :1727 Loss: 0.124804\n",
      "step :1728 Loss: 0.05508\n",
      "step :1729 Loss: 0.081757\n",
      "step :1730 Loss: 0.0868228\n",
      "step :1731 Loss: 0.0906105\n",
      "step :1732 Loss: 0.0929736\n",
      "step :1733 Loss: 0.117695\n",
      "step :1734 Loss: 0.14114\n",
      "step :1735 Loss: 0.195957\n",
      "step :1736 Loss: 0.124539\n",
      "step :1737 Loss: 0.0602097\n",
      "step :1738 Loss: 0.193207\n",
      "step :1739 Loss: 0.13545\n",
      "step :1740 Loss: 0.15779\n",
      "step :1741 Loss: 0.0713353\n",
      "step :1742 Loss: 0.132822\n",
      "step :1743 Loss: 0.0513607\n",
      "step :1744 Loss: 0.138284\n",
      "step :1745 Loss: 0.0466364\n",
      "step :1746 Loss: 0.114807\n",
      "step :1747 Loss: 0.0871531\n",
      "step :1748 Loss: 0.18049\n",
      "step :1749 Loss: 0.189422\n",
      "step :1750 Loss: 0.149925\n",
      "step :1751 Loss: 0.212265\n",
      "step :1752 Loss: 0.16118\n",
      "step :1753 Loss: 0.195512\n",
      "step :1754 Loss: 0.152265\n",
      "step :1755 Loss: 0.0516257\n",
      "step :1756 Loss: 0.139454\n",
      "step :1757 Loss: 0.0534371\n",
      "step :1758 Loss: 0.0457451\n",
      "step :1759 Loss: 0.0574793\n",
      "step :1760 Loss: 0.196341\n",
      "step :1761 Loss: 0.0494498\n",
      "step :1762 Loss: 0.163438\n",
      "step :1763 Loss: 0.101334\n",
      "step :1764 Loss: 0.160874\n",
      "step :1765 Loss: 0.148058\n",
      "step :1766 Loss: 0.0416196\n",
      "step :1767 Loss: 0.139641\n",
      "step :1768 Loss: 0.102787\n",
      "step :1769 Loss: 0.230944\n",
      "step :1770 Loss: 0.0799202\n",
      "step :1771 Loss: 0.0634851\n",
      "step :1772 Loss: 0.13314\n",
      "step :1773 Loss: 0.0558482\n",
      "step :1774 Loss: 0.083716\n",
      "step :1775 Loss: 0.131004\n",
      "step :1776 Loss: 0.221262\n",
      "step :1777 Loss: 0.0982966\n",
      "step :1778 Loss: 0.140232\n",
      "step :1779 Loss: 0.200109\n",
      "step :1780 Loss: 0.119253\n",
      "step :1781 Loss: 0.0483669\n",
      "step :1782 Loss: 0.0314741\n",
      "step :1783 Loss: 0.0675038\n",
      "step :1784 Loss: 0.154907\n",
      "step :1785 Loss: 0.15193\n",
      "step :1786 Loss: 0.142306\n",
      "step :1787 Loss: 0.100195\n",
      "step :1788 Loss: 0.140151\n",
      "step :1789 Loss: 0.106289\n",
      "step :1790 Loss: 0.135281\n",
      "step :1791 Loss: 0.12498\n",
      "step :1792 Loss: 0.203568\n",
      "step :1793 Loss: 0.0826477\n",
      "step :1794 Loss: 0.177912\n",
      "step :1795 Loss: 0.0394411\n",
      "step :1796 Loss: 0.21664\n",
      "step :1797 Loss: 0.10488\n",
      "step :1798 Loss: 0.0843714\n",
      "step :1799 Loss: 0.11284\n",
      "step :1800 Loss: 0.057131\n",
      "step :1801 Loss: 0.179027\n",
      "step :1802 Loss: 0.0896972\n",
      "step :1803 Loss: 0.0868895\n",
      "step :1804 Loss: 0.167195\n",
      "step :1805 Loss: 0.139569\n",
      "step :1806 Loss: 0.154622\n",
      "step :1807 Loss: 0.195875\n",
      "step :1808 Loss: 0.186371\n",
      "step :1809 Loss: 0.177114\n",
      "step :1810 Loss: 0.0901954\n",
      "step :1811 Loss: 0.0333955\n",
      "step :1812 Loss: 0.20727\n",
      "step :1813 Loss: 0.128177\n",
      "step :1814 Loss: 0.0935098\n",
      "step :1815 Loss: 0.241026\n",
      "step :1816 Loss: 0.192813\n",
      "step :1817 Loss: 0.0843128\n",
      "step :1818 Loss: 0.123389\n",
      "step :1819 Loss: 0.234472\n",
      "step :1820 Loss: 0.159407\n",
      "step :1821 Loss: 0.0314333\n",
      "step :1822 Loss: 0.178492\n",
      "step :1823 Loss: 0.261411\n",
      "step :1824 Loss: 0.0176549\n",
      "step :1825 Loss: 0.0698732\n",
      "step :1826 Loss: 0.133516\n",
      "step :1827 Loss: 0.0800359\n",
      "step :1828 Loss: 0.114298\n",
      "step :1829 Loss: 0.12973\n",
      "step :1830 Loss: 0.170993\n",
      "step :1831 Loss: 0.137364\n",
      "step :1832 Loss: 0.162\n",
      "step :1833 Loss: 0.0325706\n",
      "step :1834 Loss: 0.126537\n",
      "step :1835 Loss: 0.0586675\n",
      "step :1836 Loss: 0.100907\n",
      "step :1837 Loss: 0.117422\n",
      "step :1838 Loss: 0.12182\n",
      "step :1839 Loss: 0.174946\n",
      "step :1840 Loss: 0.0847596\n",
      "step :1841 Loss: 0.161509\n",
      "step :1842 Loss: 0.13563\n",
      "step :1843 Loss: 0.0853974\n",
      "step :1844 Loss: 0.165121\n",
      "step :1845 Loss: 0.186718\n",
      "step :1846 Loss: 0.0945523\n",
      "step :1847 Loss: 0.165618\n",
      "step :1848 Loss: 0.256385\n",
      "step :1849 Loss: 0.0510036\n",
      "step :1850 Loss: 0.10645\n",
      "step :1851 Loss: 0.130473\n",
      "step :1852 Loss: 0.173377\n",
      "step :1853 Loss: 0.115535\n",
      "step :1854 Loss: 0.100063\n",
      "step :1855 Loss: 0.146655\n",
      "step :1856 Loss: 0.113746\n",
      "step :1857 Loss: 0.117838\n",
      "step :1858 Loss: 0.0937469\n",
      "step :1859 Loss: 0.0856303\n",
      "step :1860 Loss: 0.145092\n",
      "step :1861 Loss: 0.0948128\n",
      "step :1862 Loss: 0.187349\n",
      "step :1863 Loss: 0.198262\n",
      "step :1864 Loss: 0.178899\n",
      "step :1865 Loss: 0.193566\n",
      "step :1866 Loss: 0.112555\n",
      "step :1867 Loss: 0.0784678\n",
      "step :1868 Loss: 0.201155\n",
      "step :1869 Loss: 0.167588\n",
      "step :1870 Loss: 0.0925689\n",
      "step :1871 Loss: 0.0723308\n",
      "step :1872 Loss: 0.216359\n",
      "step :1873 Loss: 0.176046\n",
      "step :1874 Loss: 0.105369\n",
      "step :1875 Loss: 0.141985\n",
      "step :1876 Loss: 0.163566\n",
      "step :1877 Loss: 0.206183\n",
      "step :1878 Loss: 0.272074\n",
      "step :1879 Loss: 0.0981443\n",
      "step :1880 Loss: 0.173928\n",
      "step :1881 Loss: 0.206838\n",
      "step :1882 Loss: 0.141962\n",
      "step :1883 Loss: 0.118991\n",
      "step :1884 Loss: 0.144362\n",
      "step :1885 Loss: 0.191473\n",
      "step :1886 Loss: 0.124217\n",
      "step :1887 Loss: 0.09968\n",
      "step :1888 Loss: 0.124234\n",
      "step :1889 Loss: 0.227567\n",
      "step :1890 Loss: 0.108622\n",
      "step :1891 Loss: 0.204928\n",
      "step :1892 Loss: 0.159032\n",
      "step :1893 Loss: 0.135384\n",
      "step :1894 Loss: 0.161301\n",
      "step :1895 Loss: 0.168391\n",
      "step :1896 Loss: 0.174196\n",
      "step :1897 Loss: 0.0616402\n",
      "step :1898 Loss: 0.155646\n",
      "step :1899 Loss: 0.0918093\n",
      "step :1900 Loss: 0.117853\n",
      "step :1901 Loss: 0.0823066\n",
      "step :1902 Loss: 0.0958522\n",
      "step :1903 Loss: 0.0883451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :1904 Loss: 0.113627\n",
      "step :1905 Loss: 0.171644\n",
      "step :1906 Loss: 0.133018\n",
      "step :1907 Loss: 0.151123\n",
      "step :1908 Loss: 0.211166\n",
      "step :1909 Loss: 0.144559\n",
      "step :1910 Loss: 0.0633918\n",
      "step :1911 Loss: 0.07115\n",
      "step :1912 Loss: 0.0409409\n",
      "step :1913 Loss: 0.0829662\n",
      "step :1914 Loss: 0.142824\n",
      "step :1915 Loss: 0.106947\n",
      "step :1916 Loss: 0.0858575\n",
      "step :1917 Loss: 0.121607\n",
      "step :1918 Loss: 0.118176\n",
      "step :1919 Loss: 0.130002\n",
      "step :1920 Loss: 0.101261\n",
      "step :1921 Loss: 0.0978236\n",
      "step :1922 Loss: 0.0459107\n",
      "step :1923 Loss: 0.0847273\n",
      "step :1924 Loss: 0.19087\n",
      "step :1925 Loss: 0.145902\n",
      "step :1926 Loss: 0.0775976\n",
      "step :1927 Loss: 0.0781121\n",
      "step :1928 Loss: 0.0987315\n",
      "step :1929 Loss: 0.0374947\n",
      "step :1930 Loss: 0.0735371\n",
      "step :1931 Loss: 0.130528\n",
      "step :1932 Loss: 0.10375\n",
      "step :1933 Loss: 0.0952303\n",
      "step :1934 Loss: 0.170627\n",
      "step :1935 Loss: 0.0943794\n",
      "step :1936 Loss: 0.0744861\n",
      "step :1937 Loss: 0.143946\n",
      "step :1938 Loss: 0.129795\n",
      "step :1939 Loss: 0.134791\n",
      "step :1940 Loss: 0.134953\n",
      "step :1941 Loss: 0.0687209\n",
      "step :1942 Loss: 0.152365\n",
      "step :1943 Loss: 0.103597\n",
      "step :1944 Loss: 0.0702934\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn8s_final.ckpt\n",
      "step :1945 Loss: 0.0871313\n",
      "step :1946 Loss: 0.0930332\n",
      "step :1947 Loss: 0.215706\n",
      "step :1948 Loss: 0.162761\n",
      "step :1949 Loss: 0.128444\n",
      "step :1950 Loss: 0.162817\n",
      "step :1951 Loss: 0.098976\n",
      "step :1952 Loss: 0.109772\n",
      "step :1953 Loss: 0.126477\n",
      "step :1954 Loss: 0.183499\n",
      "step :1955 Loss: 0.059774\n",
      "step :1956 Loss: 0.0920096\n",
      "step :1957 Loss: 0.164562\n",
      "step :1958 Loss: 0.105356\n",
      "step :1959 Loss: 0.125404\n",
      "step :1960 Loss: 0.0651194\n",
      "step :1961 Loss: 0.0756347\n",
      "step :1962 Loss: 0.195269\n",
      "step :1963 Loss: 0.238908\n",
      "step :1964 Loss: 0.0370079\n",
      "step :1965 Loss: 0.120712\n",
      "step :1966 Loss: 0.191492\n",
      "step :1967 Loss: 0.178088\n",
      "step :1968 Loss: 0.188465\n",
      "step :1969 Loss: 0.1305\n",
      "step :1970 Loss: 0.0543036\n",
      "step :1971 Loss: 0.13284\n",
      "step :1972 Loss: 0.0573194\n",
      "step :1973 Loss: 0.173257\n",
      "step :1974 Loss: 0.125107\n",
      "step :1975 Loss: 0.0845886\n",
      "step :1976 Loss: 0.275435\n",
      "step :1977 Loss: 0.182125\n",
      "step :1978 Loss: 0.0796551\n",
      "step :1979 Loss: 0.502491\n",
      "step :1980 Loss: 0.194715\n",
      "step :1981 Loss: 0.148422\n",
      "step :1982 Loss: 0.0760631\n",
      "step :1983 Loss: 0.0969629\n",
      "step :1984 Loss: 0.14988\n",
      "step :1985 Loss: 0.180449\n",
      "step :1986 Loss: 0.0529531\n",
      "step :1987 Loss: 0.138278\n",
      "step :1988 Loss: 0.122826\n",
      "step :1989 Loss: 0.0825917\n",
      "step :1990 Loss: 0.0913245\n",
      "step :1991 Loss: 0.111594\n",
      "step :1992 Loss: 0.137035\n",
      "step :1993 Loss: 0.106955\n",
      "step :1994 Loss: 0.124903\n",
      "step :1995 Loss: 0.164437\n",
      "step :1996 Loss: 0.113694\n",
      "step :1997 Loss: 0.0741907\n",
      "step :1998 Loss: 0.124075\n",
      "step :1999 Loss: 0.238624\n",
      "step :2000 Loss: 0.0840474\n",
      "step :2001 Loss: 0.161486\n",
      "step :2002 Loss: 0.113431\n",
      "step :2003 Loss: 0.0959338\n",
      "step :2004 Loss: 0.0439499\n",
      "step :2005 Loss: 0.133589\n",
      "step :2006 Loss: 0.172823\n",
      "step :2007 Loss: 0.130353\n",
      "step :2008 Loss: 0.172273\n",
      "step :2009 Loss: 0.107674\n",
      "step :2010 Loss: 0.117901\n",
      "step :2011 Loss: 0.172815\n",
      "step :2012 Loss: 0.157322\n",
      "step :2013 Loss: 0.0711025\n",
      "step :2014 Loss: 0.141155\n",
      "step :2015 Loss: 0.107574\n",
      "step :2016 Loss: 0.192185\n",
      "step :2017 Loss: 0.151423\n",
      "step :2018 Loss: 0.136824\n",
      "step :2019 Loss: 0.170194\n",
      "step :2020 Loss: 0.18703\n",
      "step :2021 Loss: 0.161937\n",
      "step :2022 Loss: 0.0906668\n",
      "step :2023 Loss: 0.267788\n",
      "step :2024 Loss: 0.113082\n",
      "step :2025 Loss: 0.184517\n",
      "step :2026 Loss: 0.0810287\n",
      "step :2027 Loss: 0.262846\n",
      "step :2028 Loss: 0.109102\n",
      "step :2029 Loss: 0.0587569\n",
      "step :2030 Loss: 0.178158\n",
      "step :2031 Loss: 0.0940116\n",
      "step :2032 Loss: 0.169699\n",
      "step :2033 Loss: 0.110731\n",
      "step :2034 Loss: 0.0720217\n",
      "step :2035 Loss: 0.12384\n",
      "step :2036 Loss: 0.126501\n",
      "step :2037 Loss: 0.24652\n",
      "step :2038 Loss: 0.113088\n",
      "step :2039 Loss: 0.0726011\n",
      "step :2040 Loss: 0.150693\n",
      "step :2041 Loss: 0.0944619\n",
      "step :2042 Loss: 0.0895514\n",
      "step :2043 Loss: 0.0880905\n",
      "step :2044 Loss: 0.0771903\n",
      "step :2045 Loss: 0.180447\n",
      "step :2046 Loss: 0.124552\n",
      "step :2047 Loss: 0.195021\n",
      "step :2048 Loss: 0.212695\n",
      "step :2049 Loss: 0.300867\n",
      "step :2050 Loss: 0.0914337\n",
      "step :2051 Loss: 0.145213\n",
      "step :2052 Loss: 0.1588\n",
      "step :2053 Loss: 0.210695\n",
      "step :2054 Loss: 0.104006\n",
      "step :2055 Loss: 0.105845\n",
      "step :2056 Loss: 0.109488\n",
      "step :2057 Loss: 0.116664\n",
      "step :2058 Loss: 0.139027\n",
      "step :2059 Loss: 0.0588699\n",
      "step :2060 Loss: 0.119954\n",
      "step :2061 Loss: 0.206369\n",
      "step :2062 Loss: 0.324157\n",
      "step :2063 Loss: 0.118994\n",
      "step :2064 Loss: 0.0870112\n",
      "step :2065 Loss: 0.0819005\n",
      "step :2066 Loss: 0.120288\n",
      "step :2067 Loss: 0.160961\n",
      "step :2068 Loss: 0.0852471\n",
      "step :2069 Loss: 0.0996645\n",
      "step :2070 Loss: 0.108392\n",
      "step :2071 Loss: 0.0995492\n",
      "step :2072 Loss: 0.139891\n",
      "step :2073 Loss: 0.0929386\n",
      "step :2074 Loss: 0.121619\n",
      "step :2075 Loss: 0.0842042\n",
      "step :2076 Loss: 0.145178\n",
      "step :2077 Loss: 0.061602\n",
      "step :2078 Loss: 0.126996\n",
      "step :2079 Loss: 0.0834547\n",
      "step :2080 Loss: 0.134032\n",
      "step :2081 Loss: 0.135439\n",
      "step :2082 Loss: 0.1246\n",
      "step :2083 Loss: 0.0700054\n",
      "step :2084 Loss: 0.0551903\n",
      "step :2085 Loss: 0.110598\n",
      "step :2086 Loss: 0.0378661\n",
      "step :2087 Loss: 0.0409854\n",
      "step :2088 Loss: 0.136594\n",
      "step :2089 Loss: 0.0951847\n",
      "step :2090 Loss: 0.0773361\n",
      "step :2091 Loss: 0.277441\n",
      "step :2092 Loss: 0.114464\n",
      "step :2093 Loss: 0.212147\n",
      "step :2094 Loss: 0.0732684\n",
      "step :2095 Loss: 0.0884541\n",
      "step :2096 Loss: 0.138638\n",
      "step :2097 Loss: 0.136594\n",
      "step :2098 Loss: 0.0688788\n",
      "step :2099 Loss: 0.0959744\n",
      "step :2100 Loss: 0.109209\n",
      "step :2101 Loss: 0.11785\n",
      "step :2102 Loss: 0.134452\n",
      "step :2103 Loss: 0.107749\n",
      "step :2104 Loss: 0.0932246\n",
      "step :2105 Loss: 0.0795874\n",
      "step :2106 Loss: 0.119656\n",
      "step :2107 Loss: 0.253882\n",
      "step :2108 Loss: 0.174287\n",
      "step :2109 Loss: 0.116955\n",
      "step :2110 Loss: 0.0381736\n",
      "step :2111 Loss: 0.141361\n",
      "step :2112 Loss: 0.0861689\n",
      "step :2113 Loss: 0.140936\n",
      "step :2114 Loss: 0.252509\n",
      "step :2115 Loss: 0.0349615\n",
      "step :2116 Loss: 0.133986\n",
      "step :2117 Loss: 0.103857\n",
      "step :2118 Loss: 0.0840257\n",
      "step :2119 Loss: 0.219954\n",
      "step :2120 Loss: 0.141779\n",
      "step :2121 Loss: 0.0734525\n",
      "step :2122 Loss: 0.053483\n",
      "step :2123 Loss: 0.0683553\n",
      "step :2124 Loss: 0.21766\n",
      "step :2125 Loss: 0.240027\n",
      "step :2126 Loss: 0.103571\n",
      "step :2127 Loss: 0.0902702\n",
      "step :2128 Loss: 0.162058\n",
      "step :2129 Loss: 0.0674988\n",
      "step :2130 Loss: 0.168378\n",
      "step :2131 Loss: 0.127404\n",
      "step :2132 Loss: 0.0871812\n",
      "step :2133 Loss: 0.129939\n",
      "step :2134 Loss: 0.152269\n",
      "step :2135 Loss: 0.180155\n",
      "step :2136 Loss: 0.112245\n",
      "step :2137 Loss: 0.079449\n",
      "step :2138 Loss: 0.096221\n",
      "step :2139 Loss: 0.0774076\n",
      "step :2140 Loss: 0.125706\n",
      "step :2141 Loss: 0.185541\n",
      "step :2142 Loss: 0.14575\n",
      "step :2143 Loss: 0.250581\n",
      "step :2144 Loss: 0.0420735\n",
      "step :2145 Loss: 0.0565771\n",
      "step :2146 Loss: 0.0993275\n",
      "step :2147 Loss: 0.0326452\n",
      "step :2148 Loss: 0.144433\n",
      "step :2149 Loss: 0.0641652\n",
      "step :2150 Loss: 0.123993\n",
      "step :2151 Loss: 0.239284\n",
      "step :2152 Loss: 0.0798694\n",
      "step :2153 Loss: 0.27428\n",
      "step :2154 Loss: 0.188716\n",
      "step :2155 Loss: 0.105681\n",
      "step :2156 Loss: 0.0763745\n",
      "step :2157 Loss: 0.0458171\n",
      "step :2158 Loss: 0.098216\n",
      "step :2159 Loss: 0.239863\n",
      "step :2160 Loss: 0.0850732\n",
      "step :2161 Loss: 0.140764\n",
      "step :2162 Loss: 0.24854\n",
      "step :2163 Loss: 0.121623\n",
      "step :2164 Loss: 0.0961771\n",
      "step :2165 Loss: 0.275333\n",
      "step :2166 Loss: 0.091594\n",
      "step :2167 Loss: 0.163613\n",
      "step :2168 Loss: 0.105974\n",
      "step :2169 Loss: 0.0369021\n",
      "step :2170 Loss: 0.155609\n",
      "step :2171 Loss: 0.117267\n",
      "step :2172 Loss: 0.181611\n",
      "step :2173 Loss: 0.168121\n",
      "step :2174 Loss: 0.0792294\n",
      "step :2175 Loss: 0.109503\n",
      "step :2176 Loss: 0.168221\n",
      "step :2177 Loss: 0.0826249\n",
      "step :2178 Loss: 0.075426\n",
      "step :2179 Loss: 0.0981187\n",
      "step :2180 Loss: 0.088492\n",
      "step :2181 Loss: 0.203115\n",
      "step :2182 Loss: 0.135236\n",
      "step :2183 Loss: 0.102742\n",
      "step :2184 Loss: 0.356287\n",
      "step :2185 Loss: 0.0587044\n",
      "step :2186 Loss: 0.147553\n",
      "step :2187 Loss: 0.125966\n",
      "step :2188 Loss: 0.0994223\n",
      "step :2189 Loss: 0.114038\n",
      "step :2190 Loss: 0.152477\n",
      "step :2191 Loss: 0.101583\n",
      "step :2192 Loss: 0.187056\n",
      "step :2193 Loss: 0.0711641\n",
      "step :2194 Loss: 0.185488\n",
      "step :2195 Loss: 0.25005\n",
      "step :2196 Loss: 0.118548\n",
      "step :2197 Loss: 0.189185\n",
      "step :2198 Loss: 0.181188\n",
      "step :2199 Loss: 0.207179\n",
      "step :2200 Loss: 0.0539285\n",
      "step :2201 Loss: 0.151026\n",
      "step :2202 Loss: 0.06297\n",
      "step :2203 Loss: 0.0960805\n",
      "step :2204 Loss: 0.0281154\n",
      "step :2205 Loss: 0.0545819\n",
      "step :2206 Loss: 0.098017\n",
      "step :2207 Loss: 0.0440907\n",
      "step :2208 Loss: 0.231867\n",
      "step :2209 Loss: 0.139814\n",
      "step :2210 Loss: 0.129546\n",
      "step :2211 Loss: 0.079379\n",
      "step :2212 Loss: 0.0833863\n",
      "step :2213 Loss: 0.11324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :2214 Loss: 0.169438\n",
      "step :2215 Loss: 0.127888\n",
      "step :2216 Loss: 0.21838\n",
      "step :2217 Loss: 0.260696\n",
      "step :2218 Loss: 0.0994474\n",
      "step :2219 Loss: 0.190767\n",
      "step :2220 Loss: 0.0619265\n",
      "step :2221 Loss: 0.233485\n",
      "step :2222 Loss: 0.113105\n",
      "step :2223 Loss: 0.254467\n",
      "step :2224 Loss: 0.0632499\n",
      "step :2225 Loss: 0.134701\n",
      "step :2226 Loss: 0.29545\n",
      "step :2227 Loss: 0.0616224\n",
      "step :2228 Loss: 0.0688887\n",
      "step :2229 Loss: 0.0707431\n",
      "step :2230 Loss: 0.196106\n",
      "step :2231 Loss: 0.202141\n",
      "step :2232 Loss: 0.156423\n",
      "step :2233 Loss: 0.142786\n",
      "step :2234 Loss: 0.0861569\n",
      "step :2235 Loss: 0.116084\n",
      "step :2236 Loss: 0.076467\n",
      "step :2237 Loss: 0.0946672\n",
      "step :2238 Loss: 0.131755\n",
      "step :2239 Loss: 0.166388\n",
      "step :2240 Loss: 0.0761301\n",
      "step :2241 Loss: 0.0852254\n",
      "step :2242 Loss: 0.0620668\n",
      "step :2243 Loss: 0.105961\n",
      "step :2244 Loss: 0.0979922\n",
      "step :2245 Loss: 0.0963292\n",
      "step :2246 Loss: 0.0831083\n",
      "step :2247 Loss: 0.126317\n",
      "step :2248 Loss: 0.0735016\n",
      "step :2249 Loss: 0.105779\n",
      "step :2250 Loss: 0.181947\n",
      "step :2251 Loss: 0.025623\n",
      "step :2252 Loss: 0.26534\n",
      "step :2253 Loss: 0.153865\n",
      "step :2254 Loss: 0.18558\n",
      "step :2255 Loss: 0.222597\n",
      "step :2256 Loss: 0.081322\n",
      "step :2257 Loss: 0.0834303\n",
      "step :2258 Loss: 0.0813141\n",
      "step :2259 Loss: 0.051451\n",
      "step :2260 Loss: 0.121646\n",
      "step :2261 Loss: 0.0707236\n",
      "step :2262 Loss: 0.14704\n",
      "step :2263 Loss: 0.150167\n",
      "step :2264 Loss: 0.0343761\n",
      "step :2265 Loss: 0.0902845\n",
      "step :2266 Loss: 0.133081\n",
      "step :2267 Loss: 0.119091\n",
      "step :2268 Loss: 0.0891748\n",
      "step :2269 Loss: 0.14201\n",
      "step :2270 Loss: 0.130206\n",
      "step :2271 Loss: 0.0861429\n",
      "step :2272 Loss: 0.229894\n",
      "step :2273 Loss: 0.0510815\n",
      "step :2274 Loss: 0.0760592\n",
      "step :2275 Loss: 0.18724\n",
      "step :2276 Loss: 0.211775\n",
      "step :2277 Loss: 0.11053\n",
      "step :2278 Loss: 0.0943317\n",
      "step :2279 Loss: 0.114554\n",
      "step :2280 Loss: 0.127433\n",
      "step :2281 Loss: 0.100147\n",
      "step :2282 Loss: 0.213712\n",
      "step :2283 Loss: 0.103485\n",
      "step :2284 Loss: 0.0722064\n",
      "step :2285 Loss: 0.17324\n",
      "step :2286 Loss: 0.130945\n",
      "step :2287 Loss: 0.11606\n",
      "step :2288 Loss: 0.120387\n",
      "step :2289 Loss: 0.120882\n",
      "step :2290 Loss: 0.061563\n",
      "step :2291 Loss: 0.203726\n",
      "step :2292 Loss: 0.110843\n",
      "step :2293 Loss: 0.0938043\n",
      "step :2294 Loss: 0.324517\n",
      "step :2295 Loss: 0.0230271\n",
      "step :2296 Loss: 0.117396\n",
      "step :2297 Loss: 0.140588\n",
      "step :2298 Loss: 0.100586\n",
      "step :2299 Loss: 0.225615\n",
      "step :2300 Loss: 0.114037\n",
      "step :2301 Loss: 0.219163\n",
      "step :2302 Loss: 0.190584\n",
      "step :2303 Loss: 0.218165\n",
      "step :2304 Loss: 0.240205\n",
      "step :2305 Loss: 0.156418\n",
      "step :2306 Loss: 0.230386\n",
      "step :2307 Loss: 0.098547\n",
      "step :2308 Loss: 0.188672\n",
      "step :2309 Loss: 0.119049\n",
      "step :2310 Loss: 0.128138\n",
      "step :2311 Loss: 0.140792\n",
      "step :2312 Loss: 0.132132\n",
      "step :2313 Loss: 0.116836\n",
      "step :2314 Loss: 0.0430437\n",
      "step :2315 Loss: 0.0382513\n",
      "step :2316 Loss: 0.145011\n",
      "step :2317 Loss: 0.0445377\n",
      "step :2318 Loss: 0.133179\n",
      "step :2319 Loss: 0.144714\n",
      "step :2320 Loss: 0.0744379\n",
      "step :2321 Loss: 0.168262\n",
      "step :2322 Loss: 0.241305\n",
      "step :2323 Loss: 0.106744\n",
      "step :2324 Loss: 0.182925\n",
      "step :2325 Loss: 0.0800349\n",
      "step :2326 Loss: 0.120631\n",
      "step :2327 Loss: 0.179523\n",
      "step :2328 Loss: 0.221406\n",
      "step :2329 Loss: 0.117828\n",
      "step :2330 Loss: 0.125391\n",
      "step :2331 Loss: 0.132286\n",
      "step :2332 Loss: 0.0475481\n",
      "step :2333 Loss: 0.0885188\n",
      "step :2334 Loss: 0.155336\n",
      "step :2335 Loss: 0.145354\n",
      "step :2336 Loss: 0.158942\n",
      "step :2337 Loss: 0.0850185\n",
      "step :2338 Loss: 0.178972\n",
      "step :2339 Loss: 0.14153\n",
      "step :2340 Loss: 0.222036\n",
      "step :2341 Loss: 0.0842066\n",
      "step :2342 Loss: 0.0733005\n",
      "step :2343 Loss: 0.244171\n",
      "step :2344 Loss: 0.154838\n",
      "step :2345 Loss: 0.113791\n",
      "step :2346 Loss: 0.110493\n",
      "step :2347 Loss: 0.081267\n",
      "step :2348 Loss: 0.0803485\n",
      "step :2349 Loss: 0.0743714\n",
      "step :2350 Loss: 0.0255331\n",
      "step :2351 Loss: 0.158499\n",
      "step :2352 Loss: 0.104281\n",
      "step :2353 Loss: 0.127744\n",
      "step :2354 Loss: 0.185388\n",
      "step :2355 Loss: 0.0345547\n",
      "step :2356 Loss: 0.0881275\n",
      "step :2357 Loss: 0.13215\n",
      "step :2358 Loss: 0.0638493\n",
      "step :2359 Loss: 0.0817801\n",
      "step :2360 Loss: 0.122669\n",
      "step :2361 Loss: 0.137162\n",
      "step :2362 Loss: 0.0511461\n",
      "step :2363 Loss: 0.196118\n",
      "step :2364 Loss: 0.090315\n",
      "step :2365 Loss: 0.0518465\n",
      "step :2366 Loss: 0.172964\n",
      "step :2367 Loss: 0.109927\n",
      "step :2368 Loss: 0.106978\n",
      "step :2369 Loss: 0.213075\n",
      "step :2370 Loss: 0.177716\n",
      "step :2371 Loss: 0.1247\n",
      "step :2372 Loss: 0.0666566\n",
      "step :2373 Loss: 0.106167\n",
      "step :2374 Loss: 0.162514\n",
      "step :2375 Loss: 0.0824374\n",
      "step :2376 Loss: 0.244881\n",
      "step :2377 Loss: 0.251435\n",
      "step :2378 Loss: 0.0988344\n",
      "step :2379 Loss: 0.206162\n",
      "step :2380 Loss: 0.0740509\n",
      "step :2381 Loss: 0.0934465\n",
      "step :2382 Loss: 0.106843\n",
      "step :2383 Loss: 0.102736\n",
      "step :2384 Loss: 0.0454097\n",
      "step :2385 Loss: 0.0478695\n",
      "step :2386 Loss: 0.143033\n",
      "step :2387 Loss: 0.0920286\n",
      "step :2388 Loss: 0.139823\n",
      "step :2389 Loss: 0.0964701\n",
      "step :2390 Loss: 0.196985\n",
      "step :2391 Loss: 0.172542\n",
      "step :2392 Loss: 0.113959\n",
      "step :2393 Loss: 0.062102\n",
      "step :2394 Loss: 0.295723\n",
      "step :2395 Loss: 0.302828\n",
      "step :2396 Loss: 0.135358\n",
      "step :2397 Loss: 0.139478\n",
      "step :2398 Loss: 0.163959\n",
      "step :2399 Loss: 0.0259274\n",
      "step :2400 Loss: 0.151746\n",
      "step :2401 Loss: 0.131112\n",
      "step :2402 Loss: 0.14755\n",
      "step :2403 Loss: 0.163758\n",
      "step :2404 Loss: 0.13798\n",
      "step :2405 Loss: 0.174468\n",
      "step :2406 Loss: 0.145721\n",
      "step :2407 Loss: 0.158882\n",
      "step :2408 Loss: 0.190258\n",
      "step :2409 Loss: 0.0539552\n",
      "step :2410 Loss: 0.200385\n",
      "step :2411 Loss: 0.0764938\n",
      "step :2412 Loss: 0.0922204\n",
      "step :2413 Loss: 0.0795894\n",
      "step :2414 Loss: 0.109092\n",
      "step :2415 Loss: 0.212887\n",
      "step :2416 Loss: 0.297081\n",
      "step :2417 Loss: 0.0797672\n",
      "step :2418 Loss: 0.112691\n",
      "step :2419 Loss: 0.0243251\n",
      "step :2420 Loss: 0.106823\n",
      "step :2421 Loss: 0.122107\n",
      "step :2422 Loss: 0.242081\n",
      "step :2423 Loss: 0.193637\n",
      "step :2424 Loss: 0.107879\n",
      "step :2425 Loss: 0.0611223\n",
      "step :2426 Loss: 0.191283\n",
      "step :2427 Loss: 0.250627\n",
      "step :2428 Loss: 0.088461\n",
      "step :2429 Loss: 0.0852458\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn8s_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(combined_op)\n",
    "    init_fn(sess)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # Let's read off 3 batches just for example\n",
    "    for i in xrange(486 * 5):\n",
    "    \n",
    "        cross_entropy, summary_string, _ = sess.run([ cross_entropy_sum,\n",
    "                                                      merged_summary_op,\n",
    "                                                      train_step ])\n",
    "\n",
    "        summary_string_writer.add_summary(summary_string, 486 * 10 + i)\n",
    "        \n",
    "        print(\"step :\" + str(i) + \" Loss: \" + str(cross_entropy))\n",
    "        \n",
    "        if i % 486 == 0:\n",
    "            save_path = saver.save(sess, \"/home/shou/network/dataset/model_fcn8s_final.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    save_path = saver.save(sess, \"/home/shou/network/dataset/model_fcn8s_final.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "summary_string_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

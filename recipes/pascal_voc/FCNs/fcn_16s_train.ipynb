{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Use second GPU -- change if you want to use a first one\n",
    "\n",
    "# Add a path to a custom fork of TF-Slim\n",
    "# Get it from here:\n",
    "# https://github.com/warmspringwinds/models/tree/fully_conv_vgg\n",
    "sys.path.append(\"/home/shou/network/tf-models/research/slim\")\n",
    "\n",
    "# Add path to the cloned library\n",
    "sys.path.append(\"/home/shou/network/fcn/tf-image-segmentation\")\n",
    "\n",
    "checkpoints_dir = '/home/shou/network/checkpoints'\n",
    "log_folder = '/home/shou/network/log_folder_fcn32s'\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from tf_image_segmentation.utils.tf_records import read_tfrecord_and_decode_into_image_annotation_pair_tensors\n",
    "from tf_image_segmentation.models.fcn_16s import FCN_16s\n",
    "\n",
    "from tf_image_segmentation.utils.pascal_voc import pascal_segmentation_lut\n",
    "\n",
    "from tf_image_segmentation.utils.training import get_valid_logits_and_labels\n",
    "\n",
    "from tf_image_segmentation.utils.augmentation import (distort_randomly_image_color,\n",
    "                                                      flip_randomly_left_right_image_with_annotation,\n",
    "                                                      scale_randomly_image_with_annotation_with_fixed_size_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:36: calling cond (from tensorflow.python.ops.control_flow_ops) with fn2 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:36: calling cond (from tensorflow.python.ops.control_flow_ops) with fn1 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:40: calling cond (from tensorflow.python.ops.control_flow_ops) with fn2 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/shou/network/fcn/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:40: calling cond (from tensorflow.python.ops.control_flow_ops) with fn1 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From <ipython-input-2-b8ded496d368>:48: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "image_train_size = [384, 384]\n",
    "number_of_classes = 2\n",
    "tfrecord_filename = '/home/shou/network/dataset/pascal_augmented_train_island.tfrecords'\n",
    "pascal_voc_lut = pascal_segmentation_lut(number_of_classes)\n",
    "class_labels = pascal_voc_lut.keys()\n",
    "\n",
    "fcn_32s_checkpoint_path = '/home/shou/network/dataset/model_fcn32s.ckpt'\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    [tfrecord_filename], num_epochs=5)\n",
    "\n",
    "image, annotation = read_tfrecord_and_decode_into_image_annotation_pair_tensors(filename_queue)\n",
    "\n",
    "# Various data augmentation stages\n",
    "image, annotation = flip_randomly_left_right_image_with_annotation(image, annotation)\n",
    "\n",
    "# image = distort_randomly_image_color(image)\n",
    "\n",
    "resized_image, resized_annotation = scale_randomly_image_with_annotation_with_fixed_size_output(image, annotation, image_train_size)\n",
    "\n",
    "\n",
    "resized_annotation = tf.squeeze(resized_annotation)\n",
    "\n",
    "image_batch, annotation_batch = tf.train.shuffle_batch( [resized_image, resized_annotation],\n",
    "                                             batch_size=1,\n",
    "                                             capacity=3000,\n",
    "                                             num_threads=2,\n",
    "                                             min_after_dequeue=1000)\n",
    "\n",
    "upsampled_logits_batch, fcn_32s_variables_mapping = FCN_16s(image_batch_tensor=image_batch,\n",
    "                                                           number_of_classes=number_of_classes,\n",
    "                                                           is_training=True)\n",
    "\n",
    "\n",
    "valid_labels_batch_tensor, valid_logits_batch_tensor = get_valid_logits_and_labels(annotation_batch_tensor=annotation_batch,\n",
    "                                                                                     logits_batch_tensor=upsampled_logits_batch,\n",
    "                                                                                    class_labels=class_labels)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=valid_logits_batch_tensor,\n",
    "                                                          labels=valid_labels_batch_tensor)\n",
    "\n",
    "#cross_entropy_sum = tf.reduce_sum(cross_entropies)\n",
    "\n",
    "cross_entropy_sum = tf.reduce_mean(cross_entropies)\n",
    "\n",
    "pred = tf.argmax(upsampled_logits_batch, dimension=3)\n",
    "\n",
    "probabilities = tf.nn.softmax(upsampled_logits_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"adam_vars\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=0.00000001).minimize(cross_entropy_sum)\n",
    "\n",
    "\n",
    "#adam_optimizer_variables = slim.get_variables_to_restore(include=['adam_vars'])\n",
    "\n",
    "# Variable's initialization functions\n",
    "init_fn = slim.assign_from_checkpoint_fn(model_path=fcn_32s_checkpoint_path,\n",
    "                                         var_list=fcn_32s_variables_mapping)\n",
    "\n",
    "global_vars_init_op = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar('cross_entropy_loss', cross_entropy_sum)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "summary_string_writer = tf.summary.FileWriter(log_folder)\n",
    "\n",
    "# Create the log folder if doesn't exist yet\n",
    "if not os.path.exists(log_folder):\n",
    "     os.makedirs(log_folder)\n",
    "\n",
    "#optimization_variables_initializer = tf.variables_initializer(adam_optimizer_variables)\n",
    "    \n",
    "#The op for initializing the variables.\n",
    "local_vars_init_op = tf.local_variables_initializer()\n",
    "\n",
    "combined_op = tf.group(local_vars_init_op, global_vars_init_op)\n",
    "\n",
    "# We need this to save only model variables and omit\n",
    "# optimization-related and other variables.\n",
    "model_variables = slim.get_model_variables()\n",
    "saver = tf.train.Saver(model_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/shou/network/dataset/model_fcn32s.ckpt\n",
      "step :0 Loss: 0.130845\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn16s_final.ckpt\n",
      "step :1 Loss: 0.105321\n",
      "step :2 Loss: 0.0776117\n",
      "step :3 Loss: 0.106419\n",
      "step :4 Loss: 0.111803\n",
      "step :5 Loss: 0.0916703\n",
      "step :6 Loss: 0.12547\n",
      "step :7 Loss: 0.1232\n",
      "step :8 Loss: 0.103953\n",
      "step :9 Loss: 0.142505\n",
      "step :10 Loss: 0.147072\n",
      "step :11 Loss: 0.145642\n",
      "step :12 Loss: 0.0613367\n",
      "step :13 Loss: 0.143708\n",
      "step :14 Loss: 0.149368\n",
      "step :15 Loss: 0.0535257\n",
      "step :16 Loss: 0.0621817\n",
      "step :17 Loss: 0.168576\n",
      "step :18 Loss: 0.107838\n",
      "step :19 Loss: 0.102466\n",
      "step :20 Loss: 0.21853\n",
      "step :21 Loss: 0.146701\n",
      "step :22 Loss: 0.230347\n",
      "step :23 Loss: 0.103849\n",
      "step :24 Loss: 0.169017\n",
      "step :25 Loss: 0.131014\n",
      "step :26 Loss: 0.168649\n",
      "step :27 Loss: 0.177725\n",
      "step :28 Loss: 0.117607\n",
      "step :29 Loss: 0.139254\n",
      "step :30 Loss: 0.0619673\n",
      "step :31 Loss: 0.0728192\n",
      "step :32 Loss: 0.183678\n",
      "step :33 Loss: 0.163644\n",
      "step :34 Loss: 0.0538853\n",
      "step :35 Loss: 0.167817\n",
      "step :36 Loss: 0.0920086\n",
      "step :37 Loss: 0.218147\n",
      "step :38 Loss: 0.15014\n",
      "step :39 Loss: 0.0489441\n",
      "step :40 Loss: 0.0990586\n",
      "step :41 Loss: 0.0581536\n",
      "step :42 Loss: 0.299159\n",
      "step :43 Loss: 0.235541\n",
      "step :44 Loss: 0.120373\n",
      "step :45 Loss: 0.103325\n",
      "step :46 Loss: 0.0339915\n",
      "step :47 Loss: 0.126553\n",
      "step :48 Loss: 0.0502616\n",
      "step :49 Loss: 0.216725\n",
      "step :50 Loss: 0.0695128\n",
      "step :51 Loss: 0.0874866\n",
      "step :52 Loss: 0.200935\n",
      "step :53 Loss: 0.0576122\n",
      "step :54 Loss: 0.13529\n",
      "step :55 Loss: 0.185398\n",
      "step :56 Loss: 0.0565525\n",
      "step :57 Loss: 0.0704402\n",
      "step :58 Loss: 0.102789\n",
      "step :59 Loss: 0.144383\n",
      "step :60 Loss: 0.147831\n",
      "step :61 Loss: 0.167272\n",
      "step :62 Loss: 0.118247\n",
      "step :63 Loss: 0.112292\n",
      "step :64 Loss: 0.0531656\n",
      "step :65 Loss: 0.119168\n",
      "step :66 Loss: 0.134131\n",
      "step :67 Loss: 0.110713\n",
      "step :68 Loss: 0.0440528\n",
      "step :69 Loss: 0.0812083\n",
      "step :70 Loss: 0.0602181\n",
      "step :71 Loss: 0.0747818\n",
      "step :72 Loss: 0.0934293\n",
      "step :73 Loss: 0.195756\n",
      "step :74 Loss: 0.201251\n",
      "step :75 Loss: 0.260874\n",
      "step :76 Loss: 0.092433\n",
      "step :77 Loss: 0.100461\n",
      "step :78 Loss: 0.126305\n",
      "step :79 Loss: 0.0562025\n",
      "step :80 Loss: 0.177975\n",
      "step :81 Loss: 0.0943859\n",
      "step :82 Loss: 0.185619\n",
      "step :83 Loss: 0.165873\n",
      "step :84 Loss: 0.144514\n",
      "step :85 Loss: 0.253663\n",
      "step :86 Loss: 0.138664\n",
      "step :87 Loss: 0.178867\n",
      "step :88 Loss: 0.207104\n",
      "step :89 Loss: 0.0733978\n",
      "step :90 Loss: 0.115512\n",
      "step :91 Loss: 0.149393\n",
      "step :92 Loss: 0.0514471\n",
      "step :93 Loss: 0.0995344\n",
      "step :94 Loss: 0.234793\n",
      "step :95 Loss: 0.0989116\n",
      "step :96 Loss: 0.113412\n",
      "step :97 Loss: 0.138488\n",
      "step :98 Loss: 0.111679\n",
      "step :99 Loss: 0.0924652\n",
      "step :100 Loss: 0.123784\n",
      "step :101 Loss: 0.116772\n",
      "step :102 Loss: 0.132369\n",
      "step :103 Loss: 0.103466\n",
      "step :104 Loss: 0.101171\n",
      "step :105 Loss: 0.0843035\n",
      "step :106 Loss: 0.124944\n",
      "step :107 Loss: 0.154221\n",
      "step :108 Loss: 0.0799679\n",
      "step :109 Loss: 0.166123\n",
      "step :110 Loss: 0.257877\n",
      "step :111 Loss: 0.16232\n",
      "step :112 Loss: 0.182912\n",
      "step :113 Loss: 0.0957802\n",
      "step :114 Loss: 0.182409\n",
      "step :115 Loss: 0.14118\n",
      "step :116 Loss: 0.110408\n",
      "step :117 Loss: 0.12004\n",
      "step :118 Loss: 0.149541\n",
      "step :119 Loss: 0.169758\n",
      "step :120 Loss: 0.112392\n",
      "step :121 Loss: 0.138254\n",
      "step :122 Loss: 0.118376\n",
      "step :123 Loss: 0.128028\n",
      "step :124 Loss: 0.120025\n",
      "step :125 Loss: 0.123696\n",
      "step :126 Loss: 0.051738\n",
      "step :127 Loss: 0.119086\n",
      "step :128 Loss: 0.068044\n",
      "step :129 Loss: 0.137081\n",
      "step :130 Loss: 0.109116\n",
      "step :131 Loss: 0.111512\n",
      "step :132 Loss: 0.0879938\n",
      "step :133 Loss: 0.0919791\n",
      "step :134 Loss: 0.114526\n",
      "step :135 Loss: 0.133042\n",
      "step :136 Loss: 0.139368\n",
      "step :137 Loss: 0.269927\n",
      "step :138 Loss: 0.173403\n",
      "step :139 Loss: 0.0901763\n",
      "step :140 Loss: 0.143536\n",
      "step :141 Loss: 0.173366\n",
      "step :142 Loss: 0.17039\n",
      "step :143 Loss: 0.0951255\n",
      "step :144 Loss: 0.119222\n",
      "step :145 Loss: 0.174862\n",
      "step :146 Loss: 0.0778046\n",
      "step :147 Loss: 0.0962183\n",
      "step :148 Loss: 0.118021\n",
      "step :149 Loss: 0.0703957\n",
      "step :150 Loss: 0.0691937\n",
      "step :151 Loss: 0.0988533\n",
      "step :152 Loss: 0.112191\n",
      "step :153 Loss: 0.151301\n",
      "step :154 Loss: 0.136241\n",
      "step :155 Loss: 0.0534907\n",
      "step :156 Loss: 0.145901\n",
      "step :157 Loss: 0.0195446\n",
      "step :158 Loss: 0.285578\n",
      "step :159 Loss: 0.203214\n",
      "step :160 Loss: 0.0766782\n",
      "step :161 Loss: 0.153935\n",
      "step :162 Loss: 0.228255\n",
      "step :163 Loss: 0.142695\n",
      "step :164 Loss: 0.102046\n",
      "step :165 Loss: 0.213493\n",
      "step :166 Loss: 0.164972\n",
      "step :167 Loss: 0.0907136\n",
      "step :168 Loss: 0.0726347\n",
      "step :169 Loss: 0.174114\n",
      "step :170 Loss: 0.0632264\n",
      "step :171 Loss: 0.0369333\n",
      "step :172 Loss: 0.0716132\n",
      "step :173 Loss: 0.119463\n",
      "step :174 Loss: 0.0973333\n",
      "step :175 Loss: 0.101714\n",
      "step :176 Loss: 0.0908687\n",
      "step :177 Loss: 0.124517\n",
      "step :178 Loss: 0.12585\n",
      "step :179 Loss: 0.113141\n",
      "step :180 Loss: 0.101103\n",
      "step :181 Loss: 0.172944\n",
      "step :182 Loss: 0.193614\n",
      "step :183 Loss: 0.132116\n",
      "step :184 Loss: 0.137696\n",
      "step :185 Loss: 0.105103\n",
      "step :186 Loss: 0.0918199\n",
      "step :187 Loss: 0.278109\n",
      "step :188 Loss: 0.125803\n",
      "step :189 Loss: 0.148171\n",
      "step :190 Loss: 0.140539\n",
      "step :191 Loss: 0.133359\n",
      "step :192 Loss: 0.15092\n",
      "step :193 Loss: 0.0915903\n",
      "step :194 Loss: 0.0913946\n",
      "step :195 Loss: 0.179402\n",
      "step :196 Loss: 0.157696\n",
      "step :197 Loss: 0.154636\n",
      "step :198 Loss: 0.132192\n",
      "step :199 Loss: 0.0755228\n",
      "step :200 Loss: 0.148489\n",
      "step :201 Loss: 0.140606\n",
      "step :202 Loss: 0.145223\n",
      "step :203 Loss: 0.0863647\n",
      "step :204 Loss: 0.089415\n",
      "step :205 Loss: 0.149686\n",
      "step :206 Loss: 0.107475\n",
      "step :207 Loss: 0.170084\n",
      "step :208 Loss: 0.106397\n",
      "step :209 Loss: 0.0920648\n",
      "step :210 Loss: 0.119963\n",
      "step :211 Loss: 0.159714\n",
      "step :212 Loss: 0.20951\n",
      "step :213 Loss: 0.232504\n",
      "step :214 Loss: 0.168936\n",
      "step :215 Loss: 0.235955\n",
      "step :216 Loss: 0.132957\n",
      "step :217 Loss: 0.0588811\n",
      "step :218 Loss: 0.100529\n",
      "step :219 Loss: 0.114329\n",
      "step :220 Loss: 0.117011\n",
      "step :221 Loss: 0.101112\n",
      "step :222 Loss: 0.203094\n",
      "step :223 Loss: 0.0874143\n",
      "step :224 Loss: 0.132506\n",
      "step :225 Loss: 0.122911\n",
      "step :226 Loss: 0.0357207\n",
      "step :227 Loss: 0.19877\n",
      "step :228 Loss: 0.178778\n",
      "step :229 Loss: 0.0953527\n",
      "step :230 Loss: 0.177694\n",
      "step :231 Loss: 0.29857\n",
      "step :232 Loss: 0.0858904\n",
      "step :233 Loss: 0.196563\n",
      "step :234 Loss: 0.103766\n",
      "step :235 Loss: 0.119073\n",
      "step :236 Loss: 0.0738928\n",
      "step :237 Loss: 0.0915612\n",
      "step :238 Loss: 0.107942\n",
      "step :239 Loss: 0.164446\n",
      "step :240 Loss: 0.0783005\n",
      "step :241 Loss: 0.13024\n",
      "step :242 Loss: 0.154347\n",
      "step :243 Loss: 0.0748069\n",
      "step :244 Loss: 0.0435943\n",
      "step :245 Loss: 0.109238\n",
      "step :246 Loss: 0.269651\n",
      "step :247 Loss: 0.1488\n",
      "step :248 Loss: 0.0154792\n",
      "step :249 Loss: 0.0840297\n",
      "step :250 Loss: 0.177656\n",
      "step :251 Loss: 0.150369\n",
      "step :252 Loss: 0.157619\n",
      "step :253 Loss: 0.198416\n",
      "step :254 Loss: 0.0991853\n",
      "step :255 Loss: 0.186259\n",
      "step :256 Loss: 0.0922836\n",
      "step :257 Loss: 0.368581\n",
      "step :258 Loss: 0.133931\n",
      "step :259 Loss: 0.195566\n",
      "step :260 Loss: 0.0668599\n",
      "step :261 Loss: 0.0733893\n",
      "step :262 Loss: 0.088822\n",
      "step :263 Loss: 0.120238\n",
      "step :264 Loss: 0.109915\n",
      "step :265 Loss: 0.150011\n",
      "step :266 Loss: 0.0591071\n",
      "step :267 Loss: 0.09911\n",
      "step :268 Loss: 0.0729256\n",
      "step :269 Loss: 0.137171\n",
      "step :270 Loss: 0.0896464\n",
      "step :271 Loss: 0.0853761\n",
      "step :272 Loss: 0.190516\n",
      "step :273 Loss: 0.159793\n",
      "step :274 Loss: 0.156056\n",
      "step :275 Loss: 0.112942\n",
      "step :276 Loss: 0.133102\n",
      "step :277 Loss: 0.167496\n",
      "step :278 Loss: 0.177378\n",
      "step :279 Loss: 0.0612858\n",
      "step :280 Loss: 0.0873464\n",
      "step :281 Loss: 0.0789536\n",
      "step :282 Loss: 0.192102\n",
      "step :283 Loss: 0.128841\n",
      "step :284 Loss: 0.187596\n",
      "step :285 Loss: 0.0872578\n",
      "step :286 Loss: 0.119215\n",
      "step :287 Loss: 0.134888\n",
      "step :288 Loss: 0.0987347\n",
      "step :289 Loss: 0.264572\n",
      "step :290 Loss: 0.0447779\n",
      "step :291 Loss: 0.210847\n",
      "step :292 Loss: 0.117348\n",
      "step :293 Loss: 0.209869\n",
      "step :294 Loss: 0.0476872\n",
      "step :295 Loss: 0.131294\n",
      "step :296 Loss: 0.101224\n",
      "step :297 Loss: 0.105467\n",
      "step :298 Loss: 0.127517\n",
      "step :299 Loss: 0.16816\n",
      "step :300 Loss: 0.0586038\n",
      "step :301 Loss: 0.128645\n",
      "step :302 Loss: 0.0846551\n",
      "step :303 Loss: 0.0806649\n",
      "step :304 Loss: 0.0527954\n",
      "step :305 Loss: 0.03858\n",
      "step :306 Loss: 0.250763\n",
      "step :307 Loss: 0.236513\n",
      "step :308 Loss: 0.156349\n",
      "step :309 Loss: 0.0862633\n",
      "step :310 Loss: 0.120658\n",
      "step :311 Loss: 0.100611\n",
      "step :312 Loss: 0.121797\n",
      "step :313 Loss: 0.147388\n",
      "step :314 Loss: 0.100444\n",
      "step :315 Loss: 0.103576\n",
      "step :316 Loss: 0.0586756\n",
      "step :317 Loss: 0.0925931\n",
      "step :318 Loss: 0.110142\n",
      "step :319 Loss: 0.187685\n",
      "step :320 Loss: 0.173975\n",
      "step :321 Loss: 0.0995729\n",
      "step :322 Loss: 0.125844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :323 Loss: 0.122542\n",
      "step :324 Loss: 0.10092\n",
      "step :325 Loss: 0.140345\n",
      "step :326 Loss: 0.249168\n",
      "step :327 Loss: 0.131779\n",
      "step :328 Loss: 0.0789244\n",
      "step :329 Loss: 0.078738\n",
      "step :330 Loss: 0.141744\n",
      "step :331 Loss: 0.158673\n",
      "step :332 Loss: 0.167512\n",
      "step :333 Loss: 0.127524\n",
      "step :334 Loss: 0.0409013\n",
      "step :335 Loss: 0.153578\n",
      "step :336 Loss: 0.171723\n",
      "step :337 Loss: 0.0523204\n",
      "step :338 Loss: 0.136068\n",
      "step :339 Loss: 0.0418608\n",
      "step :340 Loss: 0.160776\n",
      "step :341 Loss: 0.109327\n",
      "step :342 Loss: 0.0882752\n",
      "step :343 Loss: 0.185401\n",
      "step :344 Loss: 0.0702564\n",
      "step :345 Loss: 0.0520171\n",
      "step :346 Loss: 0.183638\n",
      "step :347 Loss: 0.142665\n",
      "step :348 Loss: 0.15158\n",
      "step :349 Loss: 0.106218\n",
      "step :350 Loss: 0.152497\n",
      "step :351 Loss: 0.117833\n",
      "step :352 Loss: 0.155865\n",
      "step :353 Loss: 0.202348\n",
      "step :354 Loss: 0.154382\n",
      "step :355 Loss: 0.107271\n",
      "step :356 Loss: 0.0974207\n",
      "step :357 Loss: 0.19206\n",
      "step :358 Loss: 0.170244\n",
      "step :359 Loss: 0.135693\n",
      "step :360 Loss: 0.0781183\n",
      "step :361 Loss: 0.194\n",
      "step :362 Loss: 0.0633043\n",
      "step :363 Loss: 0.0747381\n",
      "step :364 Loss: 0.064945\n",
      "step :365 Loss: 0.0971006\n",
      "step :366 Loss: 0.151175\n",
      "step :367 Loss: 0.0909223\n",
      "step :368 Loss: 0.06684\n",
      "step :369 Loss: 0.130309\n",
      "step :370 Loss: 0.0373187\n",
      "step :371 Loss: 0.231388\n",
      "step :372 Loss: 0.141485\n",
      "step :373 Loss: 0.132557\n",
      "step :374 Loss: 0.153063\n",
      "step :375 Loss: 0.160874\n",
      "step :376 Loss: 0.106255\n",
      "step :377 Loss: 0.240323\n",
      "step :378 Loss: 0.0356778\n",
      "step :379 Loss: 0.199493\n",
      "step :380 Loss: 0.136163\n",
      "step :381 Loss: 0.168997\n",
      "step :382 Loss: 0.191978\n",
      "step :383 Loss: 0.0535739\n",
      "step :384 Loss: 0.0961978\n",
      "step :385 Loss: 0.0828977\n",
      "step :386 Loss: 0.0524551\n",
      "step :387 Loss: 0.154739\n",
      "step :388 Loss: 0.149156\n",
      "step :389 Loss: 0.158019\n",
      "step :390 Loss: 0.0489659\n",
      "step :391 Loss: 0.120582\n",
      "step :392 Loss: 0.0972282\n",
      "step :393 Loss: 0.0928329\n",
      "step :394 Loss: 0.0383417\n",
      "step :395 Loss: 0.0807943\n",
      "step :396 Loss: 0.0487309\n",
      "step :397 Loss: 0.20047\n",
      "step :398 Loss: 0.124952\n",
      "step :399 Loss: 0.12848\n",
      "step :400 Loss: 0.151441\n",
      "step :401 Loss: 0.0803062\n",
      "step :402 Loss: 0.0901311\n",
      "step :403 Loss: 0.152125\n",
      "step :404 Loss: 0.195129\n",
      "step :405 Loss: 0.210775\n",
      "step :406 Loss: 0.0905404\n",
      "step :407 Loss: 0.191329\n",
      "step :408 Loss: 0.122536\n",
      "step :409 Loss: 0.120673\n",
      "step :410 Loss: 0.174242\n",
      "step :411 Loss: 0.0936958\n",
      "step :412 Loss: 0.0837808\n",
      "step :413 Loss: 0.186394\n",
      "step :414 Loss: 0.147037\n",
      "step :415 Loss: 0.0847311\n",
      "step :416 Loss: 0.0964248\n",
      "step :417 Loss: 0.321341\n",
      "step :418 Loss: 0.198672\n",
      "step :419 Loss: 0.125553\n",
      "step :420 Loss: 0.123661\n",
      "step :421 Loss: 0.103767\n",
      "step :422 Loss: 0.279299\n",
      "step :423 Loss: 0.180684\n",
      "step :424 Loss: 0.137174\n",
      "step :425 Loss: 0.138635\n",
      "step :426 Loss: 0.204869\n",
      "step :427 Loss: 0.171885\n",
      "step :428 Loss: 0.136769\n",
      "step :429 Loss: 0.0969182\n",
      "step :430 Loss: 0.041344\n",
      "step :431 Loss: 0.0223758\n",
      "step :432 Loss: 0.179672\n",
      "step :433 Loss: 0.104515\n",
      "step :434 Loss: 0.119674\n",
      "step :435 Loss: 0.0893083\n",
      "step :436 Loss: 0.165113\n",
      "step :437 Loss: 0.126148\n",
      "step :438 Loss: 0.119156\n",
      "step :439 Loss: 0.19577\n",
      "step :440 Loss: 0.0731929\n",
      "step :441 Loss: 0.128863\n",
      "step :442 Loss: 0.114386\n",
      "step :443 Loss: 0.114299\n",
      "step :444 Loss: 0.140993\n",
      "step :445 Loss: 0.173222\n",
      "step :446 Loss: 0.18289\n",
      "step :447 Loss: 0.130424\n",
      "step :448 Loss: 0.13863\n",
      "step :449 Loss: 0.147316\n",
      "step :450 Loss: 0.266834\n",
      "step :451 Loss: 0.143769\n",
      "step :452 Loss: 0.135426\n",
      "step :453 Loss: 0.0560725\n",
      "step :454 Loss: 0.0768555\n",
      "step :455 Loss: 0.093379\n",
      "step :456 Loss: 0.10862\n",
      "step :457 Loss: 0.203262\n",
      "step :458 Loss: 0.126104\n",
      "step :459 Loss: 0.204521\n",
      "step :460 Loss: 0.0350381\n",
      "step :461 Loss: 0.0566102\n",
      "step :462 Loss: 0.0915501\n",
      "step :463 Loss: 0.177715\n",
      "step :464 Loss: 0.106578\n",
      "step :465 Loss: 0.19051\n",
      "step :466 Loss: 0.175595\n",
      "step :467 Loss: 0.222261\n",
      "step :468 Loss: 0.0576067\n",
      "step :469 Loss: 0.0538614\n",
      "step :470 Loss: 0.153566\n",
      "step :471 Loss: 0.107078\n",
      "step :472 Loss: 0.0676802\n",
      "step :473 Loss: 0.113707\n",
      "step :474 Loss: 0.185469\n",
      "step :475 Loss: 0.0933744\n",
      "step :476 Loss: 0.054372\n",
      "step :477 Loss: 0.144045\n",
      "step :478 Loss: 0.185914\n",
      "step :479 Loss: 0.0631269\n",
      "step :480 Loss: 0.093087\n",
      "step :481 Loss: 0.145678\n",
      "step :482 Loss: 0.120793\n",
      "step :483 Loss: 0.14968\n",
      "step :484 Loss: 0.0971614\n",
      "step :485 Loss: 0.279139\n",
      "step :486 Loss: 0.0924082\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn16s_final.ckpt\n",
      "step :487 Loss: 0.0405364\n",
      "step :488 Loss: 0.0474759\n",
      "step :489 Loss: 0.0420954\n",
      "step :490 Loss: 0.10884\n",
      "step :491 Loss: 0.150431\n",
      "step :492 Loss: 0.14071\n",
      "step :493 Loss: 0.0836815\n",
      "step :494 Loss: 0.118502\n",
      "step :495 Loss: 0.0486462\n",
      "step :496 Loss: 0.153809\n",
      "step :497 Loss: 0.14727\n",
      "step :498 Loss: 0.15473\n",
      "step :499 Loss: 0.0733829\n",
      "step :500 Loss: 0.300145\n",
      "step :501 Loss: 0.154881\n",
      "step :502 Loss: 0.124601\n",
      "step :503 Loss: 0.118155\n",
      "step :504 Loss: 0.106862\n",
      "step :505 Loss: 0.0833296\n",
      "step :506 Loss: 0.152036\n",
      "step :507 Loss: 0.0589895\n",
      "step :508 Loss: 0.0973412\n",
      "step :509 Loss: 0.0883843\n",
      "step :510 Loss: 0.0506766\n",
      "step :511 Loss: 0.0512017\n",
      "step :512 Loss: 0.0737756\n",
      "step :513 Loss: 0.162816\n",
      "step :514 Loss: 0.171907\n",
      "step :515 Loss: 0.0326572\n",
      "step :516 Loss: 0.108012\n",
      "step :517 Loss: 0.134425\n",
      "step :518 Loss: 0.163602\n",
      "step :519 Loss: 0.198814\n",
      "step :520 Loss: 0.106486\n",
      "step :521 Loss: 0.174374\n",
      "step :522 Loss: 0.24606\n",
      "step :523 Loss: 0.231563\n",
      "step :524 Loss: 0.117628\n",
      "step :525 Loss: 0.256755\n",
      "step :526 Loss: 0.145083\n",
      "step :527 Loss: 0.0726367\n",
      "step :528 Loss: 0.222572\n",
      "step :529 Loss: 0.139279\n",
      "step :530 Loss: 0.275313\n",
      "step :531 Loss: 0.137445\n",
      "step :532 Loss: 0.17358\n",
      "step :533 Loss: 0.265783\n",
      "step :534 Loss: 0.194831\n",
      "step :535 Loss: 0.213802\n",
      "step :536 Loss: 0.102498\n",
      "step :537 Loss: 0.133691\n",
      "step :538 Loss: 0.193143\n",
      "step :539 Loss: 0.138994\n",
      "step :540 Loss: 0.0330699\n",
      "step :541 Loss: 0.11074\n",
      "step :542 Loss: 0.123476\n",
      "step :543 Loss: 0.0861865\n",
      "step :544 Loss: 0.119744\n",
      "step :545 Loss: 0.056673\n",
      "step :546 Loss: 0.157842\n",
      "step :547 Loss: 0.399029\n",
      "step :548 Loss: 0.177088\n",
      "step :549 Loss: 0.132494\n",
      "step :550 Loss: 0.107386\n",
      "step :551 Loss: 0.107135\n",
      "step :552 Loss: 0.138325\n",
      "step :553 Loss: 0.188707\n",
      "step :554 Loss: 0.0853735\n",
      "step :555 Loss: 0.207041\n",
      "step :556 Loss: 0.143284\n",
      "step :557 Loss: 0.136626\n",
      "step :558 Loss: 0.111949\n",
      "step :559 Loss: 0.248363\n",
      "step :560 Loss: 0.0936588\n",
      "step :561 Loss: 0.252546\n",
      "step :562 Loss: 0.159626\n",
      "step :563 Loss: 0.103459\n",
      "step :564 Loss: 0.0793425\n",
      "step :565 Loss: 0.172628\n",
      "step :566 Loss: 0.0176418\n",
      "step :567 Loss: 0.189353\n",
      "step :568 Loss: 0.14147\n",
      "step :569 Loss: 0.123502\n",
      "step :570 Loss: 0.0286963\n",
      "step :571 Loss: 0.178918\n",
      "step :572 Loss: 0.0814595\n",
      "step :573 Loss: 0.138884\n",
      "step :574 Loss: 0.202941\n",
      "step :575 Loss: 0.164782\n",
      "step :576 Loss: 0.0399482\n",
      "step :577 Loss: 0.149041\n",
      "step :578 Loss: 0.0414182\n",
      "step :579 Loss: 0.142233\n",
      "step :580 Loss: 0.124326\n",
      "step :581 Loss: 0.117823\n",
      "step :582 Loss: 0.0612889\n",
      "step :583 Loss: 0.0842162\n",
      "step :584 Loss: 0.0931557\n",
      "step :585 Loss: 0.242421\n",
      "step :586 Loss: 0.073288\n",
      "step :587 Loss: 0.117488\n",
      "step :588 Loss: 0.159506\n",
      "step :589 Loss: 0.170629\n",
      "step :590 Loss: 0.0967816\n",
      "step :591 Loss: 0.100961\n",
      "step :592 Loss: 0.115409\n",
      "step :593 Loss: 0.13201\n",
      "step :594 Loss: 0.0561132\n",
      "step :595 Loss: 0.106147\n",
      "step :596 Loss: 0.105772\n",
      "step :597 Loss: 0.22119\n",
      "step :598 Loss: 0.158723\n",
      "step :599 Loss: 0.128605\n",
      "step :600 Loss: 0.183774\n",
      "step :601 Loss: 0.122045\n",
      "step :602 Loss: 0.0477714\n",
      "step :603 Loss: 0.221329\n",
      "step :604 Loss: 0.26107\n",
      "step :605 Loss: 0.0894368\n",
      "step :606 Loss: 0.168029\n",
      "step :607 Loss: 0.145344\n",
      "step :608 Loss: 0.131172\n",
      "step :609 Loss: 0.0958731\n",
      "step :610 Loss: 0.0938959\n",
      "step :611 Loss: 0.0949372\n",
      "step :612 Loss: 0.224044\n",
      "step :613 Loss: 0.0184285\n",
      "step :614 Loss: 0.0959861\n",
      "step :615 Loss: 0.149996\n",
      "step :616 Loss: 0.116011\n",
      "step :617 Loss: 0.103018\n",
      "step :618 Loss: 0.109539\n",
      "step :619 Loss: 0.0791579\n",
      "step :620 Loss: 0.118344\n",
      "step :621 Loss: 0.100982\n",
      "step :622 Loss: 0.0868997\n",
      "step :623 Loss: 0.179415\n",
      "step :624 Loss: 0.206854\n",
      "step :625 Loss: 0.224486\n",
      "step :626 Loss: 0.0882019\n",
      "step :627 Loss: 0.103678\n",
      "step :628 Loss: 0.0841083\n",
      "step :629 Loss: 0.0961316\n",
      "step :630 Loss: 0.0618574\n",
      "step :631 Loss: 0.113619\n",
      "step :632 Loss: 0.213258\n",
      "step :633 Loss: 0.137815\n",
      "step :634 Loss: 0.157725\n",
      "step :635 Loss: 0.0951136\n",
      "step :636 Loss: 0.117937\n",
      "step :637 Loss: 0.135395\n",
      "step :638 Loss: 0.151435\n",
      "step :639 Loss: 0.026907\n",
      "step :640 Loss: 0.111858\n",
      "step :641 Loss: 0.0927316\n",
      "step :642 Loss: 0.189099\n",
      "step :643 Loss: 0.172132\n",
      "step :644 Loss: 0.165931\n",
      "step :645 Loss: 0.0794777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :646 Loss: 0.0285404\n",
      "step :647 Loss: 0.0891457\n",
      "step :648 Loss: 0.0712594\n",
      "step :649 Loss: 0.20911\n",
      "step :650 Loss: 0.200012\n",
      "step :651 Loss: 0.302009\n",
      "step :652 Loss: 0.0544589\n",
      "step :653 Loss: 0.047568\n",
      "step :654 Loss: 0.130164\n",
      "step :655 Loss: 0.17175\n",
      "step :656 Loss: 0.115925\n",
      "step :657 Loss: 0.117491\n",
      "step :658 Loss: 0.133351\n",
      "step :659 Loss: 0.13336\n",
      "step :660 Loss: 0.149394\n",
      "step :661 Loss: 0.23286\n",
      "step :662 Loss: 0.113439\n",
      "step :663 Loss: 0.225383\n",
      "step :664 Loss: 0.132231\n",
      "step :665 Loss: 0.132537\n",
      "step :666 Loss: 0.0818166\n",
      "step :667 Loss: 0.098675\n",
      "step :668 Loss: 0.130928\n",
      "step :669 Loss: 0.0817765\n",
      "step :670 Loss: 0.357762\n",
      "step :671 Loss: 0.153639\n",
      "step :672 Loss: 0.153727\n",
      "step :673 Loss: 0.0758151\n",
      "step :674 Loss: 0.0648266\n",
      "step :675 Loss: 0.158712\n",
      "step :676 Loss: 0.138761\n",
      "step :677 Loss: 0.131191\n",
      "step :678 Loss: 0.233919\n",
      "step :679 Loss: 0.174041\n",
      "step :680 Loss: 0.0735399\n",
      "step :681 Loss: 0.11978\n",
      "step :682 Loss: 0.195001\n",
      "step :683 Loss: 0.137406\n",
      "step :684 Loss: 0.157004\n",
      "step :685 Loss: 0.172508\n",
      "step :686 Loss: 0.0658923\n",
      "step :687 Loss: 0.25222\n",
      "step :688 Loss: 0.038821\n",
      "step :689 Loss: 0.122167\n",
      "step :690 Loss: 0.143052\n",
      "step :691 Loss: 0.0859425\n",
      "step :692 Loss: 0.2901\n",
      "step :693 Loss: 0.105624\n",
      "step :694 Loss: 0.225247\n",
      "step :695 Loss: 0.165313\n",
      "step :696 Loss: 0.106495\n",
      "step :697 Loss: 0.0329391\n",
      "step :698 Loss: 0.0876157\n",
      "step :699 Loss: 0.105198\n",
      "step :700 Loss: 0.108919\n",
      "step :701 Loss: 0.154471\n",
      "step :702 Loss: 0.0669042\n",
      "step :703 Loss: 0.239734\n",
      "step :704 Loss: 0.198923\n",
      "step :705 Loss: 0.105302\n",
      "step :706 Loss: 0.281839\n",
      "step :707 Loss: 0.134204\n",
      "step :708 Loss: 0.138534\n",
      "step :709 Loss: 0.0814944\n",
      "step :710 Loss: 0.193672\n",
      "step :711 Loss: 0.0646189\n",
      "step :712 Loss: 0.158304\n",
      "step :713 Loss: 0.0665015\n",
      "step :714 Loss: 0.147203\n",
      "step :715 Loss: 0.0763569\n",
      "step :716 Loss: 0.149674\n",
      "step :717 Loss: 0.0590121\n",
      "step :718 Loss: 0.179021\n",
      "step :719 Loss: 0.210379\n",
      "step :720 Loss: 0.10598\n",
      "step :721 Loss: 0.0886403\n",
      "step :722 Loss: 0.124883\n",
      "step :723 Loss: 0.133891\n",
      "step :724 Loss: 0.110786\n",
      "step :725 Loss: 0.0726858\n",
      "step :726 Loss: 0.0747721\n",
      "step :727 Loss: 0.135263\n",
      "step :728 Loss: 0.122841\n",
      "step :729 Loss: 0.193082\n",
      "step :730 Loss: 0.140178\n",
      "step :731 Loss: 0.308008\n",
      "step :732 Loss: 0.0772049\n",
      "step :733 Loss: 0.179694\n",
      "step :734 Loss: 0.100131\n",
      "step :735 Loss: 0.255198\n",
      "step :736 Loss: 0.200394\n",
      "step :737 Loss: 0.18887\n",
      "step :738 Loss: 0.0207787\n",
      "step :739 Loss: 0.10701\n",
      "step :740 Loss: 0.0572686\n",
      "step :741 Loss: 0.306489\n",
      "step :742 Loss: 0.246218\n",
      "step :743 Loss: 0.145602\n",
      "step :744 Loss: 0.160647\n",
      "step :745 Loss: 0.145818\n",
      "step :746 Loss: 0.181686\n",
      "step :747 Loss: 0.122714\n",
      "step :748 Loss: 0.120255\n",
      "step :749 Loss: 0.177859\n",
      "step :750 Loss: 0.122432\n",
      "step :751 Loss: 0.153076\n",
      "step :752 Loss: 0.185\n",
      "step :753 Loss: 0.153216\n",
      "step :754 Loss: 0.0688211\n",
      "step :755 Loss: 0.0644098\n",
      "step :756 Loss: 0.156272\n",
      "step :757 Loss: 0.160515\n",
      "step :758 Loss: 0.0761072\n",
      "step :759 Loss: 0.0936177\n",
      "step :760 Loss: 0.132976\n",
      "step :761 Loss: 0.0782771\n",
      "step :762 Loss: 0.118116\n",
      "step :763 Loss: 0.101204\n",
      "step :764 Loss: 0.132632\n",
      "step :765 Loss: 0.0924921\n",
      "step :766 Loss: 0.195012\n",
      "step :767 Loss: 0.188755\n",
      "step :768 Loss: 0.14972\n",
      "step :769 Loss: 0.158179\n",
      "step :770 Loss: 0.144859\n",
      "step :771 Loss: 0.137743\n",
      "step :772 Loss: 0.107513\n",
      "step :773 Loss: 0.144451\n",
      "step :774 Loss: 0.0416822\n",
      "step :775 Loss: 0.139376\n",
      "step :776 Loss: 0.159109\n",
      "step :777 Loss: 0.143844\n",
      "step :778 Loss: 0.0809019\n",
      "step :779 Loss: 0.126818\n",
      "step :780 Loss: 0.162337\n",
      "step :781 Loss: 0.243603\n",
      "step :782 Loss: 0.0259934\n",
      "step :783 Loss: 0.203425\n",
      "step :784 Loss: 0.107597\n",
      "step :785 Loss: 0.0888371\n",
      "step :786 Loss: 0.0822532\n",
      "step :787 Loss: 0.153611\n",
      "step :788 Loss: 0.19199\n",
      "step :789 Loss: 0.018951\n",
      "step :790 Loss: 0.0813209\n",
      "step :791 Loss: 0.0880476\n",
      "step :792 Loss: 0.125862\n",
      "step :793 Loss: 0.130813\n",
      "step :794 Loss: 0.146815\n",
      "step :795 Loss: 0.146163\n",
      "step :796 Loss: 0.0700543\n",
      "step :797 Loss: 0.0646294\n",
      "step :798 Loss: 0.152244\n",
      "step :799 Loss: 0.126884\n",
      "step :800 Loss: 0.0960538\n",
      "step :801 Loss: 0.198142\n",
      "step :802 Loss: 0.0557839\n",
      "step :803 Loss: 0.098044\n",
      "step :804 Loss: 0.0866849\n",
      "step :805 Loss: 0.240649\n",
      "step :806 Loss: 0.174074\n",
      "step :807 Loss: 0.0938388\n",
      "step :808 Loss: 0.132659\n",
      "step :809 Loss: 0.0625604\n",
      "step :810 Loss: 0.121708\n",
      "step :811 Loss: 0.163429\n",
      "step :812 Loss: 0.0478155\n",
      "step :813 Loss: 0.0972832\n",
      "step :814 Loss: 0.139451\n",
      "step :815 Loss: 0.136979\n",
      "step :816 Loss: 0.156794\n",
      "step :817 Loss: 0.0522286\n",
      "step :818 Loss: 0.265986\n",
      "step :819 Loss: 0.0829601\n",
      "step :820 Loss: 0.0579176\n",
      "step :821 Loss: 0.152173\n",
      "step :822 Loss: 0.175534\n",
      "step :823 Loss: 0.035777\n",
      "step :824 Loss: 0.0964527\n",
      "step :825 Loss: 0.225909\n",
      "step :826 Loss: 0.110019\n",
      "step :827 Loss: 0.0972608\n",
      "step :828 Loss: 0.0613273\n",
      "step :829 Loss: 0.123989\n",
      "step :830 Loss: 0.0806203\n",
      "step :831 Loss: 0.0787559\n",
      "step :832 Loss: 0.150493\n",
      "step :833 Loss: 0.251194\n",
      "step :834 Loss: 0.0586348\n",
      "step :835 Loss: 0.197659\n",
      "step :836 Loss: 0.144127\n",
      "step :837 Loss: 0.33889\n",
      "step :838 Loss: 0.09777\n",
      "step :839 Loss: 0.146602\n",
      "step :840 Loss: 0.069713\n",
      "step :841 Loss: 0.108353\n",
      "step :842 Loss: 0.0874338\n",
      "step :843 Loss: 0.175464\n",
      "step :844 Loss: 0.0955368\n",
      "step :845 Loss: 0.0934379\n",
      "step :846 Loss: 0.134872\n",
      "step :847 Loss: 0.0973566\n",
      "step :848 Loss: 0.125322\n",
      "step :849 Loss: 0.117088\n",
      "step :850 Loss: 0.193072\n",
      "step :851 Loss: 0.0552668\n",
      "step :852 Loss: 0.244022\n",
      "step :853 Loss: 0.0906311\n",
      "step :854 Loss: 0.105524\n",
      "step :855 Loss: 0.129144\n",
      "step :856 Loss: 0.184231\n",
      "step :857 Loss: 0.0712325\n",
      "step :858 Loss: 0.237888\n",
      "step :859 Loss: 0.186552\n",
      "step :860 Loss: 0.0824597\n",
      "step :861 Loss: 0.139862\n",
      "step :862 Loss: 0.0954796\n",
      "step :863 Loss: 0.140929\n",
      "step :864 Loss: 0.157003\n",
      "step :865 Loss: 0.0382002\n",
      "step :866 Loss: 0.162763\n",
      "step :867 Loss: 0.0491326\n",
      "step :868 Loss: 0.106171\n",
      "step :869 Loss: 0.154422\n",
      "step :870 Loss: 0.149388\n",
      "step :871 Loss: 0.104964\n",
      "step :872 Loss: 0.20429\n",
      "step :873 Loss: 0.124451\n",
      "step :874 Loss: 0.105432\n",
      "step :875 Loss: 0.117352\n",
      "step :876 Loss: 0.140254\n",
      "step :877 Loss: 0.188922\n",
      "step :878 Loss: 0.0806443\n",
      "step :879 Loss: 0.0978169\n",
      "step :880 Loss: 0.0553393\n",
      "step :881 Loss: 0.136575\n",
      "step :882 Loss: 0.434671\n",
      "step :883 Loss: 0.0712499\n",
      "step :884 Loss: 0.185856\n",
      "step :885 Loss: 0.170418\n",
      "step :886 Loss: 0.129249\n",
      "step :887 Loss: 0.0988254\n",
      "step :888 Loss: 0.0503907\n",
      "step :889 Loss: 0.140029\n",
      "step :890 Loss: 0.0727199\n",
      "step :891 Loss: 0.0657342\n",
      "step :892 Loss: 0.243573\n",
      "step :893 Loss: 0.212283\n",
      "step :894 Loss: 0.0724828\n",
      "step :895 Loss: 0.123724\n",
      "step :896 Loss: 0.11643\n",
      "step :897 Loss: 0.0410118\n",
      "step :898 Loss: 0.18424\n",
      "step :899 Loss: 0.0735906\n",
      "step :900 Loss: 0.146662\n",
      "step :901 Loss: 0.110847\n",
      "step :902 Loss: 0.207445\n",
      "step :903 Loss: 0.227667\n",
      "step :904 Loss: 0.042494\n",
      "step :905 Loss: 0.0693413\n",
      "step :906 Loss: 0.0619877\n",
      "step :907 Loss: 0.117164\n",
      "step :908 Loss: 0.0795712\n",
      "step :909 Loss: 0.114989\n",
      "step :910 Loss: 0.272373\n",
      "step :911 Loss: 0.0621778\n",
      "step :912 Loss: 0.0369306\n",
      "step :913 Loss: 0.104042\n",
      "step :914 Loss: 0.165161\n",
      "step :915 Loss: 0.139774\n",
      "step :916 Loss: 0.0312247\n",
      "step :917 Loss: 0.0850719\n",
      "step :918 Loss: 0.108061\n",
      "step :919 Loss: 0.21545\n",
      "step :920 Loss: 0.144352\n",
      "step :921 Loss: 0.0817201\n",
      "step :922 Loss: 0.0962606\n",
      "step :923 Loss: 0.219588\n",
      "step :924 Loss: 0.199017\n",
      "step :925 Loss: 0.108712\n",
      "step :926 Loss: 0.141575\n",
      "step :927 Loss: 0.13116\n",
      "step :928 Loss: 0.214143\n",
      "step :929 Loss: 0.0501454\n",
      "step :930 Loss: 0.150281\n",
      "step :931 Loss: 0.125283\n",
      "step :932 Loss: 0.065963\n",
      "step :933 Loss: 0.101723\n",
      "step :934 Loss: 0.113472\n",
      "step :935 Loss: 0.0945135\n",
      "step :936 Loss: 0.26869\n",
      "step :937 Loss: 0.106295\n",
      "step :938 Loss: 0.145791\n",
      "step :939 Loss: 0.130918\n",
      "step :940 Loss: 0.118239\n",
      "step :941 Loss: 0.0923815\n",
      "step :942 Loss: 0.0869716\n",
      "step :943 Loss: 0.0879557\n",
      "step :944 Loss: 0.142263\n",
      "step :945 Loss: 0.13086\n",
      "step :946 Loss: 0.0455823\n",
      "step :947 Loss: 0.16098\n",
      "step :948 Loss: 0.236345\n",
      "step :949 Loss: 0.0635985\n",
      "step :950 Loss: 0.079107\n",
      "step :951 Loss: 0.120764\n",
      "step :952 Loss: 0.168502\n",
      "step :953 Loss: 0.166258\n",
      "step :954 Loss: 0.0971106\n",
      "step :955 Loss: 0.0336278\n",
      "step :956 Loss: 0.118446\n",
      "step :957 Loss: 0.13563\n",
      "step :958 Loss: 0.166953\n",
      "step :959 Loss: 0.247467\n",
      "step :960 Loss: 0.216262\n",
      "step :961 Loss: 0.0816084\n",
      "step :962 Loss: 0.132261\n",
      "step :963 Loss: 0.147288\n",
      "step :964 Loss: 0.131531\n",
      "step :965 Loss: 0.133069\n",
      "step :966 Loss: 0.0394459\n",
      "step :967 Loss: 0.195124\n",
      "step :968 Loss: 0.119695\n",
      "step :969 Loss: 0.246674\n",
      "step :970 Loss: 0.193637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :971 Loss: 0.132261\n",
      "step :972 Loss: 0.235683\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn16s_final.ckpt\n",
      "step :973 Loss: 0.11998\n",
      "step :974 Loss: 0.152946\n",
      "step :975 Loss: 0.0838493\n",
      "step :976 Loss: 0.112153\n",
      "step :977 Loss: 0.302278\n",
      "step :978 Loss: 0.0787115\n",
      "step :979 Loss: 0.259138\n",
      "step :980 Loss: 0.137562\n",
      "step :981 Loss: 0.241011\n",
      "step :982 Loss: 0.0850359\n",
      "step :983 Loss: 0.095538\n",
      "step :984 Loss: 0.18589\n",
      "step :985 Loss: 0.0640356\n",
      "step :986 Loss: 0.0720918\n",
      "step :987 Loss: 0.127889\n",
      "step :988 Loss: 0.148029\n",
      "step :989 Loss: 0.264517\n",
      "step :990 Loss: 0.25365\n",
      "step :991 Loss: 0.238294\n",
      "step :992 Loss: 0.167987\n",
      "step :993 Loss: 0.175498\n",
      "step :994 Loss: 0.172475\n",
      "step :995 Loss: 0.0480551\n",
      "step :996 Loss: 0.141083\n",
      "step :997 Loss: 0.148905\n",
      "step :998 Loss: 0.16309\n",
      "step :999 Loss: 0.235332\n",
      "step :1000 Loss: 0.100301\n",
      "step :1001 Loss: 0.0399039\n",
      "step :1002 Loss: 0.169208\n",
      "step :1003 Loss: 0.0898172\n",
      "step :1004 Loss: 0.124095\n",
      "step :1005 Loss: 0.151779\n",
      "step :1006 Loss: 0.163037\n",
      "step :1007 Loss: 0.140407\n",
      "step :1008 Loss: 0.0592491\n",
      "step :1009 Loss: 0.121469\n",
      "step :1010 Loss: 0.076688\n",
      "step :1011 Loss: 0.110163\n",
      "step :1012 Loss: 0.235046\n",
      "step :1013 Loss: 0.0679804\n",
      "step :1014 Loss: 0.120956\n",
      "step :1015 Loss: 0.102077\n",
      "step :1016 Loss: 0.238299\n",
      "step :1017 Loss: 0.245067\n",
      "step :1018 Loss: 0.0892164\n",
      "step :1019 Loss: 0.149645\n",
      "step :1020 Loss: 0.154578\n",
      "step :1021 Loss: 0.0763093\n",
      "step :1022 Loss: 0.229548\n",
      "step :1023 Loss: 0.194098\n",
      "step :1024 Loss: 0.135449\n",
      "step :1025 Loss: 0.10642\n",
      "step :1026 Loss: 0.0971207\n",
      "step :1027 Loss: 0.0878628\n",
      "step :1028 Loss: 0.110039\n",
      "step :1029 Loss: 0.181443\n",
      "step :1030 Loss: 0.104698\n",
      "step :1031 Loss: 0.158483\n",
      "step :1032 Loss: 0.166464\n",
      "step :1033 Loss: 0.0623166\n",
      "step :1034 Loss: 0.0621598\n",
      "step :1035 Loss: 0.237412\n",
      "step :1036 Loss: 0.229341\n",
      "step :1037 Loss: 0.0635373\n",
      "step :1038 Loss: 0.141158\n",
      "step :1039 Loss: 0.109263\n",
      "step :1040 Loss: 0.0398776\n",
      "step :1041 Loss: 0.0829297\n",
      "step :1042 Loss: 0.0940076\n",
      "step :1043 Loss: 0.125604\n",
      "step :1044 Loss: 0.113784\n",
      "step :1045 Loss: 0.154934\n",
      "step :1046 Loss: 0.173071\n",
      "step :1047 Loss: 0.257689\n",
      "step :1048 Loss: 0.147165\n",
      "step :1049 Loss: 0.0800207\n",
      "step :1050 Loss: 0.056356\n",
      "step :1051 Loss: 0.116085\n",
      "step :1052 Loss: 0.0960681\n",
      "step :1053 Loss: 0.075865\n",
      "step :1054 Loss: 0.237046\n",
      "step :1055 Loss: 0.165204\n",
      "step :1056 Loss: 0.231808\n",
      "step :1057 Loss: 0.0711205\n",
      "step :1058 Loss: 0.108782\n",
      "step :1059 Loss: 0.167755\n",
      "step :1060 Loss: 0.126208\n",
      "step :1061 Loss: 0.115132\n",
      "step :1062 Loss: 0.139705\n",
      "step :1063 Loss: 0.127605\n",
      "step :1064 Loss: 0.086121\n",
      "step :1065 Loss: 0.164881\n",
      "step :1066 Loss: 0.0773891\n",
      "step :1067 Loss: 0.168933\n",
      "step :1068 Loss: 0.194918\n",
      "step :1069 Loss: 0.259409\n",
      "step :1070 Loss: 0.0789783\n",
      "step :1071 Loss: 0.165926\n",
      "step :1072 Loss: 0.119156\n",
      "step :1073 Loss: 0.0961374\n",
      "step :1074 Loss: 0.180768\n",
      "step :1075 Loss: 0.117181\n",
      "step :1076 Loss: 0.0758624\n",
      "step :1077 Loss: 0.0847299\n",
      "step :1078 Loss: 0.111397\n",
      "step :1079 Loss: 0.0648761\n",
      "step :1080 Loss: 0.155478\n",
      "step :1081 Loss: 0.128976\n",
      "step :1082 Loss: 0.251687\n",
      "step :1083 Loss: 0.10807\n",
      "step :1084 Loss: 0.120997\n",
      "step :1085 Loss: 0.0418153\n",
      "step :1086 Loss: 0.0873965\n",
      "step :1087 Loss: 0.170628\n",
      "step :1088 Loss: 0.142119\n",
      "step :1089 Loss: 0.0587812\n",
      "step :1090 Loss: 0.0668125\n",
      "step :1091 Loss: 0.0900796\n",
      "step :1092 Loss: 0.20988\n",
      "step :1093 Loss: 0.090264\n",
      "step :1094 Loss: 0.103242\n",
      "step :1095 Loss: 0.161776\n",
      "step :1096 Loss: 0.131975\n",
      "step :1097 Loss: 0.24196\n",
      "step :1098 Loss: 0.118326\n",
      "step :1099 Loss: 0.119564\n",
      "step :1100 Loss: 0.0847125\n",
      "step :1101 Loss: 0.107305\n",
      "step :1102 Loss: 0.0780503\n",
      "step :1103 Loss: 0.113027\n",
      "step :1104 Loss: 0.125313\n",
      "step :1105 Loss: 0.189601\n",
      "step :1106 Loss: 0.0639328\n",
      "step :1107 Loss: 0.112796\n",
      "step :1108 Loss: 0.0726398\n",
      "step :1109 Loss: 0.132465\n",
      "step :1110 Loss: 0.117128\n",
      "step :1111 Loss: 0.110064\n",
      "step :1112 Loss: 0.137402\n",
      "step :1113 Loss: 0.112232\n",
      "step :1114 Loss: 0.106779\n",
      "step :1115 Loss: 0.265298\n",
      "step :1116 Loss: 0.0634624\n",
      "step :1117 Loss: 0.136461\n",
      "step :1118 Loss: 0.0441755\n",
      "step :1119 Loss: 0.0838707\n",
      "step :1120 Loss: 0.126883\n",
      "step :1121 Loss: 0.0877353\n",
      "step :1122 Loss: 0.116365\n",
      "step :1123 Loss: 0.0421867\n",
      "step :1124 Loss: 0.202058\n",
      "step :1125 Loss: 0.14393\n",
      "step :1126 Loss: 0.178052\n",
      "step :1127 Loss: 0.107648\n",
      "step :1128 Loss: 0.122136\n",
      "step :1129 Loss: 0.176291\n",
      "step :1130 Loss: 0.158152\n",
      "step :1131 Loss: 0.0447041\n",
      "step :1132 Loss: 0.257041\n",
      "step :1133 Loss: 0.0778604\n",
      "step :1134 Loss: 0.121024\n",
      "step :1135 Loss: 0.240725\n",
      "step :1136 Loss: 0.152912\n",
      "step :1137 Loss: 0.15184\n",
      "step :1138 Loss: 0.176799\n",
      "step :1139 Loss: 0.170961\n",
      "step :1140 Loss: 0.0589243\n",
      "step :1141 Loss: 0.0789994\n",
      "step :1142 Loss: 0.167165\n",
      "step :1143 Loss: 0.0573609\n",
      "step :1144 Loss: 0.0476324\n",
      "step :1145 Loss: 0.11971\n",
      "step :1146 Loss: 0.20498\n",
      "step :1147 Loss: 0.0914679\n",
      "step :1148 Loss: 0.193489\n",
      "step :1149 Loss: 0.0866558\n",
      "step :1150 Loss: 0.108614\n",
      "step :1151 Loss: 0.0877952\n",
      "step :1152 Loss: 0.196658\n",
      "step :1153 Loss: 0.0852078\n",
      "step :1154 Loss: 0.175985\n",
      "step :1155 Loss: 0.166114\n",
      "step :1156 Loss: 0.122621\n",
      "step :1157 Loss: 0.247677\n",
      "step :1158 Loss: 0.10696\n",
      "step :1159 Loss: 0.0846887\n",
      "step :1160 Loss: 0.185808\n",
      "step :1161 Loss: 0.0972913\n",
      "step :1162 Loss: 0.0812218\n",
      "step :1163 Loss: 0.2032\n",
      "step :1164 Loss: 0.159108\n",
      "step :1165 Loss: 0.149738\n",
      "step :1166 Loss: 0.0951394\n",
      "step :1167 Loss: 0.0406926\n",
      "step :1168 Loss: 0.229456\n",
      "step :1169 Loss: 0.0624773\n",
      "step :1170 Loss: 0.123437\n",
      "step :1171 Loss: 0.131482\n",
      "step :1172 Loss: 0.164365\n",
      "step :1173 Loss: 0.210724\n",
      "step :1174 Loss: 0.0750752\n",
      "step :1175 Loss: 0.183402\n",
      "step :1176 Loss: 0.195858\n",
      "step :1177 Loss: 0.159857\n",
      "step :1178 Loss: 0.10376\n",
      "step :1179 Loss: 0.158691\n",
      "step :1180 Loss: 0.0931103\n",
      "step :1181 Loss: 0.1463\n",
      "step :1182 Loss: 0.230914\n",
      "step :1183 Loss: 0.0662409\n",
      "step :1184 Loss: 0.0413246\n",
      "step :1185 Loss: 0.134175\n",
      "step :1186 Loss: 0.0193569\n",
      "step :1187 Loss: 0.0947828\n",
      "step :1188 Loss: 0.099327\n",
      "step :1189 Loss: 0.0999773\n",
      "step :1190 Loss: 0.133413\n",
      "step :1191 Loss: 0.229352\n",
      "step :1192 Loss: 0.103366\n",
      "step :1193 Loss: 0.0689985\n",
      "step :1194 Loss: 0.190735\n",
      "step :1195 Loss: 0.0799407\n",
      "step :1196 Loss: 0.135652\n",
      "step :1197 Loss: 0.0978992\n",
      "step :1198 Loss: 0.112092\n",
      "step :1199 Loss: 0.120804\n",
      "step :1200 Loss: 0.244805\n",
      "step :1201 Loss: 0.192004\n",
      "step :1202 Loss: 0.136715\n",
      "step :1203 Loss: 0.195882\n",
      "step :1204 Loss: 0.0829623\n",
      "step :1205 Loss: 0.0395528\n",
      "step :1206 Loss: 0.0323091\n",
      "step :1207 Loss: 0.132799\n",
      "step :1208 Loss: 0.0904799\n",
      "step :1209 Loss: 0.140192\n",
      "step :1210 Loss: 0.183653\n",
      "step :1211 Loss: 0.155123\n",
      "step :1212 Loss: 0.0878413\n",
      "step :1213 Loss: 0.141469\n",
      "step :1214 Loss: 0.213619\n",
      "step :1215 Loss: 0.104283\n",
      "step :1216 Loss: 0.11291\n",
      "step :1217 Loss: 0.0891503\n",
      "step :1218 Loss: 0.122571\n",
      "step :1219 Loss: 0.0998468\n",
      "step :1220 Loss: 0.183365\n",
      "step :1221 Loss: 0.0974393\n",
      "step :1222 Loss: 0.0809428\n",
      "step :1223 Loss: 0.0820545\n",
      "step :1224 Loss: 0.128769\n",
      "step :1225 Loss: 0.147447\n",
      "step :1226 Loss: 0.12268\n",
      "step :1227 Loss: 0.0784683\n",
      "step :1228 Loss: 0.109248\n",
      "step :1229 Loss: 0.200322\n",
      "step :1230 Loss: 0.207012\n",
      "step :1231 Loss: 0.0761044\n",
      "step :1232 Loss: 0.0736339\n",
      "step :1233 Loss: 0.418206\n",
      "step :1234 Loss: 0.137068\n",
      "step :1235 Loss: 0.148976\n",
      "step :1236 Loss: 0.160165\n",
      "step :1237 Loss: 0.103789\n",
      "step :1238 Loss: 0.133757\n",
      "step :1239 Loss: 0.0931918\n",
      "step :1240 Loss: 0.189316\n",
      "step :1241 Loss: 0.0535151\n",
      "step :1242 Loss: 0.0674371\n",
      "step :1243 Loss: 0.186007\n",
      "step :1244 Loss: 0.0847798\n",
      "step :1245 Loss: 0.021386\n",
      "step :1246 Loss: 0.132692\n",
      "step :1247 Loss: 0.146585\n",
      "step :1248 Loss: 0.100946\n",
      "step :1249 Loss: 0.0556813\n",
      "step :1250 Loss: 0.0728251\n",
      "step :1251 Loss: 0.260008\n",
      "step :1252 Loss: 0.258964\n",
      "step :1253 Loss: 0.181467\n",
      "step :1254 Loss: 0.0682593\n",
      "step :1255 Loss: 0.132327\n",
      "step :1256 Loss: 0.06168\n",
      "step :1257 Loss: 0.133219\n",
      "step :1258 Loss: 0.258752\n",
      "step :1259 Loss: 0.169575\n",
      "step :1260 Loss: 0.125332\n",
      "step :1261 Loss: 0.170701\n",
      "step :1262 Loss: 0.116145\n",
      "step :1263 Loss: 0.144692\n",
      "step :1264 Loss: 0.138023\n",
      "step :1265 Loss: 0.144773\n",
      "step :1266 Loss: 0.0143525\n",
      "step :1267 Loss: 0.168593\n",
      "step :1268 Loss: 0.109598\n",
      "step :1269 Loss: 0.166533\n",
      "step :1270 Loss: 0.121746\n",
      "step :1271 Loss: 0.0292096\n",
      "step :1272 Loss: 0.0825474\n",
      "step :1273 Loss: 0.101487\n",
      "step :1274 Loss: 0.103445\n",
      "step :1275 Loss: 0.121422\n",
      "step :1276 Loss: 0.0954022\n",
      "step :1277 Loss: 0.140492\n",
      "step :1278 Loss: 0.0691657\n",
      "step :1279 Loss: 0.205749\n",
      "step :1280 Loss: 0.0551794\n",
      "step :1281 Loss: 0.0421898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :1282 Loss: 0.209129\n",
      "step :1283 Loss: 0.111241\n",
      "step :1284 Loss: 0.133297\n",
      "step :1285 Loss: 0.0945024\n",
      "step :1286 Loss: 0.0842751\n",
      "step :1287 Loss: 0.121979\n",
      "step :1288 Loss: 0.23453\n",
      "step :1289 Loss: 0.0310579\n",
      "step :1290 Loss: 0.071081\n",
      "step :1291 Loss: 0.182977\n",
      "step :1292 Loss: 0.0993285\n",
      "step :1293 Loss: 0.0819896\n",
      "step :1294 Loss: 0.176304\n",
      "step :1295 Loss: 0.153641\n",
      "step :1296 Loss: 0.145204\n",
      "step :1297 Loss: 0.153607\n",
      "step :1298 Loss: 0.160272\n",
      "step :1299 Loss: 0.16764\n",
      "step :1300 Loss: 0.0790604\n",
      "step :1301 Loss: 0.213595\n",
      "step :1302 Loss: 0.259232\n",
      "step :1303 Loss: 0.0996541\n",
      "step :1304 Loss: 0.0692215\n",
      "step :1305 Loss: 0.113765\n",
      "step :1306 Loss: 0.171057\n",
      "step :1307 Loss: 0.0490608\n",
      "step :1308 Loss: 0.14377\n",
      "step :1309 Loss: 0.0982296\n",
      "step :1310 Loss: 0.0872344\n",
      "step :1311 Loss: 0.229665\n",
      "step :1312 Loss: 0.101496\n",
      "step :1313 Loss: 0.106457\n",
      "step :1314 Loss: 0.107017\n",
      "step :1315 Loss: 0.0697514\n",
      "step :1316 Loss: 0.0393984\n",
      "step :1317 Loss: 0.172977\n",
      "step :1318 Loss: 0.217972\n",
      "step :1319 Loss: 0.194695\n",
      "step :1320 Loss: 0.229631\n",
      "step :1321 Loss: 0.0580355\n",
      "step :1322 Loss: 0.130437\n",
      "step :1323 Loss: 0.12719\n",
      "step :1324 Loss: 0.175851\n",
      "step :1325 Loss: 0.0390573\n",
      "step :1326 Loss: 0.0595248\n",
      "step :1327 Loss: 0.0741019\n",
      "step :1328 Loss: 0.0516792\n",
      "step :1329 Loss: 0.027154\n",
      "step :1330 Loss: 0.206744\n",
      "step :1331 Loss: 0.0508015\n",
      "step :1332 Loss: 0.112058\n",
      "step :1333 Loss: 0.0755071\n",
      "step :1334 Loss: 0.155747\n",
      "step :1335 Loss: 0.0528284\n",
      "step :1336 Loss: 0.0554652\n",
      "step :1337 Loss: 0.120069\n",
      "step :1338 Loss: 0.149661\n",
      "step :1339 Loss: 0.120458\n",
      "step :1340 Loss: 0.0758487\n",
      "step :1341 Loss: 0.124688\n",
      "step :1342 Loss: 0.0793697\n",
      "step :1343 Loss: 0.0510614\n",
      "step :1344 Loss: 0.134616\n",
      "step :1345 Loss: 0.224895\n",
      "step :1346 Loss: 0.179441\n",
      "step :1347 Loss: 0.0860323\n",
      "step :1348 Loss: 0.175752\n",
      "step :1349 Loss: 0.0363493\n",
      "step :1350 Loss: 0.150111\n",
      "step :1351 Loss: 0.132553\n",
      "step :1352 Loss: 0.256873\n",
      "step :1353 Loss: 0.0947231\n",
      "step :1354 Loss: 0.161127\n",
      "step :1355 Loss: 0.0945442\n",
      "step :1356 Loss: 0.0936477\n",
      "step :1357 Loss: 0.0765194\n",
      "step :1358 Loss: 0.0644015\n",
      "step :1359 Loss: 0.115279\n",
      "step :1360 Loss: 0.156438\n",
      "step :1361 Loss: 0.122837\n",
      "step :1362 Loss: 0.101001\n",
      "step :1363 Loss: 0.10173\n",
      "step :1364 Loss: 0.182408\n",
      "step :1365 Loss: 0.158463\n",
      "step :1366 Loss: 0.219646\n",
      "step :1367 Loss: 0.0852212\n",
      "step :1368 Loss: 0.0656142\n",
      "step :1369 Loss: 0.147011\n",
      "step :1370 Loss: 0.142667\n",
      "step :1371 Loss: 0.194595\n",
      "step :1372 Loss: 0.0990369\n",
      "step :1373 Loss: 0.184965\n",
      "step :1374 Loss: 0.245855\n",
      "step :1375 Loss: 0.144612\n",
      "step :1376 Loss: 0.152789\n",
      "step :1377 Loss: 0.131218\n",
      "step :1378 Loss: 0.0451404\n",
      "step :1379 Loss: 0.250057\n",
      "step :1380 Loss: 0.114634\n",
      "step :1381 Loss: 0.125381\n",
      "step :1382 Loss: 0.259501\n",
      "step :1383 Loss: 0.105628\n",
      "step :1384 Loss: 0.151653\n",
      "step :1385 Loss: 0.222268\n",
      "step :1386 Loss: 0.0864412\n",
      "step :1387 Loss: 0.119153\n",
      "step :1388 Loss: 0.100916\n",
      "step :1389 Loss: 0.192824\n",
      "step :1390 Loss: 0.151822\n",
      "step :1391 Loss: 0.0467991\n",
      "step :1392 Loss: 0.148449\n",
      "step :1393 Loss: 0.104533\n",
      "step :1394 Loss: 0.116624\n",
      "step :1395 Loss: 0.134697\n",
      "step :1396 Loss: 0.0817561\n",
      "step :1397 Loss: 0.0516565\n",
      "step :1398 Loss: 0.054038\n",
      "step :1399 Loss: 0.129415\n",
      "step :1400 Loss: 0.258353\n",
      "step :1401 Loss: 0.123035\n",
      "step :1402 Loss: 0.0296732\n",
      "step :1403 Loss: 0.111372\n",
      "step :1404 Loss: 0.0737114\n",
      "step :1405 Loss: 0.0866991\n",
      "step :1406 Loss: 0.149565\n",
      "step :1407 Loss: 0.132555\n",
      "step :1408 Loss: 0.157354\n",
      "step :1409 Loss: 0.189854\n",
      "step :1410 Loss: 0.116516\n",
      "step :1411 Loss: 0.194228\n",
      "step :1412 Loss: 0.206747\n",
      "step :1413 Loss: 0.16983\n",
      "step :1414 Loss: 0.0227378\n",
      "step :1415 Loss: 0.0908598\n",
      "step :1416 Loss: 0.0961505\n",
      "step :1417 Loss: 0.144439\n",
      "step :1418 Loss: 0.112092\n",
      "step :1419 Loss: 0.117215\n",
      "step :1420 Loss: 0.102144\n",
      "step :1421 Loss: 0.109348\n",
      "step :1422 Loss: 0.130995\n",
      "step :1423 Loss: 0.108368\n",
      "step :1424 Loss: 0.0624654\n",
      "step :1425 Loss: 0.118643\n",
      "step :1426 Loss: 0.124819\n",
      "step :1427 Loss: 0.113496\n",
      "step :1428 Loss: 0.0761202\n",
      "step :1429 Loss: 0.081865\n",
      "step :1430 Loss: 0.200261\n",
      "step :1431 Loss: 0.239032\n",
      "step :1432 Loss: 0.145634\n",
      "step :1433 Loss: 0.130829\n",
      "step :1434 Loss: 0.246341\n",
      "step :1435 Loss: 0.165132\n",
      "step :1436 Loss: 0.142877\n",
      "step :1437 Loss: 0.0492073\n",
      "step :1438 Loss: 0.209098\n",
      "step :1439 Loss: 0.101761\n",
      "step :1440 Loss: 0.129789\n",
      "step :1441 Loss: 0.12714\n",
      "step :1442 Loss: 0.215358\n",
      "step :1443 Loss: 0.0933533\n",
      "step :1444 Loss: 0.199153\n",
      "step :1445 Loss: 0.0956272\n",
      "step :1446 Loss: 0.209005\n",
      "step :1447 Loss: 0.0912536\n",
      "step :1448 Loss: 0.0869606\n",
      "step :1449 Loss: 0.166876\n",
      "step :1450 Loss: 0.101765\n",
      "step :1451 Loss: 0.0838517\n",
      "step :1452 Loss: 0.107592\n",
      "step :1453 Loss: 0.0800327\n",
      "step :1454 Loss: 0.196213\n",
      "step :1455 Loss: 0.0934506\n",
      "step :1456 Loss: 0.0645061\n",
      "step :1457 Loss: 0.110997\n",
      "step :1458 Loss: 0.100236\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn16s_final.ckpt\n",
      "step :1459 Loss: 0.149494\n",
      "step :1460 Loss: 0.076801\n",
      "step :1461 Loss: 0.160431\n",
      "step :1462 Loss: 0.120399\n",
      "step :1463 Loss: 0.253485\n",
      "step :1464 Loss: 0.190019\n",
      "step :1465 Loss: 0.0881459\n",
      "step :1466 Loss: 0.0709026\n",
      "step :1467 Loss: 0.125015\n",
      "step :1468 Loss: 0.0553365\n",
      "step :1469 Loss: 0.0210137\n",
      "step :1470 Loss: 0.112275\n",
      "step :1471 Loss: 0.0770805\n",
      "step :1472 Loss: 0.223941\n",
      "step :1473 Loss: 0.144371\n",
      "step :1474 Loss: 0.080074\n",
      "step :1475 Loss: 0.0350457\n",
      "step :1476 Loss: 0.0716376\n",
      "step :1477 Loss: 0.077524\n",
      "step :1478 Loss: 0.187732\n",
      "step :1479 Loss: 0.291421\n",
      "step :1480 Loss: 0.2081\n",
      "step :1481 Loss: 0.137273\n",
      "step :1482 Loss: 0.261843\n",
      "step :1483 Loss: 0.0835463\n",
      "step :1484 Loss: 0.348161\n",
      "step :1485 Loss: 0.0215061\n",
      "step :1486 Loss: 0.09011\n",
      "step :1487 Loss: 0.0757192\n",
      "step :1488 Loss: 0.194619\n",
      "step :1489 Loss: 0.100952\n",
      "step :1490 Loss: 0.105553\n",
      "step :1491 Loss: 0.068526\n",
      "step :1492 Loss: 0.16827\n",
      "step :1493 Loss: 0.0905318\n",
      "step :1494 Loss: 0.102184\n",
      "step :1495 Loss: 0.106557\n",
      "step :1496 Loss: 0.0778409\n",
      "step :1497 Loss: 0.0511304\n",
      "step :1498 Loss: 0.10956\n",
      "step :1499 Loss: 0.171031\n",
      "step :1500 Loss: 0.131702\n",
      "step :1501 Loss: 0.111973\n",
      "step :1502 Loss: 0.0991783\n",
      "step :1503 Loss: 0.0867886\n",
      "step :1504 Loss: 0.129714\n",
      "step :1505 Loss: 0.0800903\n",
      "step :1506 Loss: 0.189742\n",
      "step :1507 Loss: 0.111418\n",
      "step :1508 Loss: 0.0562011\n",
      "step :1509 Loss: 0.180407\n",
      "step :1510 Loss: 0.16076\n",
      "step :1511 Loss: 0.0478433\n",
      "step :1512 Loss: 0.0811218\n",
      "step :1513 Loss: 0.174684\n",
      "step :1514 Loss: 0.0296945\n",
      "step :1515 Loss: 0.0904561\n",
      "step :1516 Loss: 0.0863081\n",
      "step :1517 Loss: 0.0864324\n",
      "step :1518 Loss: 0.115389\n",
      "step :1519 Loss: 0.125964\n",
      "step :1520 Loss: 0.119404\n",
      "step :1521 Loss: 0.120802\n",
      "step :1522 Loss: 0.0882996\n",
      "step :1523 Loss: 0.167366\n",
      "step :1524 Loss: 0.131547\n",
      "step :1525 Loss: 0.17308\n",
      "step :1526 Loss: 0.18583\n",
      "step :1527 Loss: 0.159885\n",
      "step :1528 Loss: 0.145156\n",
      "step :1529 Loss: 0.284017\n",
      "step :1530 Loss: 0.0251096\n",
      "step :1531 Loss: 0.0863867\n",
      "step :1532 Loss: 0.24915\n",
      "step :1533 Loss: 0.195865\n",
      "step :1534 Loss: 0.141386\n",
      "step :1535 Loss: 0.102252\n",
      "step :1536 Loss: 0.0952782\n",
      "step :1537 Loss: 0.121769\n",
      "step :1538 Loss: 0.0867981\n",
      "step :1539 Loss: 0.0691619\n",
      "step :1540 Loss: 0.129861\n",
      "step :1541 Loss: 0.130199\n",
      "step :1542 Loss: 0.109016\n",
      "step :1543 Loss: 0.107526\n",
      "step :1544 Loss: 0.121933\n",
      "step :1545 Loss: 0.126174\n",
      "step :1546 Loss: 0.172792\n",
      "step :1547 Loss: 0.100673\n",
      "step :1548 Loss: 0.0375853\n",
      "step :1549 Loss: 0.0996801\n",
      "step :1550 Loss: 0.143555\n",
      "step :1551 Loss: 0.177553\n",
      "step :1552 Loss: 0.163958\n",
      "step :1553 Loss: 0.136957\n",
      "step :1554 Loss: 0.127279\n",
      "step :1555 Loss: 0.162027\n",
      "step :1556 Loss: 0.24525\n",
      "step :1557 Loss: 0.309854\n",
      "step :1558 Loss: 0.103573\n",
      "step :1559 Loss: 0.135279\n",
      "step :1560 Loss: 0.064968\n",
      "step :1561 Loss: 0.187487\n",
      "step :1562 Loss: 0.123154\n",
      "step :1563 Loss: 0.194446\n",
      "step :1564 Loss: 0.109411\n",
      "step :1565 Loss: 0.152947\n",
      "step :1566 Loss: 0.167368\n",
      "step :1567 Loss: 0.0930694\n",
      "step :1568 Loss: 0.0866111\n",
      "step :1569 Loss: 0.132786\n",
      "step :1570 Loss: 0.138464\n",
      "step :1571 Loss: 0.0846745\n",
      "step :1572 Loss: 0.0926467\n",
      "step :1573 Loss: 0.14395\n",
      "step :1574 Loss: 0.18796\n",
      "step :1575 Loss: 0.0757946\n",
      "step :1576 Loss: 0.122408\n",
      "step :1577 Loss: 0.0694354\n",
      "step :1578 Loss: 0.112351\n",
      "step :1579 Loss: 0.066221\n",
      "step :1580 Loss: 0.190208\n",
      "step :1581 Loss: 0.151585\n",
      "step :1582 Loss: 0.14913\n",
      "step :1583 Loss: 0.121214\n",
      "step :1584 Loss: 0.117556\n",
      "step :1585 Loss: 0.0495375\n",
      "step :1586 Loss: 0.109268\n",
      "step :1587 Loss: 0.0704697\n",
      "step :1588 Loss: 0.166354\n",
      "step :1589 Loss: 0.0315197\n",
      "step :1590 Loss: 0.12642\n",
      "step :1591 Loss: 0.0801826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :1592 Loss: 0.105079\n",
      "step :1593 Loss: 0.114089\n",
      "step :1594 Loss: 0.123845\n",
      "step :1595 Loss: 0.133053\n",
      "step :1596 Loss: 0.128761\n",
      "step :1597 Loss: 0.155865\n",
      "step :1598 Loss: 0.153922\n",
      "step :1599 Loss: 0.130228\n",
      "step :1600 Loss: 0.239088\n",
      "step :1601 Loss: 0.0923748\n",
      "step :1602 Loss: 0.118439\n",
      "step :1603 Loss: 0.157889\n",
      "step :1604 Loss: 0.0946632\n",
      "step :1605 Loss: 0.305386\n",
      "step :1606 Loss: 0.0961758\n",
      "step :1607 Loss: 0.0796675\n",
      "step :1608 Loss: 0.120356\n",
      "step :1609 Loss: 0.288362\n",
      "step :1610 Loss: 0.115409\n",
      "step :1611 Loss: 0.176875\n",
      "step :1612 Loss: 0.262545\n",
      "step :1613 Loss: 0.0865687\n",
      "step :1614 Loss: 0.0903167\n",
      "step :1615 Loss: 0.0865153\n",
      "step :1616 Loss: 0.192621\n",
      "step :1617 Loss: 0.262701\n",
      "step :1618 Loss: 0.0937341\n",
      "step :1619 Loss: 0.234691\n",
      "step :1620 Loss: 0.150529\n",
      "step :1621 Loss: 0.200284\n",
      "step :1622 Loss: 0.142848\n",
      "step :1623 Loss: 0.0745257\n",
      "step :1624 Loss: 0.142456\n",
      "step :1625 Loss: 0.166273\n",
      "step :1626 Loss: 0.111366\n",
      "step :1627 Loss: 0.170418\n",
      "step :1628 Loss: 0.241091\n",
      "step :1629 Loss: 0.105554\n",
      "step :1630 Loss: 0.0988895\n",
      "step :1631 Loss: 0.0374102\n",
      "step :1632 Loss: 0.0834276\n",
      "step :1633 Loss: 0.185195\n",
      "step :1634 Loss: 0.164465\n",
      "step :1635 Loss: 0.191254\n",
      "step :1636 Loss: 0.182447\n",
      "step :1637 Loss: 0.231153\n",
      "step :1638 Loss: 0.105219\n",
      "step :1639 Loss: 0.182651\n",
      "step :1640 Loss: 0.0855173\n",
      "step :1641 Loss: 0.0950136\n",
      "step :1642 Loss: 0.0294723\n",
      "step :1643 Loss: 0.132294\n",
      "step :1644 Loss: 0.185041\n",
      "step :1645 Loss: 0.0678004\n",
      "step :1646 Loss: 0.168392\n",
      "step :1647 Loss: 0.0873867\n",
      "step :1648 Loss: 0.0966958\n",
      "step :1649 Loss: 0.207914\n",
      "step :1650 Loss: 0.0888647\n",
      "step :1651 Loss: 0.294383\n",
      "step :1652 Loss: 0.175014\n",
      "step :1653 Loss: 0.0999353\n",
      "step :1654 Loss: 0.192379\n",
      "step :1655 Loss: 0.202194\n",
      "step :1656 Loss: 0.0550041\n",
      "step :1657 Loss: 0.0983198\n",
      "step :1658 Loss: 0.136554\n",
      "step :1659 Loss: 0.152496\n",
      "step :1660 Loss: 0.135837\n",
      "step :1661 Loss: 0.237243\n",
      "step :1662 Loss: 0.12279\n",
      "step :1663 Loss: 0.0798728\n",
      "step :1664 Loss: 0.0662391\n",
      "step :1665 Loss: 0.0653568\n",
      "step :1666 Loss: 0.115924\n",
      "step :1667 Loss: 0.0949376\n",
      "step :1668 Loss: 0.046045\n",
      "step :1669 Loss: 0.173208\n",
      "step :1670 Loss: 0.212211\n",
      "step :1671 Loss: 0.0882202\n",
      "step :1672 Loss: 0.156132\n",
      "step :1673 Loss: 0.149414\n",
      "step :1674 Loss: 0.116314\n",
      "step :1675 Loss: 0.205346\n",
      "step :1676 Loss: 0.0747264\n",
      "step :1677 Loss: 0.0576994\n",
      "step :1678 Loss: 0.136254\n",
      "step :1679 Loss: 0.0255708\n",
      "step :1680 Loss: 0.133747\n",
      "step :1681 Loss: 0.0841756\n",
      "step :1682 Loss: 0.141863\n",
      "step :1683 Loss: 0.210412\n",
      "step :1684 Loss: 0.129005\n",
      "step :1685 Loss: 0.0452175\n",
      "step :1686 Loss: 0.240982\n",
      "step :1687 Loss: 0.0372492\n",
      "step :1688 Loss: 0.122084\n",
      "step :1689 Loss: 0.0972842\n",
      "step :1690 Loss: 0.0160889\n",
      "step :1691 Loss: 0.134938\n",
      "step :1692 Loss: 0.201245\n",
      "step :1693 Loss: 0.168218\n",
      "step :1694 Loss: 0.16256\n",
      "step :1695 Loss: 0.0773655\n",
      "step :1696 Loss: 0.196572\n",
      "step :1697 Loss: 0.188356\n",
      "step :1698 Loss: 0.137328\n",
      "step :1699 Loss: 0.131172\n",
      "step :1700 Loss: 0.254752\n",
      "step :1701 Loss: 0.115364\n",
      "step :1702 Loss: 0.142738\n",
      "step :1703 Loss: 0.0848534\n",
      "step :1704 Loss: 0.245446\n",
      "step :1705 Loss: 0.131789\n",
      "step :1706 Loss: 0.0810288\n",
      "step :1707 Loss: 0.129174\n",
      "step :1708 Loss: 0.133953\n",
      "step :1709 Loss: 0.125141\n",
      "step :1710 Loss: 0.190913\n",
      "step :1711 Loss: 0.0680351\n",
      "step :1712 Loss: 0.107927\n",
      "step :1713 Loss: 0.0463055\n",
      "step :1714 Loss: 0.215827\n",
      "step :1715 Loss: 0.114583\n",
      "step :1716 Loss: 0.134882\n",
      "step :1717 Loss: 0.180821\n",
      "step :1718 Loss: 0.150067\n",
      "step :1719 Loss: 0.124975\n",
      "step :1720 Loss: 0.0607633\n",
      "step :1721 Loss: 0.0701063\n",
      "step :1722 Loss: 0.114962\n",
      "step :1723 Loss: 0.0858614\n",
      "step :1724 Loss: 0.0655603\n",
      "step :1725 Loss: 0.0365916\n",
      "step :1726 Loss: 0.285129\n",
      "step :1727 Loss: 0.100533\n",
      "step :1728 Loss: 0.123141\n",
      "step :1729 Loss: 0.0826764\n",
      "step :1730 Loss: 0.158379\n",
      "step :1731 Loss: 0.242685\n",
      "step :1732 Loss: 0.0889994\n",
      "step :1733 Loss: 0.124159\n",
      "step :1734 Loss: 0.092792\n",
      "step :1735 Loss: 0.112185\n",
      "step :1736 Loss: 0.263673\n",
      "step :1737 Loss: 0.0359601\n",
      "step :1738 Loss: 0.103031\n",
      "step :1739 Loss: 0.117562\n",
      "step :1740 Loss: 0.0954599\n",
      "step :1741 Loss: 0.207481\n",
      "step :1742 Loss: 0.0852362\n",
      "step :1743 Loss: 0.136522\n",
      "step :1744 Loss: 0.205725\n",
      "step :1745 Loss: 0.164639\n",
      "step :1746 Loss: 0.169091\n",
      "step :1747 Loss: 0.132161\n",
      "step :1748 Loss: 0.0691209\n",
      "step :1749 Loss: 0.25005\n",
      "step :1750 Loss: 0.0862455\n",
      "step :1751 Loss: 0.195475\n",
      "step :1752 Loss: 0.118862\n",
      "step :1753 Loss: 0.0932562\n",
      "step :1754 Loss: 0.095966\n",
      "step :1755 Loss: 0.0606127\n",
      "step :1756 Loss: 0.190228\n",
      "step :1757 Loss: 0.0600259\n",
      "step :1758 Loss: 0.158357\n",
      "step :1759 Loss: 0.1956\n",
      "step :1760 Loss: 0.0680103\n",
      "step :1761 Loss: 0.126236\n",
      "step :1762 Loss: 0.187628\n",
      "step :1763 Loss: 0.157001\n",
      "step :1764 Loss: 0.114151\n",
      "step :1765 Loss: 0.206184\n",
      "step :1766 Loss: 0.218659\n",
      "step :1767 Loss: 0.139278\n",
      "step :1768 Loss: 0.0972377\n",
      "step :1769 Loss: 0.118601\n",
      "step :1770 Loss: 0.0842793\n",
      "step :1771 Loss: 0.0614633\n",
      "step :1772 Loss: 0.165606\n",
      "step :1773 Loss: 0.107872\n",
      "step :1774 Loss: 0.100862\n",
      "step :1775 Loss: 0.176536\n",
      "step :1776 Loss: 0.128807\n",
      "step :1777 Loss: 0.157909\n",
      "step :1778 Loss: 0.154157\n",
      "step :1779 Loss: 0.0833893\n",
      "step :1780 Loss: 0.0926859\n",
      "step :1781 Loss: 0.224153\n",
      "step :1782 Loss: 0.157966\n",
      "step :1783 Loss: 0.0845664\n",
      "step :1784 Loss: 0.115422\n",
      "step :1785 Loss: 0.184846\n",
      "step :1786 Loss: 0.133378\n",
      "step :1787 Loss: 0.0865366\n",
      "step :1788 Loss: 0.12388\n",
      "step :1789 Loss: 0.24331\n",
      "step :1790 Loss: 0.0320353\n",
      "step :1791 Loss: 0.0884362\n",
      "step :1792 Loss: 0.0928674\n",
      "step :1793 Loss: 0.137664\n",
      "step :1794 Loss: 0.122182\n",
      "step :1795 Loss: 0.0546019\n",
      "step :1796 Loss: 0.0712273\n",
      "step :1797 Loss: 0.166172\n",
      "step :1798 Loss: 0.163116\n",
      "step :1799 Loss: 0.0695597\n",
      "step :1800 Loss: 0.176681\n",
      "step :1801 Loss: 0.202254\n",
      "step :1802 Loss: 0.113981\n",
      "step :1803 Loss: 0.0991525\n",
      "step :1804 Loss: 0.328505\n",
      "step :1805 Loss: 0.103555\n",
      "step :1806 Loss: 0.0908325\n",
      "step :1807 Loss: 0.069065\n",
      "step :1808 Loss: 0.247436\n",
      "step :1809 Loss: 0.217617\n",
      "step :1810 Loss: 0.090331\n",
      "step :1811 Loss: 0.0395675\n",
      "step :1812 Loss: 0.237531\n",
      "step :1813 Loss: 0.0731523\n",
      "step :1814 Loss: 0.151688\n",
      "step :1815 Loss: 0.0900429\n",
      "step :1816 Loss: 0.0625497\n",
      "step :1817 Loss: 0.106392\n",
      "step :1818 Loss: 0.211866\n",
      "step :1819 Loss: 0.144226\n",
      "step :1820 Loss: 0.126774\n",
      "step :1821 Loss: 0.293124\n",
      "step :1822 Loss: 0.162056\n",
      "step :1823 Loss: 0.089186\n",
      "step :1824 Loss: 0.091238\n",
      "step :1825 Loss: 0.232422\n",
      "step :1826 Loss: 0.0846717\n",
      "step :1827 Loss: 0.120301\n",
      "step :1828 Loss: 0.169391\n",
      "step :1829 Loss: 0.0522984\n",
      "step :1830 Loss: 0.110019\n",
      "step :1831 Loss: 0.135053\n",
      "step :1832 Loss: 0.102168\n",
      "step :1833 Loss: 0.226742\n",
      "step :1834 Loss: 0.145383\n",
      "step :1835 Loss: 0.0956025\n",
      "step :1836 Loss: 0.0966737\n",
      "step :1837 Loss: 0.12986\n",
      "step :1838 Loss: 0.0519469\n",
      "step :1839 Loss: 0.0598135\n",
      "step :1840 Loss: 0.151925\n",
      "step :1841 Loss: 0.0609742\n",
      "step :1842 Loss: 0.108262\n",
      "step :1843 Loss: 0.120745\n",
      "step :1844 Loss: 0.206061\n",
      "step :1845 Loss: 0.104437\n",
      "step :1846 Loss: 0.0241256\n",
      "step :1847 Loss: 0.158573\n",
      "step :1848 Loss: 0.145076\n",
      "step :1849 Loss: 0.228304\n",
      "step :1850 Loss: 0.144957\n",
      "step :1851 Loss: 0.0668587\n",
      "step :1852 Loss: 0.115763\n",
      "step :1853 Loss: 0.173716\n",
      "step :1854 Loss: 0.110019\n",
      "step :1855 Loss: 0.146532\n",
      "step :1856 Loss: 0.12986\n",
      "step :1857 Loss: 0.0917894\n",
      "step :1858 Loss: 0.0269507\n",
      "step :1859 Loss: 0.121909\n",
      "step :1860 Loss: 0.148226\n",
      "step :1861 Loss: 0.144288\n",
      "step :1862 Loss: 0.119081\n",
      "step :1863 Loss: 0.159218\n",
      "step :1864 Loss: 0.0984852\n",
      "step :1865 Loss: 0.13883\n",
      "step :1866 Loss: 0.0337207\n",
      "step :1867 Loss: 0.055322\n",
      "step :1868 Loss: 0.10437\n",
      "step :1869 Loss: 0.248512\n",
      "step :1870 Loss: 0.21663\n",
      "step :1871 Loss: 0.305146\n",
      "step :1872 Loss: 0.0694482\n",
      "step :1873 Loss: 0.0270548\n",
      "step :1874 Loss: 0.128571\n",
      "step :1875 Loss: 0.0662317\n",
      "step :1876 Loss: 0.0822003\n",
      "step :1877 Loss: 0.0628052\n",
      "step :1878 Loss: 0.046608\n",
      "step :1879 Loss: 0.203315\n",
      "step :1880 Loss: 0.208113\n",
      "step :1881 Loss: 0.211305\n",
      "step :1882 Loss: 0.138158\n",
      "step :1883 Loss: 0.140073\n",
      "step :1884 Loss: 0.229472\n",
      "step :1885 Loss: 0.22242\n",
      "step :1886 Loss: 0.0736694\n",
      "step :1887 Loss: 0.109281\n",
      "step :1888 Loss: 0.114036\n",
      "step :1889 Loss: 0.152073\n",
      "step :1890 Loss: 0.199072\n",
      "step :1891 Loss: 0.0873496\n",
      "step :1892 Loss: 0.16097\n",
      "step :1893 Loss: 0.101732\n",
      "step :1894 Loss: 0.0508942\n",
      "step :1895 Loss: 0.143948\n",
      "step :1896 Loss: 0.082259\n",
      "step :1897 Loss: 0.0763746\n",
      "step :1898 Loss: 0.187402\n",
      "step :1899 Loss: 0.133593\n",
      "step :1900 Loss: 0.129287\n",
      "step :1901 Loss: 0.133725\n",
      "step :1902 Loss: 0.218717\n",
      "step :1903 Loss: 0.129123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :1904 Loss: 0.287075\n",
      "step :1905 Loss: 0.100674\n",
      "step :1906 Loss: 0.0549912\n",
      "step :1907 Loss: 0.0644745\n",
      "step :1908 Loss: 0.104136\n",
      "step :1909 Loss: 0.155575\n",
      "step :1910 Loss: 0.218437\n",
      "step :1911 Loss: 0.0810082\n",
      "step :1912 Loss: 0.138341\n",
      "step :1913 Loss: 0.179157\n",
      "step :1914 Loss: 0.1133\n",
      "step :1915 Loss: 0.148515\n",
      "step :1916 Loss: 0.0653238\n",
      "step :1917 Loss: 0.0951457\n",
      "step :1918 Loss: 0.0622249\n",
      "step :1919 Loss: 0.0883135\n",
      "step :1920 Loss: 0.170486\n",
      "step :1921 Loss: 0.0410369\n",
      "step :1922 Loss: 0.167852\n",
      "step :1923 Loss: 0.134029\n",
      "step :1924 Loss: 0.129541\n",
      "step :1925 Loss: 0.153573\n",
      "step :1926 Loss: 0.27637\n",
      "step :1927 Loss: 0.250395\n",
      "step :1928 Loss: 0.151859\n",
      "step :1929 Loss: 0.0466458\n",
      "step :1930 Loss: 0.0763548\n",
      "step :1931 Loss: 0.156216\n",
      "step :1932 Loss: 0.139754\n",
      "step :1933 Loss: 0.0901253\n",
      "step :1934 Loss: 0.155346\n",
      "step :1935 Loss: 0.129846\n",
      "step :1936 Loss: 0.146979\n",
      "step :1937 Loss: 0.209537\n",
      "step :1938 Loss: 0.223807\n",
      "step :1939 Loss: 0.168266\n",
      "step :1940 Loss: 0.120517\n",
      "step :1941 Loss: 0.131836\n",
      "step :1942 Loss: 0.0684905\n",
      "step :1943 Loss: 0.206642\n",
      "step :1944 Loss: 0.134965\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn16s_final.ckpt\n",
      "step :1945 Loss: 0.0966892\n",
      "step :1946 Loss: 0.130409\n",
      "step :1947 Loss: 0.0405425\n",
      "step :1948 Loss: 0.12976\n",
      "step :1949 Loss: 0.0898394\n",
      "step :1950 Loss: 0.112784\n",
      "step :1951 Loss: 0.17204\n",
      "step :1952 Loss: 0.113807\n",
      "step :1953 Loss: 0.064372\n",
      "step :1954 Loss: 0.171388\n",
      "step :1955 Loss: 0.161411\n",
      "step :1956 Loss: 0.146482\n",
      "step :1957 Loss: 0.241373\n",
      "step :1958 Loss: 0.102761\n",
      "step :1959 Loss: 0.0234047\n",
      "step :1960 Loss: 0.0823341\n",
      "step :1961 Loss: 0.132513\n",
      "step :1962 Loss: 0.0914189\n",
      "step :1963 Loss: 0.103486\n",
      "step :1964 Loss: 0.0862274\n",
      "step :1965 Loss: 0.0654304\n",
      "step :1966 Loss: 0.0807304\n",
      "step :1967 Loss: 0.0749476\n",
      "step :1968 Loss: 0.0389872\n",
      "step :1969 Loss: 0.132809\n",
      "step :1970 Loss: 0.0732596\n",
      "step :1971 Loss: 0.200871\n",
      "step :1972 Loss: 0.0835352\n",
      "step :1973 Loss: 0.188859\n",
      "step :1974 Loss: 0.103132\n",
      "step :1975 Loss: 0.117623\n",
      "step :1976 Loss: 0.0943354\n",
      "step :1977 Loss: 0.102233\n",
      "step :1978 Loss: 0.0980212\n",
      "step :1979 Loss: 0.213166\n",
      "step :1980 Loss: 0.0926641\n",
      "step :1981 Loss: 0.216248\n",
      "step :1982 Loss: 0.0495599\n",
      "step :1983 Loss: 0.125531\n",
      "step :1984 Loss: 0.235917\n",
      "step :1985 Loss: 0.071847\n",
      "step :1986 Loss: 0.141375\n",
      "step :1987 Loss: 0.173814\n",
      "step :1988 Loss: 0.169653\n",
      "step :1989 Loss: 0.146401\n",
      "step :1990 Loss: 0.164982\n",
      "step :1991 Loss: 0.108821\n",
      "step :1992 Loss: 0.0762192\n",
      "step :1993 Loss: 0.0873939\n",
      "step :1994 Loss: 0.046918\n",
      "step :1995 Loss: 0.161877\n",
      "step :1996 Loss: 0.0884333\n",
      "step :1997 Loss: 0.183914\n",
      "step :1998 Loss: 0.0937105\n",
      "step :1999 Loss: 0.180286\n",
      "step :2000 Loss: 0.107993\n",
      "step :2001 Loss: 0.114241\n",
      "step :2002 Loss: 0.038502\n",
      "step :2003 Loss: 0.0990081\n",
      "step :2004 Loss: 0.146515\n",
      "step :2005 Loss: 0.109014\n",
      "step :2006 Loss: 0.128879\n",
      "step :2007 Loss: 0.154633\n",
      "step :2008 Loss: 0.0810448\n",
      "step :2009 Loss: 0.12302\n",
      "step :2010 Loss: 0.130206\n",
      "step :2011 Loss: 0.210037\n",
      "step :2012 Loss: 0.128331\n",
      "step :2013 Loss: 0.132108\n",
      "step :2014 Loss: 0.162953\n",
      "step :2015 Loss: 0.220246\n",
      "step :2016 Loss: 0.133885\n",
      "step :2017 Loss: 0.055147\n",
      "step :2018 Loss: 0.0745282\n",
      "step :2019 Loss: 0.120387\n",
      "step :2020 Loss: 0.0949188\n",
      "step :2021 Loss: 0.188249\n",
      "step :2022 Loss: 0.0732543\n",
      "step :2023 Loss: 0.172169\n",
      "step :2024 Loss: 0.0903615\n",
      "step :2025 Loss: 0.173433\n",
      "step :2026 Loss: 0.169226\n",
      "step :2027 Loss: 0.125613\n",
      "step :2028 Loss: 0.186788\n",
      "step :2029 Loss: 0.120671\n",
      "step :2030 Loss: 0.172101\n",
      "step :2031 Loss: 0.27123\n",
      "step :2032 Loss: 0.113846\n",
      "step :2033 Loss: 0.0905053\n",
      "step :2034 Loss: 0.179922\n",
      "step :2035 Loss: 0.143145\n",
      "step :2036 Loss: 0.113542\n",
      "step :2037 Loss: 0.101621\n",
      "step :2038 Loss: 0.122543\n",
      "step :2039 Loss: 0.155116\n",
      "step :2040 Loss: 0.196605\n",
      "step :2041 Loss: 0.061469\n",
      "step :2042 Loss: 0.105376\n",
      "step :2043 Loss: 0.0832966\n",
      "step :2044 Loss: 0.160207\n",
      "step :2045 Loss: 0.168568\n",
      "step :2046 Loss: 0.11079\n",
      "step :2047 Loss: 0.10025\n",
      "step :2048 Loss: 0.122438\n",
      "step :2049 Loss: 0.12534\n",
      "step :2050 Loss: 0.141668\n",
      "step :2051 Loss: 0.176282\n",
      "step :2052 Loss: 0.141077\n",
      "step :2053 Loss: 0.05533\n",
      "step :2054 Loss: 0.173813\n",
      "step :2055 Loss: 0.0756319\n",
      "step :2056 Loss: 0.0999267\n",
      "step :2057 Loss: 0.141964\n",
      "step :2058 Loss: 0.0524026\n",
      "step :2059 Loss: 0.136344\n",
      "step :2060 Loss: 0.141551\n",
      "step :2061 Loss: 0.160327\n",
      "step :2062 Loss: 0.0954382\n",
      "step :2063 Loss: 0.174746\n",
      "step :2064 Loss: 0.13742\n",
      "step :2065 Loss: 0.152984\n",
      "step :2066 Loss: 0.187893\n",
      "step :2067 Loss: 0.205654\n",
      "step :2068 Loss: 0.0778687\n",
      "step :2069 Loss: 0.243767\n",
      "step :2070 Loss: 0.186095\n",
      "step :2071 Loss: 0.0624391\n",
      "step :2072 Loss: 0.106358\n",
      "step :2073 Loss: 0.190253\n",
      "step :2074 Loss: 0.083534\n",
      "step :2075 Loss: 0.1872\n",
      "step :2076 Loss: 0.0759652\n",
      "step :2077 Loss: 0.21379\n",
      "step :2078 Loss: 0.0782657\n",
      "step :2079 Loss: 0.0829638\n",
      "step :2080 Loss: 0.146581\n",
      "step :2081 Loss: 0.0778728\n",
      "step :2082 Loss: 0.132173\n",
      "step :2083 Loss: 0.151855\n",
      "step :2084 Loss: 0.128575\n",
      "step :2085 Loss: 0.0747866\n",
      "step :2086 Loss: 0.0809366\n",
      "step :2087 Loss: 0.163955\n",
      "step :2088 Loss: 0.0953122\n",
      "step :2089 Loss: 0.225675\n",
      "step :2090 Loss: 0.0844543\n",
      "step :2091 Loss: 0.100978\n",
      "step :2092 Loss: 0.0941735\n",
      "step :2093 Loss: 0.094797\n",
      "step :2094 Loss: 0.122818\n",
      "step :2095 Loss: 0.139342\n",
      "step :2096 Loss: 0.102744\n",
      "step :2097 Loss: 0.265926\n",
      "step :2098 Loss: 0.0738138\n",
      "step :2099 Loss: 0.0944676\n",
      "step :2100 Loss: 0.0237421\n",
      "step :2101 Loss: 0.241135\n",
      "step :2102 Loss: 0.115308\n",
      "step :2103 Loss: 0.127204\n",
      "step :2104 Loss: 0.116809\n",
      "step :2105 Loss: 0.0902154\n",
      "step :2106 Loss: 0.0239464\n",
      "step :2107 Loss: 0.0801283\n",
      "step :2108 Loss: 0.122506\n",
      "step :2109 Loss: 0.0682033\n",
      "step :2110 Loss: 0.155233\n",
      "step :2111 Loss: 0.0649246\n",
      "step :2112 Loss: 0.136178\n",
      "step :2113 Loss: 0.182471\n",
      "step :2114 Loss: 0.0732611\n",
      "step :2115 Loss: 0.103484\n",
      "step :2116 Loss: 0.062198\n",
      "step :2117 Loss: 0.110933\n",
      "step :2118 Loss: 0.0850002\n",
      "step :2119 Loss: 0.161768\n",
      "step :2120 Loss: 0.16616\n",
      "step :2121 Loss: 0.1225\n",
      "step :2122 Loss: 0.208904\n",
      "step :2123 Loss: 0.120521\n",
      "step :2124 Loss: 0.106961\n",
      "step :2125 Loss: 0.187145\n",
      "step :2126 Loss: 0.159241\n",
      "step :2127 Loss: 0.0455797\n",
      "step :2128 Loss: 0.0832698\n",
      "step :2129 Loss: 0.152535\n",
      "step :2130 Loss: 0.204559\n",
      "step :2131 Loss: 0.15219\n",
      "step :2132 Loss: 0.159883\n",
      "step :2133 Loss: 0.120013\n",
      "step :2134 Loss: 0.108766\n",
      "step :2135 Loss: 0.0900671\n",
      "step :2136 Loss: 0.186525\n",
      "step :2137 Loss: 0.0845681\n",
      "step :2138 Loss: 0.0916062\n",
      "step :2139 Loss: 0.0866987\n",
      "step :2140 Loss: 0.243114\n",
      "step :2141 Loss: 0.16139\n",
      "step :2142 Loss: 0.165664\n",
      "step :2143 Loss: 0.114575\n",
      "step :2144 Loss: 0.346402\n",
      "step :2145 Loss: 0.189139\n",
      "step :2146 Loss: 0.170411\n",
      "step :2147 Loss: 0.142455\n",
      "step :2148 Loss: 0.11516\n",
      "step :2149 Loss: 0.0347492\n",
      "step :2150 Loss: 0.136238\n",
      "step :2151 Loss: 0.0788846\n",
      "step :2152 Loss: 0.109474\n",
      "step :2153 Loss: 0.12547\n",
      "step :2154 Loss: 0.119286\n",
      "step :2155 Loss: 0.12095\n",
      "step :2156 Loss: 0.134893\n",
      "step :2157 Loss: 0.211943\n",
      "step :2158 Loss: 0.0747323\n",
      "step :2159 Loss: 0.212799\n",
      "step :2160 Loss: 0.140067\n",
      "step :2161 Loss: 0.15563\n",
      "step :2162 Loss: 0.159238\n",
      "step :2163 Loss: 0.136788\n",
      "step :2164 Loss: 0.0750854\n",
      "step :2165 Loss: 0.0667239\n",
      "step :2166 Loss: 0.0997739\n",
      "step :2167 Loss: 0.171468\n",
      "step :2168 Loss: 0.123253\n",
      "step :2169 Loss: 0.0964295\n",
      "step :2170 Loss: 0.204138\n",
      "step :2171 Loss: 0.0496276\n",
      "step :2172 Loss: 0.121101\n",
      "step :2173 Loss: 0.0758959\n",
      "step :2174 Loss: 0.102521\n",
      "step :2175 Loss: 0.149918\n",
      "step :2176 Loss: 0.0788069\n",
      "step :2177 Loss: 0.12242\n",
      "step :2178 Loss: 0.161865\n",
      "step :2179 Loss: 0.104106\n",
      "step :2180 Loss: 0.105447\n",
      "step :2181 Loss: 0.089409\n",
      "step :2182 Loss: 0.134899\n",
      "step :2183 Loss: 0.146847\n",
      "step :2184 Loss: 0.224062\n",
      "step :2185 Loss: 0.111546\n",
      "step :2186 Loss: 0.0850222\n",
      "step :2187 Loss: 0.135776\n",
      "step :2188 Loss: 0.123534\n",
      "step :2189 Loss: 0.191249\n",
      "step :2190 Loss: 0.0239759\n",
      "step :2191 Loss: 0.23596\n",
      "step :2192 Loss: 0.144367\n",
      "step :2193 Loss: 0.0513292\n",
      "step :2194 Loss: 0.0725355\n",
      "step :2195 Loss: 0.269772\n",
      "step :2196 Loss: 0.0685727\n",
      "step :2197 Loss: 0.32547\n",
      "step :2198 Loss: 0.189112\n",
      "step :2199 Loss: 0.18461\n",
      "step :2200 Loss: 0.117591\n",
      "step :2201 Loss: 0.110825\n",
      "step :2202 Loss: 0.0992008\n",
      "step :2203 Loss: 0.24007\n",
      "step :2204 Loss: 0.154701\n",
      "step :2205 Loss: 0.132818\n",
      "step :2206 Loss: 0.105311\n",
      "step :2207 Loss: 0.0901087\n",
      "step :2208 Loss: 0.152861\n",
      "step :2209 Loss: 0.101221\n",
      "step :2210 Loss: 0.188157\n",
      "step :2211 Loss: 0.215177\n",
      "step :2212 Loss: 0.194969\n",
      "step :2213 Loss: 0.0949782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :2214 Loss: 0.0307201\n",
      "step :2215 Loss: 0.0942315\n",
      "step :2216 Loss: 0.17116\n",
      "step :2217 Loss: 0.171665\n",
      "step :2218 Loss: 0.118438\n",
      "step :2219 Loss: 0.160566\n",
      "step :2220 Loss: 0.0210082\n",
      "step :2221 Loss: 0.110844\n",
      "step :2222 Loss: 0.220304\n",
      "step :2223 Loss: 0.212174\n",
      "step :2224 Loss: 0.144891\n",
      "step :2225 Loss: 0.0733954\n",
      "step :2226 Loss: 0.200684\n",
      "step :2227 Loss: 0.0512822\n",
      "step :2228 Loss: 0.0724266\n",
      "step :2229 Loss: 0.100532\n",
      "step :2230 Loss: 0.0762746\n",
      "step :2231 Loss: 0.278377\n",
      "step :2232 Loss: 0.188678\n",
      "step :2233 Loss: 0.0976646\n",
      "step :2234 Loss: 0.192063\n",
      "step :2235 Loss: 0.151936\n",
      "step :2236 Loss: 0.159647\n",
      "step :2237 Loss: 0.0964599\n",
      "step :2238 Loss: 0.124928\n",
      "step :2239 Loss: 0.0783706\n",
      "step :2240 Loss: 0.130792\n",
      "step :2241 Loss: 0.10158\n",
      "step :2242 Loss: 0.099934\n",
      "step :2243 Loss: 0.324675\n",
      "step :2244 Loss: 0.193374\n",
      "step :2245 Loss: 0.122075\n",
      "step :2246 Loss: 0.311628\n",
      "step :2247 Loss: 0.303652\n",
      "step :2248 Loss: 0.116647\n",
      "step :2249 Loss: 0.130761\n",
      "step :2250 Loss: 0.0860373\n",
      "step :2251 Loss: 0.186083\n",
      "step :2252 Loss: 0.125969\n",
      "step :2253 Loss: 0.222101\n",
      "step :2254 Loss: 0.203943\n",
      "step :2255 Loss: 0.0928992\n",
      "step :2256 Loss: 0.0984563\n",
      "step :2257 Loss: 0.0980245\n",
      "step :2258 Loss: 0.179502\n",
      "step :2259 Loss: 0.111031\n",
      "step :2260 Loss: 0.0372814\n",
      "step :2261 Loss: 0.135433\n",
      "step :2262 Loss: 0.099564\n",
      "step :2263 Loss: 0.20628\n",
      "step :2264 Loss: 0.0892787\n",
      "step :2265 Loss: 0.0830926\n",
      "step :2266 Loss: 0.0880463\n",
      "step :2267 Loss: 0.130165\n",
      "step :2268 Loss: 0.110689\n",
      "step :2269 Loss: 0.0726996\n",
      "step :2270 Loss: 0.053656\n",
      "step :2271 Loss: 0.0367942\n",
      "step :2272 Loss: 0.097178\n",
      "step :2273 Loss: 0.112403\n",
      "step :2274 Loss: 0.115253\n",
      "step :2275 Loss: 0.180875\n",
      "step :2276 Loss: 0.15598\n",
      "step :2277 Loss: 0.153748\n",
      "step :2278 Loss: 0.116921\n",
      "step :2279 Loss: 0.0772519\n",
      "step :2280 Loss: 0.191954\n",
      "step :2281 Loss: 0.14399\n",
      "step :2282 Loss: 0.148116\n",
      "step :2283 Loss: 0.197496\n",
      "step :2284 Loss: 0.1382\n",
      "step :2285 Loss: 0.0789652\n",
      "step :2286 Loss: 0.0817473\n",
      "step :2287 Loss: 0.113393\n",
      "step :2288 Loss: 0.171243\n",
      "step :2289 Loss: 0.0931634\n",
      "step :2290 Loss: 0.110635\n",
      "step :2291 Loss: 0.108979\n",
      "step :2292 Loss: 0.0563402\n",
      "step :2293 Loss: 0.112691\n",
      "step :2294 Loss: 0.165995\n",
      "step :2295 Loss: 0.168167\n",
      "step :2296 Loss: 0.0712011\n",
      "step :2297 Loss: 0.190968\n",
      "step :2298 Loss: 0.0609904\n",
      "step :2299 Loss: 0.143904\n",
      "step :2300 Loss: 0.157211\n",
      "step :2301 Loss: 0.19576\n",
      "step :2302 Loss: 0.178169\n",
      "step :2303 Loss: 0.0482365\n",
      "step :2304 Loss: 0.11767\n",
      "step :2305 Loss: 0.117766\n",
      "step :2306 Loss: 0.0507293\n",
      "step :2307 Loss: 0.0781056\n",
      "step :2308 Loss: 0.142594\n",
      "step :2309 Loss: 0.141642\n",
      "step :2310 Loss: 0.153273\n",
      "step :2311 Loss: 0.0787599\n",
      "step :2312 Loss: 0.0664952\n",
      "step :2313 Loss: 0.147061\n",
      "step :2314 Loss: 0.144909\n",
      "step :2315 Loss: 0.184302\n",
      "step :2316 Loss: 0.0898468\n",
      "step :2317 Loss: 0.0846988\n",
      "step :2318 Loss: 0.216332\n",
      "step :2319 Loss: 0.169828\n",
      "step :2320 Loss: 0.119515\n",
      "step :2321 Loss: 0.0570209\n",
      "step :2322 Loss: 0.0663482\n",
      "step :2323 Loss: 0.170681\n",
      "step :2324 Loss: 0.0930796\n",
      "step :2325 Loss: 0.147843\n",
      "step :2326 Loss: 0.240929\n",
      "step :2327 Loss: 0.135034\n",
      "step :2328 Loss: 0.0940325\n",
      "step :2329 Loss: 0.237477\n",
      "step :2330 Loss: 0.150751\n",
      "step :2331 Loss: 0.12219\n",
      "step :2332 Loss: 0.106638\n",
      "step :2333 Loss: 0.272725\n",
      "step :2334 Loss: 0.0745103\n",
      "step :2335 Loss: 0.0417786\n",
      "step :2336 Loss: 0.242719\n",
      "step :2337 Loss: 0.165949\n",
      "step :2338 Loss: 0.126654\n",
      "step :2339 Loss: 0.142498\n",
      "step :2340 Loss: 0.317384\n",
      "step :2341 Loss: 0.126236\n",
      "step :2342 Loss: 0.111282\n",
      "step :2343 Loss: 0.107883\n",
      "step :2344 Loss: 0.189527\n",
      "step :2345 Loss: 0.13768\n",
      "step :2346 Loss: 0.189389\n",
      "step :2347 Loss: 0.0923661\n",
      "step :2348 Loss: 0.135058\n",
      "step :2349 Loss: 0.059457\n",
      "step :2350 Loss: 0.140523\n",
      "step :2351 Loss: 0.105209\n",
      "step :2352 Loss: 0.142302\n",
      "step :2353 Loss: 0.16539\n",
      "step :2354 Loss: 0.125427\n",
      "step :2355 Loss: 0.0920936\n",
      "step :2356 Loss: 0.116184\n",
      "step :2357 Loss: 0.115435\n",
      "step :2358 Loss: 0.140034\n",
      "step :2359 Loss: 0.09187\n",
      "step :2360 Loss: 0.0675467\n",
      "step :2361 Loss: 0.104579\n",
      "step :2362 Loss: 0.167051\n",
      "step :2363 Loss: 0.103889\n",
      "step :2364 Loss: 0.139802\n",
      "step :2365 Loss: 0.233355\n",
      "step :2366 Loss: 0.0476677\n",
      "step :2367 Loss: 0.0798729\n",
      "step :2368 Loss: 0.0462621\n",
      "step :2369 Loss: 0.061203\n",
      "step :2370 Loss: 0.055401\n",
      "step :2371 Loss: 0.041884\n",
      "step :2372 Loss: 0.112476\n",
      "step :2373 Loss: 0.11493\n",
      "step :2374 Loss: 0.120427\n",
      "step :2375 Loss: 0.0781427\n",
      "step :2376 Loss: 0.224251\n",
      "step :2377 Loss: 0.035071\n",
      "step :2378 Loss: 0.053974\n",
      "step :2379 Loss: 0.128152\n",
      "step :2380 Loss: 0.13647\n",
      "step :2381 Loss: 0.108045\n",
      "step :2382 Loss: 0.0384997\n",
      "step :2383 Loss: 0.15046\n",
      "step :2384 Loss: 0.0713394\n",
      "step :2385 Loss: 0.122085\n",
      "step :2386 Loss: 0.077347\n",
      "step :2387 Loss: 0.161476\n",
      "step :2388 Loss: 0.0819382\n",
      "step :2389 Loss: 0.178451\n",
      "step :2390 Loss: 0.149204\n",
      "step :2391 Loss: 0.10542\n",
      "step :2392 Loss: 0.467088\n",
      "step :2393 Loss: 0.224857\n",
      "step :2394 Loss: 0.0734653\n",
      "step :2395 Loss: 0.113922\n",
      "step :2396 Loss: 0.179269\n",
      "step :2397 Loss: 0.168505\n",
      "step :2398 Loss: 0.217019\n",
      "step :2399 Loss: 0.0869194\n",
      "step :2400 Loss: 0.0967252\n",
      "step :2401 Loss: 0.0751468\n",
      "step :2402 Loss: 0.136491\n",
      "step :2403 Loss: 0.246023\n",
      "step :2404 Loss: 0.0516714\n",
      "step :2405 Loss: 0.11698\n",
      "step :2406 Loss: 0.127418\n",
      "step :2407 Loss: 0.251626\n",
      "step :2408 Loss: 0.249735\n",
      "step :2409 Loss: 0.194969\n",
      "step :2410 Loss: 0.083504\n",
      "step :2411 Loss: 0.111846\n",
      "step :2412 Loss: 0.200142\n",
      "step :2413 Loss: 0.180262\n",
      "step :2414 Loss: 0.0833505\n",
      "step :2415 Loss: 0.0830214\n",
      "step :2416 Loss: 0.284905\n",
      "step :2417 Loss: 0.193509\n",
      "step :2418 Loss: 0.167677\n",
      "step :2419 Loss: 0.114284\n",
      "step :2420 Loss: 0.0744319\n",
      "step :2421 Loss: 0.13362\n",
      "step :2422 Loss: 0.128953\n",
      "step :2423 Loss: 0.112124\n",
      "step :2424 Loss: 0.149791\n",
      "step :2425 Loss: 0.129606\n",
      "step :2426 Loss: 0.187325\n",
      "step :2427 Loss: 0.093019\n",
      "step :2428 Loss: 0.0984001\n",
      "step :2429 Loss: 0.115864\n",
      "18044.940232\n",
      "Model saved in file: /home/shou/network/dataset/model_fcn16s_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(combined_op)\n",
    "    init_fn(sess)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    time_start=time.time()\n",
    "    # Let's read off 3 batches just for example\n",
    "    for i in xrange(486 * 5):\n",
    "    \n",
    "        cross_entropy, summary_string, _ = sess.run([ cross_entropy_sum,\n",
    "                                                      merged_summary_op,\n",
    "                                                      train_step ])\n",
    "\n",
    "        summary_string_writer.add_summary(summary_string, 486 * 5 + i)\n",
    "        \n",
    "        print(\"step :\" + str(i) + \" Loss: \" + str(cross_entropy))\n",
    "        \n",
    "        if i % 486 == 0:\n",
    "            save_path = saver.save(sess, \"/home/shou/network/dataset/model_fcn16s_final.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    time_end = time.time()\n",
    "    print(str(time_end-time_start))\n",
    "    \n",
    "    save_path = saver.save(sess, \"/home/shou/network/dataset/model_fcn16s_final.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "summary_string_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
